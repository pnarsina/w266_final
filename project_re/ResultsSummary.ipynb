{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload\n",
    "import pickle, os, json\n",
    "import torch\n",
    "from pathlib import Path, PureWindowsPath, PurePosixPath\n",
    "from util.tools import load_config\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hyperparams': <util.tools.config at 0x22fa66f0f70>,\n",
       " 'programsettings': <util.tools.config at 0x22fa66f06d0>,\n",
       " 'modelconfig': <util.tools.config at 0x22fa66f0a90>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_folder = \"config/\"\n",
    "config = load_config(config_folder)\n",
    "config.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>lr</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>train_max_seq</th>\n",
       "      <th>class_weights</th>\n",
       "      <th>Kernel_1</th>\n",
       "      <th>Kernel_2</th>\n",
       "      <th>Kernel_3</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "      <th>train_mcc</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>dev_mcc</th>\n",
       "      <th>dev_f1_score</th>\n",
       "      <th>test_mcc</th>\n",
       "      <th>test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reports\\re\\multi_model_experiment_results_2020-11-30 14_26_09_634176.pkl</td>\n",
       "      <td>BioBERT_CNN_fc</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>24</td>\n",
       "      <td>256</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.184182</td>\n",
       "      <td>2.186153</td>\n",
       "      <td>-0.015727</td>\n",
       "      <td>0.073449</td>\n",
       "      <td>-0.014934</td>\n",
       "      <td>0.07267</td>\n",
       "      <td>-0.013602</td>\n",
       "      <td>0.045466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  file_name  \\\n",
       "0  reports\\re\\multi_model_experiment_results_2020-11-30 14_26_09_634176.pkl   \n",
       "\n",
       "       model_name       lr  train_batch_size  train_max_seq  \\\n",
       "0  BioBERT_CNN_fc  0.00001                24            256   \n",
       "\n",
       "                                   class_weights  Kernel_1  Kernel_2  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]         2         3   \n",
       "\n",
       "   Kernel_3  train_loss  dev_loss  train_mcc  train_f1_score   dev_mcc  \\\n",
       "0         4    2.184182  2.186153  -0.015727        0.073449 -0.014934   \n",
       "\n",
       "   dev_f1_score  test_mcc  test_f1_score  \n",
       "0       0.07267 -0.013602       0.045466  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_folder_name = config.programsettings.REPORTS_DIR\n",
    "data_folder = Path(reports_folder_name)\n",
    "list_results_files = list(data_folder.glob('**/multi_model_*.pkl'))\n",
    "\n",
    "all_results = []\n",
    "all_data_source = []\n",
    "\n",
    "for results_file in list_results_files:\n",
    "#     print(results_file)\n",
    "    with open(results_file,\"rb\") as f:\n",
    "        lst_results = pickle.load(f)\n",
    "#     print(\"File Name:\", results_file, ' \\n results: ', lst_results)\n",
    "    \n",
    "    for results in lst_results:\n",
    "        max_seq = 0\n",
    "        lr = 0\n",
    "        train_batch = 0\n",
    "        max_seq = \"\"\n",
    "        model_name = \"\"\n",
    "        class_weights = '[]'\n",
    "        kernel_1 = 0\n",
    "        kernel_2 = 0\n",
    "        kernel_3 = 0\n",
    "        if (len(results[0]) > 50):\n",
    "#             print(results[0])\n",
    "            results_json = json.loads(results[0])\n",
    "            max_seq = results_json[\"hyperparams\"][\"MAX_SEQ_LENGTH\"]\n",
    "            lr = results_json[\"hyperparams\"][\"LEARNING_RATE\"]\n",
    "            train_batch = results_json[\"hyperparams\"][\"TRAIN_BATCH_SIZE\"]\n",
    "            model_name = results_json[\"programsettings\"][\"MODEL_NAME\"]\n",
    "            try:\n",
    "                class_weights = results_json[\"hyperparams\"][\"LOSS_FN_CLASS_WEIGHTS\"]\n",
    "            \n",
    "            except: \n",
    "                class_weights = '[]'\n",
    "                \n",
    "            try:\n",
    "                kernel_1 = results_json[\"modelconfig\"][\"KERNEL_1\"]\n",
    "                kernel_2 = results_json[\"modelconfig\"][\"KERNEL_2\"]\n",
    "                kernel_3 = results_json[\"modelconfig\"][\"KERNEL_3\"]\n",
    "            \n",
    "            except: \n",
    "                class_weights = '[]'\n",
    "\n",
    "        result_store = [results_file, model_name,lr, train_batch,max_seq, class_weights,kernel_1, kernel_2,kernel_3,results[1], \\\n",
    "                        results[2], results[3], results[4], results[5],results[6], results[15], results[16] ]\n",
    "        all_results.append(result_store)\n",
    "        \n",
    "#       Creating list of all dev_labels and dev_preds, so that we can run classification reports for all of them together\n",
    "        if (len(results) > 7):\n",
    "            all_data_source.append([results[7],results[8], results[13], results[14], result_store])\n",
    "                                \n",
    "    \n",
    "columns = ['file_name','model_name', 'lr', 'train_batch_size','train_max_seq','class_weights','Kernel_1','Kernel_2','Kernel_3','train_loss', \\\n",
    "           'dev_loss', 'train_mcc', 'train_f1_score','dev_mcc','dev_f1_score', 'test_mcc', 'test_f1_score' ]\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns = columns)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us generate Classification report for all the things we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " [WindowsPath('reports/re/multi_model_experiment_results_2020-11-30 14_26_09_634176.pkl'), 'BioBERT_CNN_fc', 1e-05, 24, 256, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 2, 3, 4, 2.1841821670532227, 2.1861525535583497, -0.015727100503186858, 0.07344949675776744, -0.014933973451609156, 0.07267033478415591, -0.013602342968071335, 0.045465608182594694]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.06      0.05        18\n",
      "           1       0.00      0.00      0.00         5\n",
      "           2       0.12      0.30      0.17        10\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.17      0.33      0.22         3\n",
      "           5       0.06      0.17      0.09         6\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.04      0.50      0.08         2\n",
      "           8       1.00      0.02      0.05        43\n",
      "\n",
      "    accuracy                           0.08        99\n",
      "   macro avg       0.16      0.15      0.07        99\n",
      "weighted avg       0.46      0.08      0.06        99\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.30      0.15        44\n",
      "           1       0.00      0.00      0.00        53\n",
      "           2       0.07      0.20      0.11        50\n",
      "           3       0.00      0.00      0.00        55\n",
      "           4       0.01      0.10      0.01        10\n",
      "           5       0.02      0.02      0.02        57\n",
      "           6       0.00      0.00      0.00        46\n",
      "           7       0.01      0.12      0.02        33\n",
      "           8       0.59      0.05      0.10       652\n",
      "\n",
      "    accuracy                           0.06      1000\n",
      "   macro avg       0.09      0.09      0.05      1000\n",
      "weighted avg       0.40      0.06      0.08      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_data_source)):\n",
    "    dev_labels, dev_preds, test_preds, test_labels, model_config = all_data_source[i]\n",
    "    print(\"\\n \\n \\n\",model_config)\n",
    "    print(classification_report(dev_labels,dev_preds ))\n",
    "    print(classification_report(test_labels,test_preds ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
