{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'C:\\\\prabhu\\\\edu\\\\code\\\\w266\\\\final_project\\\\config.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import prepare_vocab\n",
    "import train\n",
    "import eval\n",
    "import config\n",
    "from data.loader import DataLoader\n",
    "from utils import constant\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_params = config.VocabParameters()\n",
    "training_params = config.TrainingParameters()\n",
    "eval_params = config.EvalParameters()\n",
    "opt = vars(vocab_params)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "opt.update(vars(training_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files...\n",
      "2525296 tokens from 68124 examples loaded from ./dataset/tacred/train.json.\n",
      "802558 tokens from 22631 examples loaded from ./dataset/tacred/dev.json.\n",
      "539009 tokens from 15509 examples loaded from ./dataset/tacred/test.json.\n",
      "loading glove...\n",
      "2195892 words loaded from glove.\n",
      "building vocab...\n",
      "vocab built with 55950/62152 words.\n",
      "calculating oov...\n",
      "train oov: 20546/2525296 (0.81%)\n",
      "dev oov: 45801/802558 (5.71%)\n",
      "test oov: 33634/539009 (6.24%)\n",
      "building embeddings...\n",
      "embedding size: 55950 x 300\n",
      "dumping to files...\n",
      "all done.\n"
     ]
    }
   ],
   "source": [
    "# %Load glove vectors, get IDs for each token, lookup for each id in embedings and create embeding file for out dataset.\n",
    "vocab = prepare_vocab.prepare_voabulary (vocab_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n"
     ]
    }
   ],
   "source": [
    "from utils.vocab import Vocab\n",
    "vocab_file = vocab_params.vocab_dir + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./dataset/tacred with batch size 50...\n",
      "1363 batches created for ./dataset/tacred/train.json\n",
      "453 batches created for ./dataset/tacred/dev.json\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(vocab_params.data_dir, training_params.batch_size))\n",
    "train_batch = DataLoader(vocab_params.data_dir+ '/train.json', training_params.batch_size, opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(vocab_params.data_dir + '/dev.json', training_params.batch_size, opt, vocab, evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'per:employee_of': 3, 'org:alternate_names': 4, 'org:country_of_headquarters': 5, 'per:countries_of_residence': 6, 'org:city_of_headquarters': 7, 'per:cities_of_residence': 8, 'per:age': 9, 'per:stateorprovinces_of_residence': 10, 'per:origin': 11, 'org:subsidiaries': 12, 'org:parents': 13, 'per:spouse': 14, 'org:stateorprovince_of_headquarters': 15, 'per:children': 16, 'per:other_family': 17, 'per:alternate_names': 18, 'org:members': 19, 'per:siblings': 20, 'per:schools_attended': 21, 'per:parents': 22, 'per:date_of_death': 23, 'org:member_of': 24, 'org:founded_by': 25, 'org:website': 26, 'per:cause_of_death': 27, 'org:political/religious_affiliation': 28, 'org:founded': 29, 'per:city_of_death': 30, 'org:shareholders': 31, 'org:number_of_employees/members': 32, 'per:date_of_birth': 33, 'per:city_of_birth': 34, 'per:charges': 35, 'per:stateorprovince_of_death': 36, 'per:religion': 37, 'per:stateorprovince_of_birth': 38, 'per:country_of_birth': 39, 'org:dissolved': 40, 'per:country_of_death': 41}\n",
      "{'data_dir': './dataset/tacred', 'vocab_dir': './dataset/vocab', 'glove_dir': './dataset/glove', 'emb_dim': 300, 'vocab_file': '/vocab.pkl', 'embed_file': '/embedding.npy', 'glove_text_file': 'glove.840B.300d.txt', 'lower': False, 'min_freq': 0, 'num_class': 42, 'ner_dim': 30, 'pos_dim': 30, 'hidden_dim': 200, 'num_layers': 2, 'dropout': 0.5, 'word_dropout': 0.04, 'topn': 10000000000.0, 'lower_dest': 'lower', 'lower_action': 'store_true', 'no_lower_dest': 'lower', 'no_lower_action': 'store_false', 'attn_dest': 'attn', 'attn_action': 'store_true', 'no_attn_dest': 'attn', 'no_attn_action': 'store_false', 'attn': True, 'attn_dim': 200, 'pe_dim': 30, 'lr': 1.0, 'lr_decay': 0.9, 'optim': 'sgd', 'num_epoch': 30, 'batch_size': 50, 'max_grad_norm': 5, 'log_step': 20, 'log': 'logs.txt', 'save_epoch': 5, 'save_dir': './save_models', 'id': '00', 'info': '', 'seed': 1234, 'cuda': False, 'cpu_action': 'store_true', 'cpu': True}\n",
      "Vocab size 55950 loaded from file\n",
      "Config saved to file ./save_models/00/config.json\n",
      "Overwriting old vocab file at ./save_models/00/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : ./dataset/tacred\n",
      "\tvocab_dir : ./dataset/vocab\n",
      "\tglove_dir : ./dataset/glove\n",
      "\temb_dim : 300\n",
      "\tvocab_file : /vocab.pkl\n",
      "\tembed_file : /embedding.npy\n",
      "\tglove_text_file : glove.840B.300d.txt\n",
      "\tlower : False\n",
      "\tmin_freq : 0\n",
      "\tnum_class : 42\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 200\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower_dest : lower\n",
      "\tlower_action : store_true\n",
      "\tno_lower_dest : lower\n",
      "\tno_lower_action : store_false\n",
      "\tattn_dest : attn\n",
      "\tattn_action : store_true\n",
      "\tno_attn_dest : attn\n",
      "\tno_attn_action : store_false\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.9\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 30\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./save_models\n",
      "\tid : 00\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : False\n",
      "\tcpu_action : store_true\n",
      "\tcpu : True\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./save_models/00\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n",
      "2020-10-11 13:04:33.034817: step 20/40890 (epoch 1/30), loss = 1.258212 (0.685 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:04:47.840228: step 40/40890 (epoch 1/30), loss = 0.754567 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:05:03.117378: step 60/40890 (epoch 1/30), loss = 0.819642 (0.702 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:05:18.670789: step 80/40890 (epoch 1/30), loss = 0.545918 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:05:34.171703: step 100/40890 (epoch 1/30), loss = 0.752670 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:05:49.306573: step 120/40890 (epoch 1/30), loss = 0.977101 (0.889 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:06:04.908361: step 140/40890 (epoch 1/30), loss = 1.168537 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:06:20.526599: step 160/40890 (epoch 1/30), loss = 0.805399 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:06:36.368240: step 180/40890 (epoch 1/30), loss = 1.020030 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:06:51.980493: step 200/40890 (epoch 1/30), loss = 0.931382 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:07:07.237938: step 220/40890 (epoch 1/30), loss = 0.870774 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:07:22.233232: step 240/40890 (epoch 1/30), loss = 1.185092 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:07:37.412361: step 260/40890 (epoch 1/30), loss = 0.818716 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:07:52.530183: step 280/40890 (epoch 1/30), loss = 0.741087 (0.597 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:08:07.851285: step 300/40890 (epoch 1/30), loss = 0.627197 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:08:22.867569: step 320/40890 (epoch 1/30), loss = 0.631486 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:08:37.951236: step 340/40890 (epoch 1/30), loss = 1.038404 (0.630 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:08:53.467745: step 360/40890 (epoch 1/30), loss = 1.245947 (0.712 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:09:08.609258: step 380/40890 (epoch 1/30), loss = 1.349759 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:09:23.789666: step 400/40890 (epoch 1/30), loss = 0.723725 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:09:39.387856: step 420/40890 (epoch 1/30), loss = 0.506739 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:09:55.404030: step 440/40890 (epoch 1/30), loss = 1.057719 (0.781 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:10:10.915553: step 460/40890 (epoch 1/30), loss = 0.891380 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:10:26.410121: step 480/40890 (epoch 1/30), loss = 0.796642 (0.907 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:10:41.680290: step 500/40890 (epoch 1/30), loss = 0.469336 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:10:57.459098: step 520/40890 (epoch 1/30), loss = 0.754350 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:11:13.127203: step 540/40890 (epoch 1/30), loss = 0.402274 (0.801 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:11:28.416320: step 560/40890 (epoch 1/30), loss = 0.741191 (0.727 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:11:43.909087: step 580/40890 (epoch 1/30), loss = 0.499467 (0.659 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:11:58.961836: step 600/40890 (epoch 1/30), loss = 0.882869 (0.660 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:12:14.426485: step 620/40890 (epoch 1/30), loss = 0.590867 (0.777 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:12:30.004412: step 640/40890 (epoch 1/30), loss = 0.758468 (0.596 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:12:45.377306: step 660/40890 (epoch 1/30), loss = 0.414785 (0.700 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:13:01.376647: step 680/40890 (epoch 1/30), loss = 0.521635 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:13:17.694280: step 700/40890 (epoch 1/30), loss = 0.665271 (0.802 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:13:33.962406: step 720/40890 (epoch 1/30), loss = 0.631263 (0.759 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:13:49.999048: step 740/40890 (epoch 1/30), loss = 0.448211 (0.578 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:14:05.946405: step 760/40890 (epoch 1/30), loss = 0.514535 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:14:23.209246: step 780/40890 (epoch 1/30), loss = 0.500477 (0.938 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:14:39.749019: step 800/40890 (epoch 1/30), loss = 1.101705 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:14:55.538039: step 820/40890 (epoch 1/30), loss = 0.539694 (0.922 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:15:11.482471: step 840/40890 (epoch 1/30), loss = 0.408042 (0.786 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:15:27.523578: step 860/40890 (epoch 1/30), loss = 0.404977 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:15:43.087305: step 880/40890 (epoch 1/30), loss = 0.429124 (0.846 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:15:58.762390: step 900/40890 (epoch 1/30), loss = 0.816981 (0.706 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:16:15.016927: step 920/40890 (epoch 1/30), loss = 0.578622 (0.713 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:16:30.764818: step 940/40890 (epoch 1/30), loss = 0.332286 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:16:47.177930: step 960/40890 (epoch 1/30), loss = 0.711051 (0.719 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:17:02.899991: step 980/40890 (epoch 1/30), loss = 0.654821 (0.671 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:17:18.315749: step 1000/40890 (epoch 1/30), loss = 0.482031 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:17:33.787830: step 1020/40890 (epoch 1/30), loss = 0.444282 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:17:48.961748: step 1040/40890 (epoch 1/30), loss = 0.511974 (0.594 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:18:03.637777: step 1060/40890 (epoch 1/30), loss = 0.596279 (0.576 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:18:19.264800: step 1080/40890 (epoch 1/30), loss = 0.530833 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:18:34.486491: step 1100/40890 (epoch 1/30), loss = 0.647643 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:18:50.613369: step 1120/40890 (epoch 1/30), loss = 0.384513 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:19:06.076023: step 1140/40890 (epoch 1/30), loss = 0.644153 (0.720 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:19:20.983162: step 1160/40890 (epoch 1/30), loss = 0.297451 (0.698 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:19:35.991486: step 1180/40890 (epoch 1/30), loss = 0.551875 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:19:51.450152: step 1200/40890 (epoch 1/30), loss = 0.217051 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:20:07.247154: step 1220/40890 (epoch 1/30), loss = 0.665334 (0.802 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:20:23.066852: step 1240/40890 (epoch 1/30), loss = 0.389289 (0.759 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:20:38.649186: step 1260/40890 (epoch 1/30), loss = 0.627109 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:20:53.528400: step 1280/40890 (epoch 1/30), loss = 0.203175 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:21:08.843449: step 1300/40890 (epoch 1/30), loss = 0.481843 (0.560 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:21:23.885227: step 1320/40890 (epoch 1/30), loss = 0.317873 (0.590 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:21:39.674513: step 1340/40890 (epoch 1/30), loss = 0.236476 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:21:55.824960: step 1360/40890 (epoch 1/30), loss = 0.381254 (0.793 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 62.234%\n",
      "   Recall (micro): 41.501%\n",
      "       F1 (micro): 49.796%\n",
      "epoch 1: train_loss = 0.691695, dev_loss = 0.599954, dev_f1 = 0.4980\n",
      "model saved to ./save_models/00/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 13:22:44.376674: step 1380/40890 (epoch 2/30), loss = 0.351459 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:22:59.786305: step 1400/40890 (epoch 2/30), loss = 0.222033 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:23:15.594035: step 1420/40890 (epoch 2/30), loss = 0.553333 (0.704 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:23:31.457617: step 1440/40890 (epoch 2/30), loss = 0.569818 (0.860 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:23:47.383656: step 1460/40890 (epoch 2/30), loss = 0.438725 (0.777 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:24:03.189392: step 1480/40890 (epoch 2/30), loss = 0.485853 (0.801 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:24:20.346515: step 1500/40890 (epoch 2/30), loss = 0.554091 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:24:36.301851: step 1520/40890 (epoch 2/30), loss = 0.796463 (0.804 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:24:52.577331: step 1540/40890 (epoch 2/30), loss = 0.596498 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:25:08.611457: step 1560/40890 (epoch 2/30), loss = 0.427094 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:25:24.647577: step 1580/40890 (epoch 2/30), loss = 0.451356 (0.884 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:25:40.379827: step 1600/40890 (epoch 2/30), loss = 0.528265 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:25:56.186561: step 1620/40890 (epoch 2/30), loss = 0.338092 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:26:12.092030: step 1640/40890 (epoch 2/30), loss = 0.425448 (0.757 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:26:27.874828: step 1660/40890 (epoch 2/30), loss = 0.571385 (0.609 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:26:43.347455: step 1680/40890 (epoch 2/30), loss = 0.192519 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:26:59.212034: step 1700/40890 (epoch 2/30), loss = 0.251841 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:27:14.621707: step 1720/40890 (epoch 2/30), loss = 0.362702 (0.790 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:27:29.738496: step 1740/40890 (epoch 2/30), loss = 0.540871 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:27:45.053589: step 1760/40890 (epoch 2/30), loss = 0.561662 (0.734 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:28:00.350880: step 1780/40890 (epoch 2/30), loss = 0.384051 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:28:16.062922: step 1800/40890 (epoch 2/30), loss = 0.716035 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:28:31.687013: step 1820/40890 (epoch 2/30), loss = 0.440870 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:28:47.444875: step 1840/40890 (epoch 2/30), loss = 0.730007 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:29:02.864644: step 1860/40890 (epoch 2/30), loss = 0.512894 (0.663 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:29:18.699302: step 1880/40890 (epoch 2/30), loss = 0.730003 (0.772 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:29:34.619989: step 1900/40890 (epoch 2/30), loss = 0.637030 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:29:50.221434: step 1920/40890 (epoch 2/30), loss = 0.522554 (0.817 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:30:05.908488: step 1940/40890 (epoch 2/30), loss = 0.461238 (0.801 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:30:20.982181: step 1960/40890 (epoch 2/30), loss = 0.502616 (0.706 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:30:36.034931: step 1980/40890 (epoch 2/30), loss = 0.517018 (0.622 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:30:51.578369: step 2000/40890 (epoch 2/30), loss = 0.357891 (0.786 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:31:06.799667: step 2020/40890 (epoch 2/30), loss = 0.319641 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:31:22.285260: step 2040/40890 (epoch 2/30), loss = 0.530992 (0.781 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:31:38.512625: step 2060/40890 (epoch 2/30), loss = 0.579451 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:31:54.411112: step 2080/40890 (epoch 2/30), loss = 0.303996 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:32:10.710541: step 2100/40890 (epoch 2/30), loss = 0.467466 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:32:26.497328: step 2120/40890 (epoch 2/30), loss = 0.417209 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:32:42.383848: step 2140/40890 (epoch 2/30), loss = 0.408501 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:32:59.075076: step 2160/40890 (epoch 2/30), loss = 0.554758 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:33:15.346195: step 2180/40890 (epoch 2/30), loss = 0.416831 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:33:31.040229: step 2200/40890 (epoch 2/30), loss = 0.232025 (0.743 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:33:47.538757: step 2220/40890 (epoch 2/30), loss = 0.486383 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:34:03.175945: step 2240/40890 (epoch 2/30), loss = 0.493884 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:34:21.518896: step 2260/40890 (epoch 2/30), loss = 0.470942 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:34:37.818312: step 2280/40890 (epoch 2/30), loss = 0.314199 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:34:53.717798: step 2300/40890 (epoch 2/30), loss = 0.363959 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:35:10.441081: step 2320/40890 (epoch 2/30), loss = 0.372695 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:35:26.843222: step 2340/40890 (epoch 2/30), loss = 0.684661 (0.827 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:35:43.365009: step 2360/40890 (epoch 2/30), loss = 0.281405 (0.976 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:36:00.604004: step 2380/40890 (epoch 2/30), loss = 0.318281 (0.776 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:36:16.670045: step 2400/40890 (epoch 2/30), loss = 0.832487 (0.677 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:36:31.783632: step 2420/40890 (epoch 2/30), loss = 0.234721 (0.676 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:36:47.584381: step 2440/40890 (epoch 2/30), loss = 0.398276 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:37:03.377898: step 2460/40890 (epoch 2/30), loss = 0.337867 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:37:19.294339: step 2480/40890 (epoch 2/30), loss = 0.432942 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:37:35.044924: step 2500/40890 (epoch 2/30), loss = 0.546257 (0.638 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:37:50.272420: step 2520/40890 (epoch 2/30), loss = 0.696190 (0.733 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:38:05.292526: step 2540/40890 (epoch 2/30), loss = 0.249179 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:38:21.052284: step 2560/40890 (epoch 2/30), loss = 0.370627 (0.677 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:38:36.705428: step 2580/40890 (epoch 2/30), loss = 0.337852 (0.704 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:38:53.037756: step 2600/40890 (epoch 2/30), loss = 0.307138 (0.713 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:39:09.075871: step 2620/40890 (epoch 2/30), loss = 0.763068 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:39:24.356013: step 2640/40890 (epoch 2/30), loss = 0.400643 (0.908 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:39:39.991199: step 2660/40890 (epoch 2/30), loss = 0.260900 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:39:55.281213: step 2680/40890 (epoch 2/30), loss = 0.653163 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:40:11.373184: step 2700/40890 (epoch 2/30), loss = 0.899716 (0.714 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:40:27.377389: step 2720/40890 (epoch 2/30), loss = 0.492710 (0.859 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 62.720%\n",
      "   Recall (micro): 49.117%\n",
      "       F1 (micro): 55.091%\n",
      "epoch 2: train_loss = 0.461492, dev_loss = 0.522418, dev_f1 = 0.5509\n",
      "model saved to ./save_models/00/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 13:41:14.702371: step 2740/40890 (epoch 3/30), loss = 0.475547 (0.771 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:41:30.447269: step 2760/40890 (epoch 3/30), loss = 0.437721 (0.657 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:41:46.420960: step 2780/40890 (epoch 3/30), loss = 0.543649 (0.594 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:42:02.491987: step 2800/40890 (epoch 3/30), loss = 0.442456 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:42:19.373685: step 2820/40890 (epoch 3/30), loss = 0.439783 (0.846 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:42:35.421773: step 2840/40890 (epoch 3/30), loss = 0.371032 (0.915 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:42:51.687279: step 2860/40890 (epoch 3/30), loss = 0.227484 (0.775 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:43:08.285132: step 2880/40890 (epoch 3/30), loss = 0.383391 (0.918 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:43:24.806556: step 2900/40890 (epoch 3/30), loss = 0.351658 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:43:41.479947: step 2920/40890 (epoch 3/30), loss = 0.417486 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:43:58.439543: step 2940/40890 (epoch 3/30), loss = 0.138385 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:44:15.449060: step 2960/40890 (epoch 3/30), loss = 0.432290 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:44:32.252129: step 2980/40890 (epoch 3/30), loss = 0.496090 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:44:48.444831: step 3000/40890 (epoch 3/30), loss = 0.247792 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:45:04.738263: step 3020/40890 (epoch 3/30), loss = 0.333668 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:45:20.615807: step 3040/40890 (epoch 3/30), loss = 0.304283 (0.949 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:45:37.049254: step 3060/40890 (epoch 3/30), loss = 0.674939 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:45:53.010379: step 3080/40890 (epoch 3/30), loss = 0.275935 (0.662 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:46:08.858003: step 3100/40890 (epoch 3/30), loss = 0.274411 (0.772 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:46:24.622849: step 3120/40890 (epoch 3/30), loss = 0.475181 (0.926 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:46:40.607108: step 3140/40890 (epoch 3/30), loss = 0.321239 (0.851 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:46:57.070087: step 3160/40890 (epoch 3/30), loss = 0.389389 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:47:13.271054: step 3180/40890 (epoch 3/30), loss = 0.587744 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:47:29.527954: step 3200/40890 (epoch 3/30), loss = 0.691101 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:47:45.665001: step 3220/40890 (epoch 3/30), loss = 0.456050 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:48:02.105792: step 3240/40890 (epoch 3/30), loss = 0.266487 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:48:18.510925: step 3260/40890 (epoch 3/30), loss = 0.270900 (0.955 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:48:34.896097: step 3280/40890 (epoch 3/30), loss = 0.567984 (0.611 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:48:51.060949: step 3300/40890 (epoch 3/30), loss = 0.414305 (0.914 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:49:06.707112: step 3320/40890 (epoch 3/30), loss = 0.346561 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:49:21.722961: step 3340/40890 (epoch 3/30), loss = 0.547545 (0.877 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:49:37.177260: step 3360/40890 (epoch 3/30), loss = 0.258181 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:49:52.457694: step 3380/40890 (epoch 3/30), loss = 0.278401 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:50:07.979190: step 3400/40890 (epoch 3/30), loss = 0.447350 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:50:24.114046: step 3420/40890 (epoch 3/30), loss = 0.345186 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:50:40.295777: step 3440/40890 (epoch 3/30), loss = 0.443105 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:50:56.895390: step 3460/40890 (epoch 3/30), loss = 0.459025 (0.835 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:51:12.957441: step 3480/40890 (epoch 3/30), loss = 0.581531 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:51:28.659455: step 3500/40890 (epoch 3/30), loss = 0.536138 (0.916 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:51:45.634602: step 3520/40890 (epoch 3/30), loss = 0.386719 (0.771 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:52:02.928258: step 3540/40890 (epoch 3/30), loss = 0.665045 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:52:19.282818: step 3560/40890 (epoch 3/30), loss = 0.384977 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:52:36.181543: step 3580/40890 (epoch 3/30), loss = 0.624420 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:52:51.708016: step 3600/40890 (epoch 3/30), loss = 0.512193 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:53:08.413911: step 3620/40890 (epoch 3/30), loss = 0.298791 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:53:25.048143: step 3640/40890 (epoch 3/30), loss = 0.413408 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:53:41.439744: step 3660/40890 (epoch 3/30), loss = 0.498477 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:53:58.536838: step 3680/40890 (epoch 3/30), loss = 0.392290 (0.809 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:54:18.304665: step 3700/40890 (epoch 3/30), loss = 0.313288 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:54:35.679467: step 3720/40890 (epoch 3/30), loss = 0.415201 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:54:52.309997: step 3740/40890 (epoch 3/30), loss = 0.442739 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:55:09.219781: step 3760/40890 (epoch 3/30), loss = 0.137002 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:55:25.379740: step 3780/40890 (epoch 3/30), loss = 0.266545 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:55:41.936740: step 3800/40890 (epoch 3/30), loss = 0.470729 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:55:58.776922: step 3820/40890 (epoch 3/30), loss = 0.548967 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:56:14.898709: step 3840/40890 (epoch 3/30), loss = 0.516246 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:56:31.314814: step 3860/40890 (epoch 3/30), loss = 0.412175 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:56:46.708651: step 3880/40890 (epoch 3/30), loss = 0.381912 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:57:01.536853: step 3900/40890 (epoch 3/30), loss = 0.391199 (0.708 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:57:17.100449: step 3920/40890 (epoch 3/30), loss = 0.479034 (0.673 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:57:33.058752: step 3940/40890 (epoch 3/30), loss = 0.416537 (0.612 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:57:49.323902: step 3960/40890 (epoch 3/30), loss = 0.436230 (0.743 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:58:06.116691: step 3980/40890 (epoch 3/30), loss = 0.353541 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:58:21.108889: step 4000/40890 (epoch 3/30), loss = 0.488590 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:58:37.100293: step 4020/40890 (epoch 3/30), loss = 0.399689 (0.802 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:58:52.953901: step 4040/40890 (epoch 3/30), loss = 0.532768 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:59:09.075504: step 4060/40890 (epoch 3/30), loss = 0.117878 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 13:59:24.989732: step 4080/40890 (epoch 3/30), loss = 0.328493 (0.735 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.299%\n",
      "   Recall (micro): 49.724%\n",
      "       F1 (micro): 56.827%\n",
      "epoch 3: train_loss = 0.421486, dev_loss = 0.493718, dev_f1 = 0.5683\n",
      "model saved to ./save_models/00/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 14:00:11.992473: step 4100/40890 (epoch 4/30), loss = 0.275601 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:00:27.893455: step 4120/40890 (epoch 4/30), loss = 0.374733 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:00:44.222976: step 4140/40890 (epoch 4/30), loss = 0.393457 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:00:59.514427: step 4160/40890 (epoch 4/30), loss = 0.223970 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:01:16.355466: step 4180/40890 (epoch 4/30), loss = 0.381992 (0.896 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:01:31.543445: step 4200/40890 (epoch 4/30), loss = 0.504645 (0.598 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:01:47.287917: step 4220/40890 (epoch 4/30), loss = 0.542174 (0.695 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:02:02.948354: step 4240/40890 (epoch 4/30), loss = 0.291783 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:02:18.807201: step 4260/40890 (epoch 4/30), loss = 0.580566 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:02:35.433742: step 4280/40890 (epoch 4/30), loss = 0.325113 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:02:51.536684: step 4300/40890 (epoch 4/30), loss = 0.407814 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:03:07.416666: step 4320/40890 (epoch 4/30), loss = 0.780919 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:03:23.751900: step 4340/40890 (epoch 4/30), loss = 0.507160 (0.911 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:03:40.757156: step 4360/40890 (epoch 4/30), loss = 0.368053 (0.800 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:03:57.546864: step 4380/40890 (epoch 4/30), loss = 0.293137 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:04:15.597768: step 4400/40890 (epoch 4/30), loss = 0.161205 (1.082 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:04:32.577538: step 4420/40890 (epoch 4/30), loss = 0.391757 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:04:49.007886: step 4440/40890 (epoch 4/30), loss = 0.421467 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:05:05.355230: step 4460/40890 (epoch 4/30), loss = 0.293350 (0.843 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:05:21.167947: step 4480/40890 (epoch 4/30), loss = 0.199532 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:05:37.330649: step 4500/40890 (epoch 4/30), loss = 0.645075 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:05:54.172188: step 4520/40890 (epoch 4/30), loss = 0.652143 (0.937 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:06:11.470754: step 4540/40890 (epoch 4/30), loss = 0.512288 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:06:28.108360: step 4560/40890 (epoch 4/30), loss = 0.711020 (0.616 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:06:44.669811: step 4580/40890 (epoch 4/30), loss = 0.276421 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:07:01.276065: step 4600/40890 (epoch 4/30), loss = 0.504311 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:07:17.816463: step 4620/40890 (epoch 4/30), loss = 0.807586 (0.896 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:07:35.028224: step 4640/40890 (epoch 4/30), loss = 0.300543 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:07:52.416613: step 4660/40890 (epoch 4/30), loss = 0.207720 (0.957 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:08:09.698525: step 4680/40890 (epoch 4/30), loss = 0.255396 (0.925 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:08:25.752391: step 4700/40890 (epoch 4/30), loss = 0.614088 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:08:41.827746: step 4720/40890 (epoch 4/30), loss = 0.304594 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:08:58.407450: step 4740/40890 (epoch 4/30), loss = 0.313247 (0.925 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:09:15.023746: step 4760/40890 (epoch 4/30), loss = 0.507653 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:09:32.403863: step 4780/40890 (epoch 4/30), loss = 0.367093 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:09:49.502418: step 4800/40890 (epoch 4/30), loss = 0.334653 (0.936 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:10:06.410968: step 4820/40890 (epoch 4/30), loss = 0.617099 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:10:22.608656: step 4840/40890 (epoch 4/30), loss = 0.416411 (0.684 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:10:39.574131: step 4860/40890 (epoch 4/30), loss = 0.314532 (1.020 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:10:56.859540: step 4880/40890 (epoch 4/30), loss = 0.339998 (0.916 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:11:13.588664: step 4900/40890 (epoch 4/30), loss = 0.741088 (0.886 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:11:29.462370: step 4920/40890 (epoch 4/30), loss = 0.351805 (0.681 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:11:46.517252: step 4940/40890 (epoch 4/30), loss = 0.143959 (0.846 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:12:01.487223: step 4960/40890 (epoch 4/30), loss = 0.217853 (0.579 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:12:18.436795: step 4980/40890 (epoch 4/30), loss = 0.295341 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:12:35.226439: step 5000/40890 (epoch 4/30), loss = 0.252176 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:12:51.207706: step 5020/40890 (epoch 4/30), loss = 0.335462 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:13:08.847695: step 5040/40890 (epoch 4/30), loss = 0.374004 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:13:25.857038: step 5060/40890 (epoch 4/30), loss = 0.446439 (1.076 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:13:42.741329: step 5080/40890 (epoch 4/30), loss = 0.414594 (1.002 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:13:59.988047: step 5100/40890 (epoch 4/30), loss = 0.419140 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:14:17.752546: step 5120/40890 (epoch 4/30), loss = 0.454593 (1.015 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:14:33.394719: step 5140/40890 (epoch 4/30), loss = 0.429825 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:14:49.121154: step 5160/40890 (epoch 4/30), loss = 0.405923 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:15:05.316479: step 5180/40890 (epoch 4/30), loss = 0.162284 (0.651 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:15:21.611533: step 5200/40890 (epoch 4/30), loss = 0.392240 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:15:37.765241: step 5220/40890 (epoch 4/30), loss = 0.553670 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:15:53.527095: step 5240/40890 (epoch 4/30), loss = 0.375464 (0.687 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:16:09.457941: step 5260/40890 (epoch 4/30), loss = 0.598051 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:16:25.457423: step 5280/40890 (epoch 4/30), loss = 0.478750 (0.939 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:16:42.451457: step 5300/40890 (epoch 4/30), loss = 0.369798 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:16:59.313338: step 5320/40890 (epoch 4/30), loss = 0.471143 (0.698 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:17:15.689550: step 5340/40890 (epoch 4/30), loss = 0.374802 (0.757 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:17:30.295915: step 5360/40890 (epoch 4/30), loss = 0.214692 (0.603 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:17:46.230758: step 5380/40890 (epoch 4/30), loss = 0.477404 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:18:02.150561: step 5400/40890 (epoch 4/30), loss = 0.448528 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:18:18.724305: step 5420/40890 (epoch 4/30), loss = 0.267585 (0.982 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:18:34.859774: step 5440/40890 (epoch 4/30), loss = 0.318826 (0.955 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 65.087%\n",
      "   Recall (micro): 55.831%\n",
      "       F1 (micro): 60.105%\n",
      "epoch 4: train_loss = 0.398810, dev_loss = 0.483216, dev_f1 = 0.6010\n",
      "model saved to ./save_models/00/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 14:19:23.351967: step 5460/40890 (epoch 5/30), loss = 0.418857 (0.752 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:19:39.116753: step 5480/40890 (epoch 5/30), loss = 0.288775 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:19:54.962381: step 5500/40890 (epoch 5/30), loss = 0.411806 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:20:10.336670: step 5520/40890 (epoch 5/30), loss = 0.628193 (0.699 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:20:26.814962: step 5540/40890 (epoch 5/30), loss = 0.294569 (0.899 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:20:42.919201: step 5560/40890 (epoch 5/30), loss = 0.349841 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:20:58.413747: step 5580/40890 (epoch 5/30), loss = 0.481392 (0.903 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:21:14.309135: step 5600/40890 (epoch 5/30), loss = 0.212178 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:21:30.163259: step 5620/40890 (epoch 5/30), loss = 0.315103 (0.700 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:21:46.424391: step 5640/40890 (epoch 5/30), loss = 0.317224 (0.706 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:22:02.908110: step 5660/40890 (epoch 5/30), loss = 0.454567 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:22:18.466715: step 5680/40890 (epoch 5/30), loss = 0.537975 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:22:34.838077: step 5700/40890 (epoch 5/30), loss = 0.445455 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:22:50.750530: step 5720/40890 (epoch 5/30), loss = 0.183146 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:23:06.831620: step 5740/40890 (epoch 5/30), loss = 0.417532 (0.848 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:23:23.140339: step 5760/40890 (epoch 5/30), loss = 0.391424 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:23:39.394716: step 5780/40890 (epoch 5/30), loss = 0.344195 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:23:55.658968: step 5800/40890 (epoch 5/30), loss = 0.237568 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:24:13.983613: step 5820/40890 (epoch 5/30), loss = 0.357285 (1.012 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:24:30.279561: step 5840/40890 (epoch 5/30), loss = 0.246691 (0.664 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:24:45.854395: step 5860/40890 (epoch 5/30), loss = 0.386444 (0.905 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:25:02.616838: step 5880/40890 (epoch 5/30), loss = 0.404005 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:25:20.150271: step 5900/40890 (epoch 5/30), loss = 0.426119 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:25:36.300038: step 5920/40890 (epoch 5/30), loss = 0.353888 (0.644 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:25:52.587330: step 5940/40890 (epoch 5/30), loss = 0.518769 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:26:08.936808: step 5960/40890 (epoch 5/30), loss = 0.346594 (0.817 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:26:25.888474: step 5980/40890 (epoch 5/30), loss = 0.480016 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:26:43.509610: step 6000/40890 (epoch 5/30), loss = 0.240315 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:26:59.675036: step 6020/40890 (epoch 5/30), loss = 0.765525 (0.561 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:27:15.530639: step 6040/40890 (epoch 5/30), loss = 0.276276 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:27:30.728092: step 6060/40890 (epoch 5/30), loss = 0.446086 (0.631 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:27:46.425942: step 6080/40890 (epoch 5/30), loss = 0.461679 (0.754 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:28:02.259085: step 6100/40890 (epoch 5/30), loss = 0.253162 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:28:18.394892: step 6120/40890 (epoch 5/30), loss = 0.462462 (0.802 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:28:34.898203: step 6140/40890 (epoch 5/30), loss = 0.435344 (0.921 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:28:51.053007: step 6160/40890 (epoch 5/30), loss = 0.413134 (0.776 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:29:07.648904: step 6180/40890 (epoch 5/30), loss = 0.347301 (0.775 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:29:24.068879: step 6200/40890 (epoch 5/30), loss = 0.672310 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:29:39.602565: step 6220/40890 (epoch 5/30), loss = 0.382054 (0.568 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:29:56.600777: step 6240/40890 (epoch 5/30), loss = 0.537120 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:30:13.055424: step 6260/40890 (epoch 5/30), loss = 0.345838 (0.687 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:30:28.081676: step 6280/40890 (epoch 5/30), loss = 0.567585 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:30:44.458552: step 6300/40890 (epoch 5/30), loss = 0.591231 (0.781 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:30:59.404620: step 6320/40890 (epoch 5/30), loss = 0.352950 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:31:14.806300: step 6340/40890 (epoch 5/30), loss = 0.379642 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:31:30.687952: step 6360/40890 (epoch 5/30), loss = 0.334634 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:31:46.470973: step 6380/40890 (epoch 5/30), loss = 0.592362 (0.911 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:32:03.625483: step 6400/40890 (epoch 5/30), loss = 0.703794 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:32:21.173659: step 6420/40890 (epoch 5/30), loss = 0.404229 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:32:37.451026: step 6440/40890 (epoch 5/30), loss = 0.279953 (0.642 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:32:54.014371: step 6460/40890 (epoch 5/30), loss = 0.480351 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:33:11.085915: step 6480/40890 (epoch 5/30), loss = 0.444517 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:33:26.562834: step 6500/40890 (epoch 5/30), loss = 0.283337 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:33:42.435849: step 6520/40890 (epoch 5/30), loss = 0.326371 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:33:59.013737: step 6540/40890 (epoch 5/30), loss = 0.541308 (0.819 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:34:17.531555: step 6560/40890 (epoch 5/30), loss = 0.339269 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:34:34.946340: step 6580/40890 (epoch 5/30), loss = 0.578321 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:34:50.733398: step 6600/40890 (epoch 5/30), loss = 0.536641 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:35:06.537784: step 6620/40890 (epoch 5/30), loss = 0.268318 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:35:23.485636: step 6640/40890 (epoch 5/30), loss = 0.489877 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:35:40.008714: step 6660/40890 (epoch 5/30), loss = 0.305741 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:35:56.880871: step 6680/40890 (epoch 5/30), loss = 0.208868 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:36:13.856479: step 6700/40890 (epoch 5/30), loss = 0.292177 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:36:29.449495: step 6720/40890 (epoch 5/30), loss = 0.404527 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:36:45.259410: step 6740/40890 (epoch 5/30), loss = 0.636269 (0.955 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:37:00.734673: step 6760/40890 (epoch 5/30), loss = 0.253901 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:37:16.763568: step 6780/40890 (epoch 5/30), loss = 0.426630 (0.835 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:37:32.828466: step 6800/40890 (epoch 5/30), loss = 0.277840 (0.610 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.608%\n",
      "   Recall (micro): 56.144%\n",
      "       F1 (micro): 60.930%\n",
      "epoch 5: train_loss = 0.381972, dev_loss = 0.468309, dev_f1 = 0.6093\n",
      "model saved to ./save_models/00/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 14:38:21.447408: step 6820/40890 (epoch 6/30), loss = 0.259814 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:38:37.305451: step 6840/40890 (epoch 6/30), loss = 0.368494 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:38:52.689081: step 6860/40890 (epoch 6/30), loss = 0.223640 (0.821 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:39:08.196741: step 6880/40890 (epoch 6/30), loss = 0.289037 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:39:24.463027: step 6900/40890 (epoch 6/30), loss = 0.297718 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:39:39.851490: step 6920/40890 (epoch 6/30), loss = 0.377839 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:39:55.456180: step 6940/40890 (epoch 6/30), loss = 0.478829 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:40:11.406528: step 6960/40890 (epoch 6/30), loss = 0.457449 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:40:27.688423: step 6980/40890 (epoch 6/30), loss = 0.272999 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:40:44.335446: step 7000/40890 (epoch 6/30), loss = 0.558728 (0.898 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:41:00.428484: step 7020/40890 (epoch 6/30), loss = 0.245624 (0.929 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:41:16.394791: step 7040/40890 (epoch 6/30), loss = 0.399017 (0.912 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:41:32.559033: step 7060/40890 (epoch 6/30), loss = 0.430332 (0.765 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:41:48.515505: step 7080/40890 (epoch 6/30), loss = 0.367391 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:42:05.057588: step 7100/40890 (epoch 6/30), loss = 0.560456 (0.719 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:42:21.595910: step 7120/40890 (epoch 6/30), loss = 0.448574 (0.686 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:42:37.855433: step 7140/40890 (epoch 6/30), loss = 0.394462 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:42:53.922470: step 7160/40890 (epoch 6/30), loss = 0.314947 (0.689 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:43:10.803825: step 7180/40890 (epoch 6/30), loss = 0.318806 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:43:27.313679: step 7200/40890 (epoch 6/30), loss = 0.106387 (0.958 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:43:43.330798: step 7220/40890 (epoch 6/30), loss = 0.399481 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:44:00.490016: step 7240/40890 (epoch 6/30), loss = 0.181411 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:44:19.248856: step 7260/40890 (epoch 6/30), loss = 0.455352 (0.929 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:44:36.451381: step 7280/40890 (epoch 6/30), loss = 0.208879 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:44:53.317282: step 7300/40890 (epoch 6/30), loss = 0.331109 (0.898 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:45:09.378344: step 7320/40890 (epoch 6/30), loss = 0.295791 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:45:26.108801: step 7340/40890 (epoch 6/30), loss = 0.443915 (0.652 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:45:43.859621: step 7360/40890 (epoch 6/30), loss = 0.404860 (0.940 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:46:00.552166: step 7380/40890 (epoch 6/30), loss = 0.206828 (0.945 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:46:17.026907: step 7400/40890 (epoch 6/30), loss = 0.426176 (0.659 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:46:33.399621: step 7420/40890 (epoch 6/30), loss = 0.467622 (0.966 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:46:49.487474: step 7440/40890 (epoch 6/30), loss = 0.246782 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:47:05.636293: step 7460/40890 (epoch 6/30), loss = 0.337675 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:47:21.199756: step 7480/40890 (epoch 6/30), loss = 0.407057 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:47:37.043876: step 7500/40890 (epoch 6/30), loss = 0.584157 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:47:53.483348: step 7520/40890 (epoch 6/30), loss = 0.313807 (0.884 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:48:09.638435: step 7540/40890 (epoch 6/30), loss = 0.493526 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:48:25.968207: step 7560/40890 (epoch 6/30), loss = 0.461759 (0.790 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:48:41.277272: step 7580/40890 (epoch 6/30), loss = 0.385070 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:48:57.016976: step 7600/40890 (epoch 6/30), loss = 0.347533 (0.593 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:49:13.563730: step 7620/40890 (epoch 6/30), loss = 0.325577 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:49:29.009429: step 7640/40890 (epoch 6/30), loss = 0.245421 (0.712 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:49:45.324102: step 7660/40890 (epoch 6/30), loss = 0.318786 (0.921 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:50:00.706005: step 7680/40890 (epoch 6/30), loss = 0.581025 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:50:16.331764: step 7700/40890 (epoch 6/30), loss = 0.417176 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:50:32.154454: step 7720/40890 (epoch 6/30), loss = 0.369341 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:50:48.071297: step 7740/40890 (epoch 6/30), loss = 0.101382 (0.695 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:51:03.719085: step 7760/40890 (epoch 6/30), loss = 0.532642 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:51:20.030382: step 7780/40890 (epoch 6/30), loss = 0.505620 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:51:36.676798: step 7800/40890 (epoch 6/30), loss = 0.487139 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:51:52.696976: step 7820/40890 (epoch 6/30), loss = 0.330932 (0.917 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:52:09.417854: step 7840/40890 (epoch 6/30), loss = 0.272292 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:52:24.689297: step 7860/40890 (epoch 6/30), loss = 0.524822 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:52:41.324571: step 7880/40890 (epoch 6/30), loss = 0.481835 (0.819 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:52:57.896943: step 7900/40890 (epoch 6/30), loss = 0.317646 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:53:13.976702: step 7920/40890 (epoch 6/30), loss = 0.312945 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:53:30.580320: step 7940/40890 (epoch 6/30), loss = 0.485920 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:53:46.982825: step 7960/40890 (epoch 6/30), loss = 0.228309 (0.740 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:54:03.169543: step 7980/40890 (epoch 6/30), loss = 0.495646 (0.687 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:54:20.782779: step 8000/40890 (epoch 6/30), loss = 0.622250 (0.915 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:54:37.922312: step 8020/40890 (epoch 6/30), loss = 0.187788 (0.715 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:54:55.010058: step 8040/40890 (epoch 6/30), loss = 0.496175 (0.960 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:55:12.251954: step 8060/40890 (epoch 6/30), loss = 0.445377 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:55:28.519455: step 8080/40890 (epoch 6/30), loss = 0.462069 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:55:44.324690: step 8100/40890 (epoch 6/30), loss = 0.199282 (0.889 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:56:00.368790: step 8120/40890 (epoch 6/30), loss = 0.239903 (0.932 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:56:17.567432: step 8140/40890 (epoch 6/30), loss = 0.253083 (0.933 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:56:35.874854: step 8160/40890 (epoch 6/30), loss = 0.461033 (1.004 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.956%\n",
      "   Recall (micro): 58.113%\n",
      "       F1 (micro): 62.222%\n",
      "epoch 6: train_loss = 0.365855, dev_loss = 0.454300, dev_f1 = 0.6222\n",
      "model saved to ./save_models/00/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 14:57:25.310605: step 8180/40890 (epoch 7/30), loss = 0.455318 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:57:41.341309: step 8200/40890 (epoch 7/30), loss = 0.403350 (0.726 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:57:56.793456: step 8220/40890 (epoch 7/30), loss = 0.524517 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:58:12.658211: step 8240/40890 (epoch 7/30), loss = 0.293119 (0.687 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:58:29.390044: step 8260/40890 (epoch 7/30), loss = 0.218260 (0.772 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:58:46.289854: step 8280/40890 (epoch 7/30), loss = 0.242785 (0.819 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:59:02.581292: step 8300/40890 (epoch 7/30), loss = 0.305241 (0.926 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:59:18.889684: step 8320/40890 (epoch 7/30), loss = 0.376624 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:59:35.002673: step 8340/40890 (epoch 7/30), loss = 0.154085 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-11 14:59:51.390839: step 8360/40890 (epoch 7/30), loss = 0.293564 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:00:07.458043: step 8380/40890 (epoch 7/30), loss = 0.483948 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:00:23.297149: step 8400/40890 (epoch 7/30), loss = 0.372303 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:00:39.978873: step 8420/40890 (epoch 7/30), loss = 0.532751 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:00:56.008757: step 8440/40890 (epoch 7/30), loss = 0.385513 (0.660 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:01:11.469416: step 8460/40890 (epoch 7/30), loss = 0.423319 (0.736 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:01:27.666981: step 8480/40890 (epoch 7/30), loss = 0.246929 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:01:43.321902: step 8500/40890 (epoch 7/30), loss = 0.364285 (0.961 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:01:58.527244: step 8520/40890 (epoch 7/30), loss = 0.236822 (0.807 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:02:14.853841: step 8540/40890 (epoch 7/30), loss = 0.355149 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:02:30.656586: step 8560/40890 (epoch 7/30), loss = 0.273841 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:02:46.820258: step 8580/40890 (epoch 7/30), loss = 0.305107 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:03:03.267919: step 8600/40890 (epoch 7/30), loss = 0.244700 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:03:19.779291: step 8620/40890 (epoch 7/30), loss = 0.341720 (0.852 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:03:36.558862: step 8640/40890 (epoch 7/30), loss = 0.269114 (0.948 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:03:53.448700: step 8660/40890 (epoch 7/30), loss = 0.112468 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:04:11.401080: step 8680/40890 (epoch 7/30), loss = 0.526380 (0.931 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:04:30.010434: step 8700/40890 (epoch 7/30), loss = 0.313420 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:04:47.155945: step 8720/40890 (epoch 7/30), loss = 0.464806 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:05:04.066726: step 8740/40890 (epoch 7/30), loss = 0.368389 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:05:20.495676: step 8760/40890 (epoch 7/30), loss = 0.335016 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:05:36.529038: step 8780/40890 (epoch 7/30), loss = 0.286065 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:05:53.525602: step 8800/40890 (epoch 7/30), loss = 0.268130 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:06:09.707842: step 8820/40890 (epoch 7/30), loss = 0.466492 (0.757 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:06:25.616304: step 8840/40890 (epoch 7/30), loss = 0.184497 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:06:41.740189: step 8860/40890 (epoch 7/30), loss = 0.590949 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:06:58.190203: step 8880/40890 (epoch 7/30), loss = 0.393583 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:07:14.552451: step 8900/40890 (epoch 7/30), loss = 0.301043 (0.925 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:07:30.783808: step 8920/40890 (epoch 7/30), loss = 0.469305 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:07:46.644854: step 8940/40890 (epoch 7/30), loss = 0.327290 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:08:03.200153: step 8960/40890 (epoch 7/30), loss = 0.171741 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:08:20.073596: step 8980/40890 (epoch 7/30), loss = 0.385909 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:08:36.199781: step 9000/40890 (epoch 7/30), loss = 0.401028 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:08:52.320661: step 9020/40890 (epoch 7/30), loss = 0.264062 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:09:08.383710: step 9040/40890 (epoch 7/30), loss = 0.307965 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:09:23.719703: step 9060/40890 (epoch 7/30), loss = 0.427882 (0.655 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:09:40.266505: step 9080/40890 (epoch 7/30), loss = 0.304764 (0.912 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:09:57.471719: step 9100/40890 (epoch 7/30), loss = 0.212775 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:10:13.420073: step 9120/40890 (epoch 7/30), loss = 0.274550 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:10:29.800658: step 9140/40890 (epoch 7/30), loss = 0.295656 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:10:45.555530: step 9160/40890 (epoch 7/30), loss = 0.509729 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:11:01.408886: step 9180/40890 (epoch 7/30), loss = 0.539135 (0.852 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:11:17.047072: step 9200/40890 (epoch 7/30), loss = 0.175644 (0.758 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:11:32.379074: step 9220/40890 (epoch 7/30), loss = 0.408481 (0.672 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:11:47.651550: step 9240/40890 (epoch 7/30), loss = 0.400811 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:12:03.771456: step 9260/40890 (epoch 7/30), loss = 0.133353 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:12:19.608417: step 9280/40890 (epoch 7/30), loss = 0.212455 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:12:35.860959: step 9300/40890 (epoch 7/30), loss = 0.699133 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:12:51.946946: step 9320/40890 (epoch 7/30), loss = 0.226291 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:13:07.554774: step 9340/40890 (epoch 7/30), loss = 0.247753 (0.955 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:13:23.536037: step 9360/40890 (epoch 7/30), loss = 0.403505 (0.919 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:13:39.902154: step 9380/40890 (epoch 7/30), loss = 0.337132 (0.905 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:13:56.626435: step 9400/40890 (epoch 7/30), loss = 0.439681 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:14:14.766928: step 9420/40890 (epoch 7/30), loss = 0.164914 (0.977 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:14:31.595928: step 9440/40890 (epoch 7/30), loss = 0.341927 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:14:47.051600: step 9460/40890 (epoch 7/30), loss = 0.277631 (0.759 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:15:02.709731: step 9480/40890 (epoch 7/30), loss = 0.256558 (0.679 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:15:18.694934: step 9500/40890 (epoch 7/30), loss = 0.341354 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:15:35.344702: step 9520/40890 (epoch 7/30), loss = 0.271999 (0.908 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:15:51.440663: step 9540/40890 (epoch 7/30), loss = 0.501281 (0.664 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.884%\n",
      "   Recall (micro): 57.542%\n",
      "       F1 (micro): 62.704%\n",
      "epoch 7: train_loss = 0.355265, dev_loss = 0.446472, dev_f1 = 0.6270\n",
      "model saved to ./save_models/00/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 15:16:42.661047: step 9560/40890 (epoch 8/30), loss = 0.239692 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:16:58.480747: step 9580/40890 (epoch 8/30), loss = 0.243645 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:17:14.492516: step 9600/40890 (epoch 8/30), loss = 0.419820 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:17:30.343131: step 9620/40890 (epoch 8/30), loss = 0.212347 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:17:46.398527: step 9640/40890 (epoch 8/30), loss = 0.231479 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:18:02.104624: step 9660/40890 (epoch 8/30), loss = 0.258210 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:18:17.908700: step 9680/40890 (epoch 8/30), loss = 0.450976 (0.665 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:18:33.788262: step 9700/40890 (epoch 8/30), loss = 0.276314 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:18:50.189406: step 9720/40890 (epoch 8/30), loss = 0.134051 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:19:05.889425: step 9740/40890 (epoch 8/30), loss = 0.476756 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:19:22.113043: step 9760/40890 (epoch 8/30), loss = 0.429151 (0.766 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:19:37.722790: step 9780/40890 (epoch 8/30), loss = 0.477086 (0.839 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:19:53.543070: step 9800/40890 (epoch 8/30), loss = 0.294287 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:20:09.638033: step 9820/40890 (epoch 8/30), loss = 0.346562 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:20:25.324089: step 9840/40890 (epoch 8/30), loss = 0.386776 (0.686 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:20:40.674044: step 9860/40890 (epoch 8/30), loss = 0.422075 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:20:56.245408: step 9880/40890 (epoch 8/30), loss = 0.378456 (0.647 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:21:11.971356: step 9900/40890 (epoch 8/30), loss = 0.292377 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:21:27.249504: step 9920/40890 (epoch 8/30), loss = 0.316531 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:21:42.478630: step 9940/40890 (epoch 8/30), loss = 0.281410 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:21:57.819609: step 9960/40890 (epoch 8/30), loss = 0.290862 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:22:13.997351: step 9980/40890 (epoch 8/30), loss = 0.418532 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:22:30.130212: step 10000/40890 (epoch 8/30), loss = 0.145398 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:22:46.340866: step 10020/40890 (epoch 8/30), loss = 0.467594 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:23:02.215857: step 10040/40890 (epoch 8/30), loss = 0.195923 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:23:18.418245: step 10060/40890 (epoch 8/30), loss = 0.320311 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:23:34.753973: step 10080/40890 (epoch 8/30), loss = 0.327843 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:23:50.776750: step 10100/40890 (epoch 8/30), loss = 0.372301 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:24:07.428920: step 10120/40890 (epoch 8/30), loss = 0.352694 (0.991 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:24:24.398109: step 10140/40890 (epoch 8/30), loss = 0.341749 (0.789 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:24:39.924592: step 10160/40890 (epoch 8/30), loss = 0.373740 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:24:55.829064: step 10180/40890 (epoch 8/30), loss = 0.453754 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:25:11.505147: step 10200/40890 (epoch 8/30), loss = 0.253266 (0.838 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:25:27.780628: step 10220/40890 (epoch 8/30), loss = 0.294602 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:25:44.311185: step 10240/40890 (epoch 8/30), loss = 0.263405 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:26:00.560736: step 10260/40890 (epoch 8/30), loss = 0.294856 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:26:16.984818: step 10280/40890 (epoch 8/30), loss = 0.523621 (0.863 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:26:32.448470: step 10300/40890 (epoch 8/30), loss = 0.186289 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:26:48.393831: step 10320/40890 (epoch 8/30), loss = 0.457127 (0.643 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:27:05.032911: step 10340/40890 (epoch 8/30), loss = 0.304737 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:27:20.576121: step 10360/40890 (epoch 8/30), loss = 0.506759 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:27:36.541926: step 10380/40890 (epoch 8/30), loss = 0.417697 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:27:52.364617: step 10400/40890 (epoch 8/30), loss = 0.431522 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:28:07.768863: step 10420/40890 (epoch 8/30), loss = 0.558927 (0.817 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:28:23.582711: step 10440/40890 (epoch 8/30), loss = 0.263409 (0.677 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:28:39.638206: step 10460/40890 (epoch 8/30), loss = 0.205322 (0.788 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:28:55.297684: step 10480/40890 (epoch 8/30), loss = 0.475351 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:29:11.553230: step 10500/40890 (epoch 8/30), loss = 0.427641 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:29:27.402848: step 10520/40890 (epoch 8/30), loss = 0.217183 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:29:42.753743: step 10540/40890 (epoch 8/30), loss = 0.454225 (0.795 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:29:58.466726: step 10560/40890 (epoch 8/30), loss = 0.129152 (0.803 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:30:13.983236: step 10580/40890 (epoch 8/30), loss = 0.462423 (0.679 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:30:28.771693: step 10600/40890 (epoch 8/30), loss = 0.502125 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:30:44.193456: step 10620/40890 (epoch 8/30), loss = 0.420481 (0.814 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:30:59.705976: step 10640/40890 (epoch 8/30), loss = 0.343542 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:31:15.716165: step 10660/40890 (epoch 8/30), loss = 0.210761 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:31:31.411198: step 10680/40890 (epoch 8/30), loss = 0.192518 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:31:46.484707: step 10700/40890 (epoch 8/30), loss = 0.441236 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:32:01.660129: step 10720/40890 (epoch 8/30), loss = 0.256897 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:32:17.728343: step 10740/40890 (epoch 8/30), loss = 0.384082 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:32:33.978041: step 10760/40890 (epoch 8/30), loss = 0.236898 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:32:50.319346: step 10780/40890 (epoch 8/30), loss = 0.284314 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:33:06.476808: step 10800/40890 (epoch 8/30), loss = 0.446677 (0.802 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:33:21.780188: step 10820/40890 (epoch 8/30), loss = 0.268136 (0.722 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:33:38.107573: step 10840/40890 (epoch 8/30), loss = 0.293564 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:33:53.723816: step 10860/40890 (epoch 8/30), loss = 0.364574 (0.846 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:34:11.266321: step 10880/40890 (epoch 8/30), loss = 0.310967 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:34:29.118585: step 10900/40890 (epoch 8/30), loss = 0.258100 (0.782 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.263%\n",
      "   Recall (micro): 57.965%\n",
      "       F1 (micro): 62.694%\n",
      "epoch 8: train_loss = 0.342731, dev_loss = 0.441845, dev_f1 = 0.6269\n",
      "model saved to ./save_models/00/checkpoint_epoch_8.pt\n",
      "\n",
      "2020-10-11 15:35:18.363189: step 10920/40890 (epoch 9/30), loss = 0.266192 (0.785 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:35:33.739075: step 10940/40890 (epoch 9/30), loss = 0.450456 (0.659 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:35:49.884682: step 10960/40890 (epoch 9/30), loss = 0.095601 (0.681 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:36:05.882222: step 10980/40890 (epoch 9/30), loss = 0.472472 (0.843 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:36:23.124118: step 11000/40890 (epoch 9/30), loss = 0.216263 (0.674 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:36:38.668552: step 11020/40890 (epoch 9/30), loss = 0.338218 (0.705 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:36:54.586988: step 11040/40890 (epoch 9/30), loss = 0.479629 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:37:10.354174: step 11060/40890 (epoch 9/30), loss = 0.427800 (0.800 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:37:26.197683: step 11080/40890 (epoch 9/30), loss = 0.334658 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:37:41.996018: step 11100/40890 (epoch 9/30), loss = 0.246736 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:37:57.723658: step 11120/40890 (epoch 9/30), loss = 0.303477 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:38:13.346086: step 11140/40890 (epoch 9/30), loss = 0.292444 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:38:28.872811: step 11160/40890 (epoch 9/30), loss = 0.406200 (0.595 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:38:44.656067: step 11180/40890 (epoch 9/30), loss = 0.278751 (0.656 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:39:00.442503: step 11200/40890 (epoch 9/30), loss = 0.555708 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:39:15.385643: step 11220/40890 (epoch 9/30), loss = 0.350135 (0.694 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:39:31.114584: step 11240/40890 (epoch 9/30), loss = 0.318442 (0.630 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:39:46.743657: step 11260/40890 (epoch 9/30), loss = 0.501510 (0.786 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:40:02.102587: step 11280/40890 (epoch 9/30), loss = 0.139101 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:40:17.527343: step 11300/40890 (epoch 9/30), loss = 0.555553 (0.814 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:40:32.772577: step 11320/40890 (epoch 9/30), loss = 0.378826 (0.768 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:40:48.576319: step 11340/40890 (epoch 9/30), loss = 0.416449 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:41:04.315234: step 11360/40890 (epoch 9/30), loss = 0.243945 (0.708 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:41:20.225690: step 11380/40890 (epoch 9/30), loss = 0.203337 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:41:35.739331: step 11400/40890 (epoch 9/30), loss = 0.210545 (0.699 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:41:51.552180: step 11420/40890 (epoch 9/30), loss = 0.262844 (0.734 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:42:07.718950: step 11440/40890 (epoch 9/30), loss = 0.331340 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:42:23.733129: step 11460/40890 (epoch 9/30), loss = 0.637041 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:42:39.780043: step 11480/40890 (epoch 9/30), loss = 0.371244 (0.712 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:42:55.566300: step 11500/40890 (epoch 9/30), loss = 0.545870 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:43:11.233662: step 11520/40890 (epoch 9/30), loss = 0.320221 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:43:26.953930: step 11540/40890 (epoch 9/30), loss = 0.254836 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:43:42.537033: step 11560/40890 (epoch 9/30), loss = 0.421842 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:43:58.381919: step 11580/40890 (epoch 9/30), loss = 0.359586 (0.741 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:44:16.262108: step 11600/40890 (epoch 9/30), loss = 0.261507 (0.968 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:44:32.956469: step 11620/40890 (epoch 9/30), loss = 0.422978 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:44:49.405484: step 11640/40890 (epoch 9/30), loss = 0.305364 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:45:05.167338: step 11660/40890 (epoch 9/30), loss = 0.456064 (0.657 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:45:21.412898: step 11680/40890 (epoch 9/30), loss = 0.269938 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:45:37.813608: step 11700/40890 (epoch 9/30), loss = 0.382321 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:45:54.196632: step 11720/40890 (epoch 9/30), loss = 0.499482 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:46:09.967804: step 11740/40890 (epoch 9/30), loss = 0.254723 (0.686 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:46:26.559602: step 11760/40890 (epoch 9/30), loss = 0.359485 (0.912 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:46:42.023254: step 11780/40890 (epoch 9/30), loss = 0.410168 (0.894 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:46:58.282777: step 11800/40890 (epoch 9/30), loss = 0.266491 (0.787 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:47:14.238113: step 11820/40890 (epoch 9/30), loss = 0.469182 (0.696 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:47:29.625243: step 11840/40890 (epoch 9/30), loss = 0.268112 (0.851 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:47:45.741882: step 11860/40890 (epoch 9/30), loss = 0.237517 (0.762 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:48:01.727613: step 11880/40890 (epoch 9/30), loss = 0.384411 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:48:17.138509: step 11900/40890 (epoch 9/30), loss = 0.309616 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:48:32.870331: step 11920/40890 (epoch 9/30), loss = 0.249643 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:48:48.584419: step 11940/40890 (epoch 9/30), loss = 0.363075 (0.659 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:49:03.432715: step 11960/40890 (epoch 9/30), loss = 0.302850 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:49:18.647039: step 11980/40890 (epoch 9/30), loss = 0.448041 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:49:34.347896: step 12000/40890 (epoch 9/30), loss = 0.280169 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:49:49.921750: step 12020/40890 (epoch 9/30), loss = 0.194508 (0.803 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:50:05.839187: step 12040/40890 (epoch 9/30), loss = 0.203977 (0.762 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:50:21.134289: step 12060/40890 (epoch 9/30), loss = 0.478252 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:50:35.770153: step 12080/40890 (epoch 9/30), loss = 0.359268 (0.722 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:50:51.701553: step 12100/40890 (epoch 9/30), loss = 0.234202 (0.804 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:51:07.443460: step 12120/40890 (epoch 9/30), loss = 0.355338 (0.814 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:51:23.437068: step 12140/40890 (epoch 9/30), loss = 0.325027 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:51:39.348426: step 12160/40890 (epoch 9/30), loss = 0.337011 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:51:54.248584: step 12180/40890 (epoch 9/30), loss = 0.167227 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:52:10.179983: step 12200/40890 (epoch 9/30), loss = 0.393084 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:52:25.580910: step 12220/40890 (epoch 9/30), loss = 0.171474 (0.678 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:52:41.785852: step 12240/40890 (epoch 9/30), loss = 0.424334 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:52:57.922457: step 12260/40890 (epoch 9/30), loss = 0.347006 (0.841 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 69.483%\n",
      "   Recall (micro): 59.143%\n",
      "       F1 (micro): 63.897%\n",
      "epoch 9: train_loss = 0.334080, dev_loss = 0.437491, dev_f1 = 0.6390\n",
      "model saved to ./save_models/00/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 15:53:46.708511: step 12280/40890 (epoch 10/30), loss = 0.322208 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:54:02.978874: step 12300/40890 (epoch 10/30), loss = 0.402498 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:54:20.416710: step 12320/40890 (epoch 10/30), loss = 0.260068 (0.995 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:54:35.942196: step 12340/40890 (epoch 10/30), loss = 0.240585 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:54:52.650519: step 12360/40890 (epoch 10/30), loss = 0.206287 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:55:08.247812: step 12380/40890 (epoch 10/30), loss = 0.248550 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:55:24.315847: step 12400/40890 (epoch 10/30), loss = 0.486196 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:55:40.034368: step 12420/40890 (epoch 10/30), loss = 0.540062 (0.642 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:55:56.111538: step 12440/40890 (epoch 10/30), loss = 0.095945 (0.783 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:56:12.447481: step 12460/40890 (epoch 10/30), loss = 0.403996 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:56:28.653338: step 12480/40890 (epoch 10/30), loss = 0.246255 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:56:44.355352: step 12500/40890 (epoch 10/30), loss = 0.335381 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:57:00.280578: step 12520/40890 (epoch 10/30), loss = 0.302635 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:57:16.054404: step 12540/40890 (epoch 10/30), loss = 0.388297 (0.762 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:57:31.672642: step 12560/40890 (epoch 10/30), loss = 0.218958 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:57:46.653439: step 12580/40890 (epoch 10/30), loss = 0.561744 (0.708 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:58:02.350839: step 12600/40890 (epoch 10/30), loss = 0.341939 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:58:17.838189: step 12620/40890 (epoch 10/30), loss = 0.157529 (0.838 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:58:33.282890: step 12640/40890 (epoch 10/30), loss = 0.337578 (0.661 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:58:48.540955: step 12660/40890 (epoch 10/30), loss = 0.264647 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:59:03.855006: step 12680/40890 (epoch 10/30), loss = 0.158197 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:59:19.589932: step 12700/40890 (epoch 10/30), loss = 0.481681 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:59:35.327940: step 12720/40890 (epoch 10/30), loss = 0.195308 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-11 15:59:51.093183: step 12740/40890 (epoch 10/30), loss = 0.245864 (0.638 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:00:06.790281: step 12760/40890 (epoch 10/30), loss = 0.260148 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:00:22.622945: step 12780/40890 (epoch 10/30), loss = 0.486026 (0.911 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:00:38.440649: step 12800/40890 (epoch 10/30), loss = 0.492076 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:00:54.350108: step 12820/40890 (epoch 10/30), loss = 0.241606 (0.734 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:01:10.060101: step 12840/40890 (epoch 10/30), loss = 0.296526 (0.807 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:01:25.484856: step 12860/40890 (epoch 10/30), loss = 0.762486 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:01:40.412360: step 12880/40890 (epoch 10/30), loss = 0.402829 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:01:55.689385: step 12900/40890 (epoch 10/30), loss = 0.212458 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:02:11.448246: step 12920/40890 (epoch 10/30), loss = 0.290600 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:02:27.505310: step 12940/40890 (epoch 10/30), loss = 0.192571 (0.968 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:02:43.732918: step 12960/40890 (epoch 10/30), loss = 0.385934 (0.766 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:03:00.107238: step 12980/40890 (epoch 10/30), loss = 0.214625 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:03:16.740463: step 13000/40890 (epoch 10/30), loss = 0.487059 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:03:32.641943: step 13020/40890 (epoch 10/30), loss = 0.600936 (0.790 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:03:48.539573: step 13040/40890 (epoch 10/30), loss = 0.368115 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:04:05.363975: step 13060/40890 (epoch 10/30), loss = 0.446303 (0.954 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:04:23.886446: step 13080/40890 (epoch 10/30), loss = 0.249376 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:04:39.875692: step 13100/40890 (epoch 10/30), loss = 0.484979 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:04:56.360612: step 13120/40890 (epoch 10/30), loss = 0.394858 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:05:11.611832: step 13140/40890 (epoch 10/30), loss = 0.278476 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:05:28.067829: step 13160/40890 (epoch 10/30), loss = 0.258520 (0.695 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:05:44.490343: step 13180/40890 (epoch 10/30), loss = 0.196583 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:06:00.306053: step 13200/40890 (epoch 10/30), loss = 0.356766 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:06:16.804935: step 13220/40890 (epoch 10/30), loss = 0.576605 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:06:32.929818: step 13240/40890 (epoch 10/30), loss = 0.449753 (0.676 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:06:49.028770: step 13260/40890 (epoch 10/30), loss = 0.364408 (0.720 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:07:04.998130: step 13280/40890 (epoch 10/30), loss = 0.196877 (0.788 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:07:20.583087: step 13300/40890 (epoch 10/30), loss = 0.284129 (0.584 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:07:35.557708: step 13320/40890 (epoch 10/30), loss = 0.135616 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:07:50.607281: step 13340/40890 (epoch 10/30), loss = 0.442699 (0.768 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:08:06.498732: step 13360/40890 (epoch 10/30), loss = 0.370295 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:08:21.990007: step 13380/40890 (epoch 10/30), loss = 0.355115 (0.673 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:08:37.954319: step 13400/40890 (epoch 10/30), loss = 0.401527 (0.633 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:08:53.241443: step 13420/40890 (epoch 10/30), loss = 0.220508 (0.722 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:09:08.233355: step 13440/40890 (epoch 10/30), loss = 0.256327 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:09:23.914424: step 13460/40890 (epoch 10/30), loss = 0.430826 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:09:39.842991: step 13480/40890 (epoch 10/30), loss = 0.256940 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:09:55.911811: step 13500/40890 (epoch 10/30), loss = 0.306136 (0.852 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:10:11.789965: step 13520/40890 (epoch 10/30), loss = 0.193546 (0.698 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:10:27.475368: step 13540/40890 (epoch 10/30), loss = 0.459683 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:10:43.437167: step 13560/40890 (epoch 10/30), loss = 0.431318 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:10:59.071339: step 13580/40890 (epoch 10/30), loss = 0.166342 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:11:15.572126: step 13600/40890 (epoch 10/30), loss = 0.213529 (0.903 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:11:31.989141: step 13620/40890 (epoch 10/30), loss = 0.387844 (0.872 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.683%\n",
      "   Recall (micro): 58.904%\n",
      "       F1 (micro): 63.418%\n",
      "epoch 10: train_loss = 0.324872, dev_loss = 0.443024, dev_f1 = 0.6342\n",
      "model saved to ./save_models/00/checkpoint_epoch_10.pt\n",
      "\n",
      "2020-10-11 16:12:21.939851: step 13640/40890 (epoch 11/30), loss = 0.180140 (0.573 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:12:38.685074: step 13660/40890 (epoch 11/30), loss = 0.412238 (0.900 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:12:54.924194: step 13680/40890 (epoch 11/30), loss = 0.282430 (0.809 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:13:10.906398: step 13700/40890 (epoch 11/30), loss = 0.410595 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:13:28.355248: step 13720/40890 (epoch 11/30), loss = 0.206175 (0.909 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:13:45.047133: step 13740/40890 (epoch 11/30), loss = 0.218955 (0.649 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:14:02.020692: step 13760/40890 (epoch 11/30), loss = 0.413568 (1.132 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:14:19.943802: step 13780/40890 (epoch 11/30), loss = 0.298685 (0.609 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:14:38.201708: step 13800/40890 (epoch 11/30), loss = 0.508024 (1.101 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:14:56.814469: step 13820/40890 (epoch 11/30), loss = 0.397096 (1.058 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:15:14.959028: step 13840/40890 (epoch 11/30), loss = 0.398708 (1.026 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:15:31.504786: step 13860/40890 (epoch 11/30), loss = 0.260006 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:15:48.104095: step 13880/40890 (epoch 11/30), loss = 0.293026 (0.607 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:16:05.886620: step 13900/40890 (epoch 11/30), loss = 0.503971 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:16:22.531676: step 13920/40890 (epoch 11/30), loss = 0.392823 (0.966 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:16:38.727764: step 13940/40890 (epoch 11/30), loss = 0.333788 (0.983 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:16:56.394710: step 13960/40890 (epoch 11/30), loss = 0.305950 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:17:13.392315: step 13980/40890 (epoch 11/30), loss = 0.281232 (0.817 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:17:29.704215: step 14000/40890 (epoch 11/30), loss = 0.230138 (0.662 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:17:46.491327: step 14020/40890 (epoch 11/30), loss = 0.269623 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:18:04.914066: step 14040/40890 (epoch 11/30), loss = 0.232476 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:18:22.807335: step 14060/40890 (epoch 11/30), loss = 0.185851 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:18:41.350637: step 14080/40890 (epoch 11/30), loss = 0.277365 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:18:59.521050: step 14100/40890 (epoch 11/30), loss = 0.191080 (0.953 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:19:16.362018: step 14120/40890 (epoch 11/30), loss = 0.447754 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:19:34.219269: step 14140/40890 (epoch 11/30), loss = 0.150006 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:19:51.274673: step 14160/40890 (epoch 11/30), loss = 0.234575 (0.860 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:20:08.050825: step 14180/40890 (epoch 11/30), loss = 0.434330 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:20:25.165083: step 14200/40890 (epoch 11/30), loss = 0.393323 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:20:42.137699: step 14220/40890 (epoch 11/30), loss = 0.272220 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:20:58.033726: step 14240/40890 (epoch 11/30), loss = 0.320031 (0.941 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:21:14.202719: step 14260/40890 (epoch 11/30), loss = 0.260049 (0.826 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:21:30.422654: step 14280/40890 (epoch 11/30), loss = 0.192646 (0.848 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:21:48.670522: step 14300/40890 (epoch 11/30), loss = 0.198976 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:22:06.050987: step 14320/40890 (epoch 11/30), loss = 0.274377 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:22:24.590122: step 14340/40890 (epoch 11/30), loss = 0.453983 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:22:43.429887: step 14360/40890 (epoch 11/30), loss = 0.269339 (0.789 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:23:01.789683: step 14380/40890 (epoch 11/30), loss = 0.310544 (0.745 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:23:19.165858: step 14400/40890 (epoch 11/30), loss = 0.060075 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:23:37.379474: step 14420/40890 (epoch 11/30), loss = 0.239284 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:23:56.131851: step 14440/40890 (epoch 11/30), loss = 0.245646 (0.912 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:24:15.042120: step 14460/40890 (epoch 11/30), loss = 0.270528 (1.094 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:24:34.878611: step 14480/40890 (epoch 11/30), loss = 0.275572 (0.954 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:24:52.240165: step 14500/40890 (epoch 11/30), loss = 0.238284 (0.803 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:25:10.235763: step 14520/40890 (epoch 11/30), loss = 0.297296 (1.007 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:25:28.261523: step 14540/40890 (epoch 11/30), loss = 0.288359 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:25:45.890583: step 14560/40890 (epoch 11/30), loss = 0.330976 (0.889 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:26:03.887034: step 14580/40890 (epoch 11/30), loss = 0.137617 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:26:22.132727: step 14600/40890 (epoch 11/30), loss = 0.569715 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:26:39.617554: step 14620/40890 (epoch 11/30), loss = 0.234313 (1.120 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:26:58.738654: step 14640/40890 (epoch 11/30), loss = 0.304916 (0.979 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:27:16.041221: step 14660/40890 (epoch 11/30), loss = 0.165976 (0.632 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:27:32.157538: step 14680/40890 (epoch 11/30), loss = 0.289007 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:27:48.445615: step 14700/40890 (epoch 11/30), loss = 0.325534 (0.783 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:28:05.578456: step 14720/40890 (epoch 11/30), loss = 0.187169 (0.788 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:28:22.335699: step 14740/40890 (epoch 11/30), loss = 0.312229 (0.934 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:28:39.340143: step 14760/40890 (epoch 11/30), loss = 0.379226 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:28:55.543332: step 14780/40890 (epoch 11/30), loss = 0.229223 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:29:11.409317: step 14800/40890 (epoch 11/30), loss = 0.272369 (0.881 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:29:27.821528: step 14820/40890 (epoch 11/30), loss = 0.254527 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:29:45.192587: step 14840/40890 (epoch 11/30), loss = 0.205718 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:30:03.262920: step 14860/40890 (epoch 11/30), loss = 0.465034 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:30:20.611633: step 14880/40890 (epoch 11/30), loss = 0.284988 (0.961 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:30:36.929766: step 14900/40890 (epoch 11/30), loss = 0.187337 (0.713 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:30:54.031398: step 14920/40890 (epoch 11/30), loss = 0.181904 (0.657 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:31:10.763181: step 14940/40890 (epoch 11/30), loss = 0.317765 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:31:27.626305: step 14960/40890 (epoch 11/30), loss = 0.260424 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:31:44.384113: step 14980/40890 (epoch 11/30), loss = 0.228657 (0.755 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.052%\n",
      "   Recall (micro): 62.969%\n",
      "       F1 (micro): 65.412%\n",
      "epoch 11: train_loss = 0.317221, dev_loss = 0.435420, dev_f1 = 0.6541\n",
      "model saved to ./save_models/00/checkpoint_epoch_11.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 16:32:40.301426: step 15000/40890 (epoch 12/30), loss = 0.487865 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:32:57.822605: step 15020/40890 (epoch 12/30), loss = 0.228701 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:33:15.431837: step 15040/40890 (epoch 12/30), loss = 0.330949 (1.021 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:33:33.138888: step 15060/40890 (epoch 12/30), loss = 0.231793 (0.959 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:33:52.204280: step 15080/40890 (epoch 12/30), loss = 0.254357 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:34:11.129840: step 15100/40890 (epoch 12/30), loss = 0.197051 (1.074 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:34:29.835409: step 15120/40890 (epoch 12/30), loss = 0.256671 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:34:47.635008: step 15140/40890 (epoch 12/30), loss = 0.135249 (1.005 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:35:05.172053: step 15160/40890 (epoch 12/30), loss = 0.364608 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:35:24.292811: step 15180/40890 (epoch 12/30), loss = 0.229987 (0.937 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:35:41.764004: step 15200/40890 (epoch 12/30), loss = 0.159178 (0.944 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:35:58.967599: step 15220/40890 (epoch 12/30), loss = 0.363604 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:36:16.158308: step 15240/40890 (epoch 12/30), loss = 0.183854 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:36:34.693291: step 15260/40890 (epoch 12/30), loss = 0.294543 (1.076 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:36:53.843026: step 15280/40890 (epoch 12/30), loss = 0.373118 (1.019 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:37:10.919382: step 15300/40890 (epoch 12/30), loss = 0.290407 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:37:27.647382: step 15320/40890 (epoch 12/30), loss = 0.310250 (0.957 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:37:44.323813: step 15340/40890 (epoch 12/30), loss = 0.238393 (0.945 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:38:01.784819: step 15360/40890 (epoch 12/30), loss = 0.328693 (0.671 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:38:19.151090: step 15380/40890 (epoch 12/30), loss = 0.224260 (0.966 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:38:35.572323: step 15400/40890 (epoch 12/30), loss = 0.274994 (0.937 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:38:51.972056: step 15420/40890 (epoch 12/30), loss = 0.243180 (0.920 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:39:09.818623: step 15440/40890 (epoch 12/30), loss = 0.211108 (0.903 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:39:26.786837: step 15460/40890 (epoch 12/30), loss = 0.416687 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:39:43.214267: step 15480/40890 (epoch 12/30), loss = 0.271428 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:39:59.709286: step 15500/40890 (epoch 12/30), loss = 0.186661 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:40:16.253737: step 15520/40890 (epoch 12/30), loss = 0.274975 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:40:33.292842: step 15540/40890 (epoch 12/30), loss = 0.247246 (0.867 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:40:49.919755: step 15560/40890 (epoch 12/30), loss = 0.550330 (0.899 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:41:06.542133: step 15580/40890 (epoch 12/30), loss = 0.120822 (0.918 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:41:22.674992: step 15600/40890 (epoch 12/30), loss = 0.562404 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:41:39.121551: step 15620/40890 (epoch 12/30), loss = 0.293644 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:41:55.604865: step 15640/40890 (epoch 12/30), loss = 0.226990 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:42:12.814857: step 15660/40890 (epoch 12/30), loss = 0.344084 (0.928 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:42:30.746484: step 15680/40890 (epoch 12/30), loss = 0.261537 (0.968 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:42:48.870727: step 15700/40890 (epoch 12/30), loss = 0.216444 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:43:06.871962: step 15720/40890 (epoch 12/30), loss = 0.452281 (0.932 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:43:24.571518: step 15740/40890 (epoch 12/30), loss = 0.409723 (0.716 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:43:41.641861: step 15760/40890 (epoch 12/30), loss = 0.215482 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:43:58.940367: step 15780/40890 (epoch 12/30), loss = 0.325783 (0.942 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:44:18.207500: step 15800/40890 (epoch 12/30), loss = 0.195367 (1.071 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:44:35.829024: step 15820/40890 (epoch 12/30), loss = 0.250991 (0.907 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:44:54.351679: step 15840/40890 (epoch 12/30), loss = 0.295065 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:45:10.898494: step 15860/40890 (epoch 12/30), loss = 0.383529 (0.963 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:45:28.775562: step 15880/40890 (epoch 12/30), loss = 0.305577 (0.974 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:45:46.982910: step 15900/40890 (epoch 12/30), loss = 0.318690 (0.978 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:46:04.842933: step 15920/40890 (epoch 12/30), loss = 0.388050 (0.621 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:46:23.151263: step 15940/40890 (epoch 12/30), loss = 0.543980 (0.963 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:46:41.791800: step 15960/40890 (epoch 12/30), loss = 0.352571 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:47:00.115916: step 15980/40890 (epoch 12/30), loss = 0.467862 (0.886 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:47:17.003197: step 16000/40890 (epoch 12/30), loss = 0.448041 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:47:34.159197: step 16020/40890 (epoch 12/30), loss = 0.357172 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:47:49.936338: step 16040/40890 (epoch 12/30), loss = 0.367545 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:48:06.402875: step 16060/40890 (epoch 12/30), loss = 0.290093 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:48:23.044117: step 16080/40890 (epoch 12/30), loss = 0.324523 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:48:39.566394: step 16100/40890 (epoch 12/30), loss = 0.434181 (0.949 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:48:56.394162: step 16120/40890 (epoch 12/30), loss = 0.254816 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:49:13.435895: step 16140/40890 (epoch 12/30), loss = 0.605089 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:49:30.392850: step 16160/40890 (epoch 12/30), loss = 0.279964 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:49:47.310496: step 16180/40890 (epoch 12/30), loss = 0.273367 (0.590 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:50:04.363727: step 16200/40890 (epoch 12/30), loss = 0.311416 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:50:21.352962: step 16220/40890 (epoch 12/30), loss = 0.193357 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:50:38.158387: step 16240/40890 (epoch 12/30), loss = 0.420386 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:50:54.438531: step 16260/40890 (epoch 12/30), loss = 0.218170 (0.624 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:51:10.727260: step 16280/40890 (epoch 12/30), loss = 0.261538 (0.683 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:51:27.587963: step 16300/40890 (epoch 12/30), loss = 0.138159 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:51:44.836932: step 16320/40890 (epoch 12/30), loss = 0.440249 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-11 16:52:02.749570: step 16340/40890 (epoch 12/30), loss = 0.294192 (0.921 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.514%\n",
      "   Recall (micro): 61.405%\n",
      "       F1 (micro): 64.765%\n",
      "epoch 12: train_loss = 0.309815, dev_loss = 0.432671, dev_f1 = 0.6477\n",
      "model saved to ./save_models/00/checkpoint_epoch_12.pt\n",
      "\n",
      "2020-10-11 16:53:00.011722: step 16360/40890 (epoch 13/30), loss = 0.250293 (0.699 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:53:17.518417: step 16380/40890 (epoch 13/30), loss = 0.513847 (0.846 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:53:34.722423: step 16400/40890 (epoch 13/30), loss = 0.368130 (0.841 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:53:51.941167: step 16420/40890 (epoch 13/30), loss = 0.293653 (0.743 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:54:10.297260: step 16440/40890 (epoch 13/30), loss = 0.366300 (0.937 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:54:28.489901: step 16460/40890 (epoch 13/30), loss = 0.280646 (0.829 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:54:45.300066: step 16480/40890 (epoch 13/30), loss = 0.287910 (0.783 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:55:02.379474: step 16500/40890 (epoch 13/30), loss = 0.822126 (0.790 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:55:19.572870: step 16520/40890 (epoch 13/30), loss = 0.195074 (0.914 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:55:36.683676: step 16540/40890 (epoch 13/30), loss = 0.396755 (0.897 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:55:53.448918: step 16560/40890 (epoch 13/30), loss = 0.158241 (0.646 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:56:09.988424: step 16580/40890 (epoch 13/30), loss = 0.536524 (0.647 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:56:26.918660: step 16600/40890 (epoch 13/30), loss = 0.292818 (0.869 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:56:43.623006: step 16620/40890 (epoch 13/30), loss = 0.125658 (0.855 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:57:00.374851: step 16640/40890 (epoch 13/30), loss = 0.215256 (0.757 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:57:16.796955: step 16660/40890 (epoch 13/30), loss = 0.555498 (0.773 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:57:32.718952: step 16680/40890 (epoch 13/30), loss = 0.284958 (0.690 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:57:48.689353: step 16700/40890 (epoch 13/30), loss = 0.219442 (0.848 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:58:05.497384: step 16720/40890 (epoch 13/30), loss = 0.219896 (0.944 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:58:21.559355: step 16740/40890 (epoch 13/30), loss = 0.354645 (0.847 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:58:37.114117: step 16760/40890 (epoch 13/30), loss = 0.196098 (0.560 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:58:53.182685: step 16780/40890 (epoch 13/30), loss = 0.088922 (0.668 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:59:10.013523: step 16800/40890 (epoch 13/30), loss = 0.516642 (0.688 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:59:26.705926: step 16820/40890 (epoch 13/30), loss = 0.346087 (0.803 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:59:43.041349: step 16840/40890 (epoch 13/30), loss = 0.351296 (0.718 sec/batch), lr: 0.900000\n",
      "2020-10-11 16:59:59.376147: step 16860/40890 (epoch 13/30), loss = 0.243732 (0.854 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:00:15.808506: step 16880/40890 (epoch 13/30), loss = 0.230083 (0.834 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:00:32.477618: step 16900/40890 (epoch 13/30), loss = 0.443452 (0.660 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:00:48.659929: step 16920/40890 (epoch 13/30), loss = 0.135369 (0.695 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:01:05.141777: step 16940/40890 (epoch 13/30), loss = 0.293372 (0.792 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:01:20.916209: step 16960/40890 (epoch 13/30), loss = 0.366911 (0.753 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:01:36.966948: step 16980/40890 (epoch 13/30), loss = 0.196640 (0.788 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:01:52.875434: step 17000/40890 (epoch 13/30), loss = 0.485594 (0.682 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:02:09.174412: step 17020/40890 (epoch 13/30), loss = 0.457654 (0.682 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:02:26.196446: step 17040/40890 (epoch 13/30), loss = 0.234146 (0.836 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:02:43.543593: step 17060/40890 (epoch 13/30), loss = 0.247547 (0.823 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:03:00.993950: step 17080/40890 (epoch 13/30), loss = 0.227714 (0.703 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:03:18.312687: step 17100/40890 (epoch 13/30), loss = 0.121989 (0.871 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:03:34.722988: step 17120/40890 (epoch 13/30), loss = 0.113716 (0.904 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:03:51.675336: step 17140/40890 (epoch 13/30), loss = 0.375325 (0.908 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:04:09.992772: step 17160/40890 (epoch 13/30), loss = 0.273125 (0.975 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:04:28.478703: step 17180/40890 (epoch 13/30), loss = 0.320503 (0.904 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:04:45.891734: step 17200/40890 (epoch 13/30), loss = 0.283395 (0.862 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:05:02.470181: step 17220/40890 (epoch 13/30), loss = 0.204945 (0.745 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:05:18.999050: step 17240/40890 (epoch 13/30), loss = 0.404297 (0.857 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:05:36.044482: step 17260/40890 (epoch 13/30), loss = 0.206266 (0.830 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:05:53.503356: step 17280/40890 (epoch 13/30), loss = 0.268591 (0.922 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:06:09.953210: step 17300/40890 (epoch 13/30), loss = 0.418064 (0.769 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:06:27.438011: step 17320/40890 (epoch 13/30), loss = 0.280438 (0.869 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:06:44.499025: step 17340/40890 (epoch 13/30), loss = 0.233617 (0.785 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:07:00.936097: step 17360/40890 (epoch 13/30), loss = 0.290995 (0.744 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:07:17.487471: step 17380/40890 (epoch 13/30), loss = 0.229204 (0.895 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:07:32.838702: step 17400/40890 (epoch 13/30), loss = 0.357811 (0.816 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:07:48.686351: step 17420/40890 (epoch 13/30), loss = 0.212903 (0.826 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:08:05.298440: step 17440/40890 (epoch 13/30), loss = 0.423302 (0.838 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:08:21.102814: step 17460/40890 (epoch 13/30), loss = 0.210870 (0.850 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:08:37.936910: step 17480/40890 (epoch 13/30), loss = 0.390416 (0.932 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:08:54.253335: step 17500/40890 (epoch 13/30), loss = 0.388895 (0.681 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:09:10.000509: step 17520/40890 (epoch 13/30), loss = 0.392713 (0.546 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:09:25.915678: step 17540/40890 (epoch 13/30), loss = 0.298292 (0.817 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:09:42.414742: step 17560/40890 (epoch 13/30), loss = 0.460874 (0.854 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:09:58.883391: step 17580/40890 (epoch 13/30), loss = 0.370909 (0.822 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:10:15.583511: step 17600/40890 (epoch 13/30), loss = 0.574812 (0.905 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:10:31.679171: step 17620/40890 (epoch 13/30), loss = 0.248499 (0.736 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:10:47.484501: step 17640/40890 (epoch 13/30), loss = 0.215695 (0.881 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:11:03.222506: step 17660/40890 (epoch 13/30), loss = 0.167088 (0.762 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:11:19.575575: step 17680/40890 (epoch 13/30), loss = 0.521533 (0.912 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:11:36.034110: step 17700/40890 (epoch 13/30), loss = 0.351135 (0.769 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 69.009%\n",
      "   Recall (micro): 62.141%\n",
      "       F1 (micro): 65.395%\n",
      "epoch 13: train_loss = 0.296551, dev_loss = 0.438863, dev_f1 = 0.6540\n",
      "model saved to ./save_models/00/checkpoint_epoch_13.pt\n",
      "\n",
      "2020-10-11 17:12:27.795595: step 17720/40890 (epoch 14/30), loss = 0.448748 (0.638 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:12:44.422215: step 17740/40890 (epoch 14/30), loss = 0.261802 (0.715 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:13:00.734690: step 17760/40890 (epoch 14/30), loss = 0.246461 (0.917 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:13:17.499313: step 17780/40890 (epoch 14/30), loss = 0.245828 (0.950 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:13:34.310593: step 17800/40890 (epoch 14/30), loss = 0.102095 (0.878 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:13:50.902257: step 17820/40890 (epoch 14/30), loss = 0.406413 (0.689 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:14:08.225596: step 17840/40890 (epoch 14/30), loss = 0.321871 (0.755 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:14:26.198387: step 17860/40890 (epoch 14/30), loss = 0.251758 (0.852 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:14:42.941463: step 17880/40890 (epoch 14/30), loss = 0.155944 (0.925 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:15:00.184539: step 17900/40890 (epoch 14/30), loss = 0.311180 (0.821 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:15:16.966037: step 17920/40890 (epoch 14/30), loss = 0.271846 (0.695 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:15:33.596219: step 17940/40890 (epoch 14/30), loss = 0.344649 (0.698 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:15:50.169115: step 17960/40890 (epoch 14/30), loss = 0.137815 (0.785 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:16:06.996874: step 17980/40890 (epoch 14/30), loss = 0.658958 (0.925 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:16:23.396249: step 18000/40890 (epoch 14/30), loss = 0.290869 (0.801 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:16:40.136091: step 18020/40890 (epoch 14/30), loss = 0.239202 (0.726 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:16:56.679770: step 18040/40890 (epoch 14/30), loss = 0.160961 (0.912 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:17:12.836257: step 18060/40890 (epoch 14/30), loss = 0.285119 (0.651 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:17:29.287194: step 18080/40890 (epoch 14/30), loss = 0.529188 (0.764 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:17:45.343495: step 18100/40890 (epoch 14/30), loss = 0.208327 (0.811 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:18:01.139829: step 18120/40890 (epoch 14/30), loss = 0.262033 (0.618 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:18:17.079611: step 18140/40890 (epoch 14/30), loss = 0.268349 (0.628 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:18:33.579411: step 18160/40890 (epoch 14/30), loss = 0.216064 (0.741 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:18:50.418885: step 18180/40890 (epoch 14/30), loss = 0.362878 (0.930 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:19:06.693431: step 18200/40890 (epoch 14/30), loss = 0.202031 (0.748 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:19:23.018541: step 18220/40890 (epoch 14/30), loss = 0.186475 (0.866 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:19:39.400483: step 18240/40890 (epoch 14/30), loss = 0.228904 (0.652 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:19:56.347201: step 18260/40890 (epoch 14/30), loss = 0.220695 (0.870 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:20:12.247713: step 18280/40890 (epoch 14/30), loss = 0.367298 (0.672 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:20:28.747079: step 18300/40890 (epoch 14/30), loss = 0.445882 (0.795 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:20:44.436049: step 18320/40890 (epoch 14/30), loss = 0.221919 (0.776 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:21:00.338617: step 18340/40890 (epoch 14/30), loss = 0.375950 (0.820 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:21:16.388946: step 18360/40890 (epoch 14/30), loss = 0.250036 (0.855 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:21:32.208216: step 18380/40890 (epoch 14/30), loss = 0.427449 (0.853 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:21:48.554844: step 18400/40890 (epoch 14/30), loss = 0.256370 (0.901 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:22:05.355976: step 18420/40890 (epoch 14/30), loss = 0.258887 (0.822 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:22:22.426440: step 18440/40890 (epoch 14/30), loss = 0.100220 (0.848 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:22:39.506889: step 18460/40890 (epoch 14/30), loss = 0.221841 (0.930 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:22:56.028627: step 18480/40890 (epoch 14/30), loss = 0.248873 (0.835 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:23:12.764010: step 18500/40890 (epoch 14/30), loss = 0.465036 (0.877 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:23:30.438895: step 18520/40890 (epoch 14/30), loss = 0.373015 (0.920 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:23:46.930356: step 18540/40890 (epoch 14/30), loss = 0.305456 (0.802 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:24:04.310477: step 18560/40890 (epoch 14/30), loss = 0.396776 (0.884 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:24:22.511393: step 18580/40890 (epoch 14/30), loss = 0.416235 (0.937 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:24:38.908145: step 18600/40890 (epoch 14/30), loss = 0.297113 (0.708 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:24:55.831330: step 18620/40890 (epoch 14/30), loss = 0.301951 (0.840 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:25:13.130796: step 18640/40890 (epoch 14/30), loss = 0.375793 (0.909 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:25:29.743256: step 18660/40890 (epoch 14/30), loss = 0.190782 (0.676 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:25:47.746227: step 18680/40890 (epoch 14/30), loss = 0.202777 (0.906 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:26:05.040343: step 18700/40890 (epoch 14/30), loss = 0.258926 (0.807 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:26:21.665159: step 18720/40890 (epoch 14/30), loss = 0.306665 (0.867 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:26:38.483488: step 18740/40890 (epoch 14/30), loss = 0.266355 (0.890 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:26:54.587999: step 18760/40890 (epoch 14/30), loss = 0.207845 (0.781 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:27:10.295785: step 18780/40890 (epoch 14/30), loss = 0.369298 (0.829 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:27:26.585454: step 18800/40890 (epoch 14/30), loss = 0.193743 (0.900 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:27:42.541102: step 18820/40890 (epoch 14/30), loss = 0.335132 (0.808 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:27:59.175213: step 18840/40890 (epoch 14/30), loss = 0.426606 (0.751 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:28:15.582799: step 18860/40890 (epoch 14/30), loss = 0.181382 (0.892 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:28:31.033489: step 18880/40890 (epoch 14/30), loss = 0.287388 (0.818 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:28:46.887367: step 18900/40890 (epoch 14/30), loss = 0.155605 (0.892 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:29:02.979325: step 18920/40890 (epoch 14/30), loss = 0.132282 (0.793 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:29:19.529960: step 18940/40890 (epoch 14/30), loss = 0.443529 (0.888 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:29:35.985454: step 18960/40890 (epoch 14/30), loss = 0.298647 (0.832 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:29:52.166370: step 18980/40890 (epoch 14/30), loss = 0.696062 (0.643 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:30:07.782000: step 19000/40890 (epoch 14/30), loss = 0.297464 (0.735 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:30:23.705280: step 19020/40890 (epoch 14/30), loss = 0.357248 (0.724 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:30:39.585918: step 19040/40890 (epoch 14/30), loss = 0.276672 (0.871 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:30:56.187355: step 19060/40890 (epoch 14/30), loss = 0.233839 (0.820 sec/batch), lr: 0.900000\n",
      "2020-10-11 17:31:12.817990: step 19080/40890 (epoch 14/30), loss = 0.349298 (0.769 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.368%\n",
      "   Recall (micro): 62.399%\n",
      "       F1 (micro): 64.788%\n",
      "epoch 14: train_loss = 0.292657, dev_loss = 0.446219, dev_f1 = 0.6479\n",
      "model saved to ./save_models/00/checkpoint_epoch_14.pt\n",
      "\n",
      "2020-10-11 17:32:03.343010: step 19100/40890 (epoch 15/30), loss = 0.283346 (0.911 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:32:19.349360: step 19120/40890 (epoch 15/30), loss = 0.367574 (0.794 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:32:35.894791: step 19140/40890 (epoch 15/30), loss = 0.252319 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:32:52.591125: step 19160/40890 (epoch 15/30), loss = 0.215440 (0.897 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:33:09.227208: step 19180/40890 (epoch 15/30), loss = 0.254961 (0.738 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:33:25.627248: step 19200/40890 (epoch 15/30), loss = 0.383770 (0.864 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:33:42.460987: step 19220/40890 (epoch 15/30), loss = 0.391466 (0.839 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:33:59.048507: step 19240/40890 (epoch 15/30), loss = 0.345387 (0.721 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:34:18.199615: step 19260/40890 (epoch 15/30), loss = 0.268013 (1.032 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:34:35.755530: step 19280/40890 (epoch 15/30), loss = 0.319510 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:34:52.606493: step 19300/40890 (epoch 15/30), loss = 0.247335 (0.818 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:35:08.922698: step 19320/40890 (epoch 15/30), loss = 0.244661 (0.779 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:35:25.823373: step 19340/40890 (epoch 15/30), loss = 0.386546 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:35:42.526184: step 19360/40890 (epoch 15/30), loss = 0.212324 (0.926 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:35:59.195337: step 19380/40890 (epoch 15/30), loss = 0.230663 (0.844 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:36:15.287356: step 19400/40890 (epoch 15/30), loss = 0.226798 (0.843 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:36:32.076599: step 19420/40890 (epoch 15/30), loss = 0.544990 (0.895 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:36:48.497941: step 19440/40890 (epoch 15/30), loss = 0.267112 (0.772 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:37:05.974234: step 19460/40890 (epoch 15/30), loss = 0.319259 (0.790 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:37:21.905561: step 19480/40890 (epoch 15/30), loss = 0.366974 (0.729 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:37:37.772921: step 19500/40890 (epoch 15/30), loss = 0.151161 (0.700 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:37:54.301230: step 19520/40890 (epoch 15/30), loss = 0.284815 (0.810 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:38:10.744604: step 19540/40890 (epoch 15/30), loss = 0.202682 (0.889 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:38:27.140045: step 19560/40890 (epoch 15/30), loss = 0.144179 (0.799 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:38:43.235518: step 19580/40890 (epoch 15/30), loss = 0.446103 (0.820 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:38:59.651411: step 19600/40890 (epoch 15/30), loss = 0.446211 (0.785 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:39:16.551037: step 19620/40890 (epoch 15/30), loss = 0.217063 (0.935 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:39:32.604247: step 19640/40890 (epoch 15/30), loss = 0.199340 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:39:48.881657: step 19660/40890 (epoch 15/30), loss = 0.152251 (0.845 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:40:04.655632: step 19680/40890 (epoch 15/30), loss = 0.256180 (0.867 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:40:20.272515: step 19700/40890 (epoch 15/30), loss = 0.255102 (0.713 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:40:36.447314: step 19720/40890 (epoch 15/30), loss = 0.288383 (0.858 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:40:52.132929: step 19740/40890 (epoch 15/30), loss = 0.090406 (0.814 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:41:08.364448: step 19760/40890 (epoch 15/30), loss = 0.148909 (0.887 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:41:25.021098: step 19780/40890 (epoch 15/30), loss = 0.317176 (0.661 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:41:41.494122: step 19800/40890 (epoch 15/30), loss = 0.176515 (0.758 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:41:58.077394: step 19820/40890 (epoch 15/30), loss = 0.360694 (0.813 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:42:14.498860: step 19840/40890 (epoch 15/30), loss = 0.341385 (0.818 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:42:31.103754: step 19860/40890 (epoch 15/30), loss = 0.288119 (0.823 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:42:48.521616: step 19880/40890 (epoch 15/30), loss = 0.252604 (0.934 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:43:05.263347: step 19900/40890 (epoch 15/30), loss = 0.347439 (0.623 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:43:22.082217: step 19920/40890 (epoch 15/30), loss = 0.268072 (0.922 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:43:39.080948: step 19940/40890 (epoch 15/30), loss = 0.391140 (0.688 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:43:55.167525: step 19960/40890 (epoch 15/30), loss = 0.394729 (0.801 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:44:12.965405: step 19980/40890 (epoch 15/30), loss = 0.392566 (0.888 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:44:30.999559: step 20000/40890 (epoch 15/30), loss = 0.237608 (0.864 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:44:47.684050: step 20020/40890 (epoch 15/30), loss = 0.213956 (0.872 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:45:05.056273: step 20040/40890 (epoch 15/30), loss = 0.174169 (0.894 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:45:22.064072: step 20060/40890 (epoch 15/30), loss = 0.335863 (0.854 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:45:38.543398: step 20080/40890 (epoch 15/30), loss = 0.289779 (0.805 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:45:55.456944: step 20100/40890 (epoch 15/30), loss = 0.227292 (0.907 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:46:11.963853: step 20120/40890 (epoch 15/30), loss = 0.145042 (0.697 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:46:27.841968: step 20140/40890 (epoch 15/30), loss = 0.292987 (0.820 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:46:44.216617: step 20160/40890 (epoch 15/30), loss = 0.482038 (0.739 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:47:00.818512: step 20180/40890 (epoch 15/30), loss = 0.229457 (0.663 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:47:17.509415: step 20200/40890 (epoch 15/30), loss = 0.229627 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:47:33.690673: step 20220/40890 (epoch 15/30), loss = 0.329932 (0.782 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:47:49.613734: step 20240/40890 (epoch 15/30), loss = 0.339063 (0.737 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:48:05.826560: step 20260/40890 (epoch 15/30), loss = 0.256761 (0.938 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:48:22.742641: step 20280/40890 (epoch 15/30), loss = 0.298637 (0.802 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:48:39.594259: step 20300/40890 (epoch 15/30), loss = 0.187990 (0.946 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:48:56.333402: step 20320/40890 (epoch 15/30), loss = 0.208351 (0.832 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:49:12.831086: step 20340/40890 (epoch 15/30), loss = 0.256550 (0.870 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:49:28.704503: step 20360/40890 (epoch 15/30), loss = 0.199899 (0.828 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:49:44.892138: step 20380/40890 (epoch 15/30), loss = 0.400621 (0.714 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:50:00.762428: step 20400/40890 (epoch 15/30), loss = 0.301918 (0.879 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:50:17.583090: step 20420/40890 (epoch 15/30), loss = 0.142168 (0.923 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:50:34.201547: step 20440/40890 (epoch 15/30), loss = 0.322483 (0.884 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.351%\n",
      "   Recall (micro): 62.730%\n",
      "       F1 (micro): 64.959%\n",
      "epoch 15: train_loss = 0.282483, dev_loss = 0.446030, dev_f1 = 0.6496\n",
      "model saved to ./save_models/00/checkpoint_epoch_15.pt\n",
      "\n",
      "2020-10-11 17:51:24.716075: step 20460/40890 (epoch 16/30), loss = 0.202399 (0.838 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:51:40.779037: step 20480/40890 (epoch 16/30), loss = 0.280747 (0.718 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:51:57.220992: step 20500/40890 (epoch 16/30), loss = 0.220861 (0.817 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:52:13.569832: step 20520/40890 (epoch 16/30), loss = 0.278808 (0.994 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:52:30.872527: step 20540/40890 (epoch 16/30), loss = 0.193322 (0.801 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:52:47.359617: step 20560/40890 (epoch 16/30), loss = 0.179725 (0.751 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:53:04.268353: step 20580/40890 (epoch 16/30), loss = 0.282231 (0.739 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:53:21.038883: step 20600/40890 (epoch 16/30), loss = 0.271593 (0.888 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:53:38.040706: step 20620/40890 (epoch 16/30), loss = 0.278036 (0.832 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:53:55.211820: step 20640/40890 (epoch 16/30), loss = 0.301006 (0.890 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:54:13.495883: step 20660/40890 (epoch 16/30), loss = 0.304694 (0.761 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:54:30.940970: step 20680/40890 (epoch 16/30), loss = 0.412636 (0.875 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:54:48.129183: step 20700/40890 (epoch 16/30), loss = 0.241721 (0.883 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:55:04.971360: step 20720/40890 (epoch 16/30), loss = 0.226230 (0.767 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:55:22.106245: step 20740/40890 (epoch 16/30), loss = 0.404449 (0.686 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:55:39.209740: step 20760/40890 (epoch 16/30), loss = 0.251833 (0.720 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:55:56.734706: step 20780/40890 (epoch 16/30), loss = 0.264351 (0.922 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:56:13.846130: step 20800/40890 (epoch 16/30), loss = 0.167196 (0.996 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:56:30.841269: step 20820/40890 (epoch 16/30), loss = 0.118153 (0.737 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:56:47.917374: step 20840/40890 (epoch 16/30), loss = 0.149087 (0.771 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:57:04.907733: step 20860/40890 (epoch 16/30), loss = 0.202910 (0.758 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:57:21.618897: step 20880/40890 (epoch 16/30), loss = 0.248992 (0.789 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:57:38.592238: step 20900/40890 (epoch 16/30), loss = 0.188194 (0.885 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:57:55.149699: step 20920/40890 (epoch 16/30), loss = 0.360215 (0.814 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:58:11.826810: step 20940/40890 (epoch 16/30), loss = 0.283108 (0.849 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:58:28.596534: step 20960/40890 (epoch 16/30), loss = 0.162631 (0.890 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:58:45.444692: step 20980/40890 (epoch 16/30), loss = 0.310785 (0.900 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:59:02.115800: step 21000/40890 (epoch 16/30), loss = 0.223226 (0.859 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:59:19.007883: step 21020/40890 (epoch 16/30), loss = 0.229830 (0.775 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:59:35.249126: step 21040/40890 (epoch 16/30), loss = 0.469752 (0.599 sec/batch), lr: 0.810000\n",
      "2020-10-11 17:59:51.378778: step 21060/40890 (epoch 16/30), loss = 0.345946 (0.848 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:00:07.801655: step 21080/40890 (epoch 16/30), loss = 0.121406 (0.914 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:00:24.004935: step 21100/40890 (epoch 16/30), loss = 0.174253 (0.849 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:00:40.555225: step 21120/40890 (epoch 16/30), loss = 0.233823 (0.883 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:00:58.502189: step 21140/40890 (epoch 16/30), loss = 0.241173 (0.997 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:01:15.974808: step 21160/40890 (epoch 16/30), loss = 0.279117 (0.976 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:01:34.018418: step 21180/40890 (epoch 16/30), loss = 0.137465 (0.913 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:01:51.366401: step 21200/40890 (epoch 16/30), loss = 0.157860 (0.756 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:02:08.896499: step 21220/40890 (epoch 16/30), loss = 0.682700 (0.987 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:02:27.615030: step 21240/40890 (epoch 16/30), loss = 0.283111 (0.920 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:02:45.608046: step 21260/40890 (epoch 16/30), loss = 0.225802 (0.890 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:03:03.420892: step 21280/40890 (epoch 16/30), loss = 0.320630 (0.942 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:03:21.605942: step 21300/40890 (epoch 16/30), loss = 0.318224 (0.905 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:03:38.611260: step 21320/40890 (epoch 16/30), loss = 0.455660 (1.010 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:03:56.504548: step 21340/40890 (epoch 16/30), loss = 0.262814 (0.979 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:04:16.207728: step 21360/40890 (epoch 16/30), loss = 0.261392 (1.129 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:04:34.462407: step 21380/40890 (epoch 16/30), loss = 0.259397 (0.809 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:04:53.868653: step 21400/40890 (epoch 16/30), loss = 0.240667 (0.920 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:05:13.068769: step 21420/40890 (epoch 16/30), loss = 0.218525 (0.948 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:05:31.111990: step 21440/40890 (epoch 16/30), loss = 0.298572 (0.771 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:05:49.051809: step 21460/40890 (epoch 16/30), loss = 0.260890 (0.876 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:06:06.452833: step 21480/40890 (epoch 16/30), loss = 0.309211 (0.973 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:06:22.734978: step 21500/40890 (epoch 16/30), loss = 0.149210 (0.842 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:06:39.755938: step 21520/40890 (epoch 16/30), loss = 0.326988 (0.854 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:06:57.076078: step 21540/40890 (epoch 16/30), loss = 0.156780 (0.895 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:07:14.568445: step 21560/40890 (epoch 16/30), loss = 0.154875 (0.830 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:07:32.653826: step 21580/40890 (epoch 16/30), loss = 0.144882 (0.869 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:07:48.912270: step 21600/40890 (epoch 16/30), loss = 0.371151 (0.786 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:08:04.644960: step 21620/40890 (epoch 16/30), loss = 0.266290 (0.824 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:08:21.527612: step 21640/40890 (epoch 16/30), loss = 0.250951 (0.938 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:08:38.362948: step 21660/40890 (epoch 16/30), loss = 0.128656 (0.804 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:08:55.495128: step 21680/40890 (epoch 16/30), loss = 0.240344 (0.870 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:09:12.424914: step 21700/40890 (epoch 16/30), loss = 0.369521 (0.782 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:09:28.187033: step 21720/40890 (epoch 16/30), loss = 0.206015 (0.633 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:09:44.699398: step 21740/40890 (epoch 16/30), loss = 0.641960 (0.910 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:10:00.862597: step 21760/40890 (epoch 16/30), loss = 0.306919 (0.840 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:10:17.748768: step 21780/40890 (epoch 16/30), loss = 0.352945 (0.902 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:10:34.512569: step 21800/40890 (epoch 16/30), loss = 0.224982 (0.858 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.811%\n",
      "   Recall (micro): 63.208%\n",
      "       F1 (micro): 65.429%\n",
      "epoch 16: train_loss = 0.275425, dev_loss = 0.445317, dev_f1 = 0.6543\n",
      "model saved to ./save_models/00/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 18:11:26.409746: step 21820/40890 (epoch 17/30), loss = 0.150169 (0.851 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:11:42.993809: step 21840/40890 (epoch 17/30), loss = 0.177210 (0.732 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:11:59.492191: step 21860/40890 (epoch 17/30), loss = 0.420612 (0.754 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:12:15.764889: step 21880/40890 (epoch 17/30), loss = 0.215748 (0.919 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:12:33.114620: step 21900/40890 (epoch 17/30), loss = 0.342669 (0.700 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:12:49.809646: step 21920/40890 (epoch 17/30), loss = 0.223994 (0.792 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:13:06.853868: step 21940/40890 (epoch 17/30), loss = 0.223267 (0.831 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:13:23.821586: step 21960/40890 (epoch 17/30), loss = 0.168533 (0.729 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:13:40.939000: step 21980/40890 (epoch 17/30), loss = 0.356185 (0.731 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:13:58.528868: step 22000/40890 (epoch 17/30), loss = 0.227143 (0.837 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:14:17.653504: step 22020/40890 (epoch 17/30), loss = 0.157631 (0.987 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:14:35.492166: step 22040/40890 (epoch 17/30), loss = 0.278924 (0.948 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:14:52.290410: step 22060/40890 (epoch 17/30), loss = 0.415462 (0.722 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:15:09.482262: step 22080/40890 (epoch 17/30), loss = 0.285042 (0.914 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:15:26.538320: step 22100/40890 (epoch 17/30), loss = 0.266747 (0.806 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:15:42.934229: step 22120/40890 (epoch 17/30), loss = 0.446526 (0.871 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:15:59.568976: step 22140/40890 (epoch 17/30), loss = 0.306694 (0.965 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:16:16.332845: step 22160/40890 (epoch 17/30), loss = 0.241546 (0.903 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:16:33.536512: step 22180/40890 (epoch 17/30), loss = 0.333800 (0.723 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:16:50.441264: step 22200/40890 (epoch 17/30), loss = 0.060441 (0.858 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:17:07.605280: step 22220/40890 (epoch 17/30), loss = 0.113462 (0.894 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:17:24.296441: step 22240/40890 (epoch 17/30), loss = 0.301879 (0.888 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:17:40.979418: step 22260/40890 (epoch 17/30), loss = 0.142389 (0.809 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:17:57.609949: step 22280/40890 (epoch 17/30), loss = 0.597654 (0.835 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:18:14.044586: step 22300/40890 (epoch 17/30), loss = 0.181894 (0.815 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:18:30.691466: step 22320/40890 (epoch 17/30), loss = 0.211104 (0.919 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:18:47.869988: step 22340/40890 (epoch 17/30), loss = 0.236911 (0.880 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:19:05.093118: step 22360/40890 (epoch 17/30), loss = 0.134122 (0.903 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:19:21.785353: step 22380/40890 (epoch 17/30), loss = 0.299117 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:19:38.513936: step 22400/40890 (epoch 17/30), loss = 0.295552 (0.706 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:19:54.665857: step 22420/40890 (epoch 17/30), loss = 0.259299 (0.828 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:20:11.164082: step 22440/40890 (epoch 17/30), loss = 0.323103 (0.877 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:20:27.499614: step 22460/40890 (epoch 17/30), loss = 0.149770 (0.812 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:20:44.168672: step 22480/40890 (epoch 17/30), loss = 0.179153 (0.795 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:21:01.400839: step 22500/40890 (epoch 17/30), loss = 0.249572 (0.904 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:21:18.254088: step 22520/40890 (epoch 17/30), loss = 0.253545 (0.740 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:21:35.547029: step 22540/40890 (epoch 17/30), loss = 0.352092 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:21:52.278056: step 22560/40890 (epoch 17/30), loss = 0.207972 (0.804 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:22:08.918956: step 22580/40890 (epoch 17/30), loss = 0.331483 (0.887 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:22:26.961725: step 22600/40890 (epoch 17/30), loss = 0.258807 (0.871 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:22:45.162507: step 22620/40890 (epoch 17/30), loss = 0.198302 (0.781 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:23:02.751118: step 22640/40890 (epoch 17/30), loss = 0.372539 (0.985 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:23:20.849124: step 22660/40890 (epoch 17/30), loss = 0.234789 (0.693 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:23:38.236657: step 22680/40890 (epoch 17/30), loss = 0.123738 (0.863 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:23:56.783268: step 22700/40890 (epoch 17/30), loss = 0.350179 (0.919 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:24:15.940339: step 22720/40890 (epoch 17/30), loss = 0.141214 (0.874 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:24:34.602824: step 22740/40890 (epoch 17/30), loss = 0.235345 (0.954 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:24:52.484891: step 22760/40890 (epoch 17/30), loss = 0.176779 (0.933 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:25:09.918830: step 22780/40890 (epoch 17/30), loss = 0.197475 (0.843 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:25:27.102698: step 22800/40890 (epoch 17/30), loss = 0.171644 (0.813 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:25:44.208828: step 22820/40890 (epoch 17/30), loss = 0.281648 (0.746 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:26:01.951954: step 22840/40890 (epoch 17/30), loss = 0.246411 (0.750 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:26:18.440480: step 22860/40890 (epoch 17/30), loss = 0.148927 (0.861 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:26:35.207728: step 22880/40890 (epoch 17/30), loss = 0.268604 (0.793 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:26:52.524630: step 22900/40890 (epoch 17/30), loss = 0.363334 (0.946 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:27:09.706556: step 22920/40890 (epoch 17/30), loss = 0.235081 (0.727 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:27:26.592097: step 22940/40890 (epoch 17/30), loss = 0.314429 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:27:42.642306: step 22960/40890 (epoch 17/30), loss = 0.169332 (0.884 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:27:58.468771: step 22980/40890 (epoch 17/30), loss = 0.280450 (0.660 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:28:15.414329: step 23000/40890 (epoch 17/30), loss = 0.170214 (0.914 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:28:32.069992: step 23020/40890 (epoch 17/30), loss = 0.202606 (0.849 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:28:49.033511: step 23040/40890 (epoch 17/30), loss = 0.262346 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:29:06.166357: step 23060/40890 (epoch 17/30), loss = 0.166447 (0.786 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:29:21.502198: step 23080/40890 (epoch 17/30), loss = 0.427907 (0.804 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:29:37.977952: step 23100/40890 (epoch 17/30), loss = 0.258061 (0.815 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:29:54.054115: step 23120/40890 (epoch 17/30), loss = 0.312725 (0.818 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:30:10.603019: step 23140/40890 (epoch 17/30), loss = 0.282730 (0.707 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:30:26.922877: step 23160/40890 (epoch 17/30), loss = 0.135919 (0.813 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.025%\n",
      "   Recall (micro): 63.558%\n",
      "       F1 (micro): 65.716%\n",
      "epoch 17: train_loss = 0.267105, dev_loss = 0.444937, dev_f1 = 0.6572\n",
      "model saved to ./save_models/00/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 18:31:18.017529: step 23180/40890 (epoch 18/30), loss = 0.255611 (0.721 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:31:34.427371: step 23200/40890 (epoch 18/30), loss = 0.146613 (0.894 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:31:50.875446: step 23220/40890 (epoch 18/30), loss = 0.502259 (0.793 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:32:06.638560: step 23240/40890 (epoch 18/30), loss = 0.272061 (0.881 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:32:24.365395: step 23260/40890 (epoch 18/30), loss = 0.169227 (0.984 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:32:41.793838: step 23280/40890 (epoch 18/30), loss = 0.443010 (0.993 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:32:58.367863: step 23300/40890 (epoch 18/30), loss = 0.382356 (0.771 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:33:15.568508: step 23320/40890 (epoch 18/30), loss = 0.352880 (0.949 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:33:32.880239: step 23340/40890 (epoch 18/30), loss = 0.240371 (0.880 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:33:50.919706: step 23360/40890 (epoch 18/30), loss = 0.174006 (0.897 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:34:08.759510: step 23380/40890 (epoch 18/30), loss = 0.198990 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:34:27.814058: step 23400/40890 (epoch 18/30), loss = 0.176058 (0.782 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:34:45.413223: step 23420/40890 (epoch 18/30), loss = 0.234067 (0.785 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:35:03.206698: step 23440/40890 (epoch 18/30), loss = 0.174977 (0.834 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:35:20.324690: step 23460/40890 (epoch 18/30), loss = 0.369798 (0.864 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:35:36.675225: step 23480/40890 (epoch 18/30), loss = 0.423951 (0.599 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:35:53.636573: step 23500/40890 (epoch 18/30), loss = 0.194771 (0.879 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:36:10.644543: step 23520/40890 (epoch 18/30), loss = 0.315378 (0.932 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:36:27.980552: step 23540/40890 (epoch 18/30), loss = 0.258432 (0.930 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:36:44.330471: step 23560/40890 (epoch 18/30), loss = 0.370975 (0.773 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:37:00.771671: step 23580/40890 (epoch 18/30), loss = 0.220749 (0.942 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:37:17.829284: step 23600/40890 (epoch 18/30), loss = 0.180529 (0.903 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:37:34.930226: step 23620/40890 (epoch 18/30), loss = 0.211798 (0.811 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:37:51.950480: step 23640/40890 (epoch 18/30), loss = 0.442841 (0.878 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:38:08.252605: step 23660/40890 (epoch 18/30), loss = 0.280224 (0.653 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:38:24.747902: step 23680/40890 (epoch 18/30), loss = 0.362295 (0.762 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:38:41.581247: step 23700/40890 (epoch 18/30), loss = 0.293767 (0.923 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:38:58.273735: step 23720/40890 (epoch 18/30), loss = 0.307933 (0.624 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:39:14.914359: step 23740/40890 (epoch 18/30), loss = 0.266202 (0.890 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:39:31.245777: step 23760/40890 (epoch 18/30), loss = 0.310606 (0.807 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:39:46.769607: step 23780/40890 (epoch 18/30), loss = 0.252119 (0.645 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:40:03.088263: step 23800/40890 (epoch 18/30), loss = 0.230290 (0.707 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:40:19.004520: step 23820/40890 (epoch 18/30), loss = 0.310994 (0.778 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:40:35.454893: step 23840/40890 (epoch 18/30), loss = 0.137917 (0.884 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:40:52.105037: step 23860/40890 (epoch 18/30), loss = 0.198145 (0.694 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:41:08.955283: step 23880/40890 (epoch 18/30), loss = 0.324103 (0.716 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:41:25.860404: step 23900/40890 (epoch 18/30), loss = 0.221564 (0.753 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:41:42.789618: step 23920/40890 (epoch 18/30), loss = 0.179159 (0.884 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:41:58.792260: step 23940/40890 (epoch 18/30), loss = 0.167322 (0.838 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:42:16.133924: step 23960/40890 (epoch 18/30), loss = 0.290184 (0.673 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:42:33.811615: step 23980/40890 (epoch 18/30), loss = 0.261446 (0.678 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:42:50.993309: step 24000/40890 (epoch 18/30), loss = 0.301603 (0.978 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:43:08.758094: step 24020/40890 (epoch 18/30), loss = 0.409448 (0.901 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:43:25.532431: step 24040/40890 (epoch 18/30), loss = 0.235911 (0.728 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:43:43.019558: step 24060/40890 (epoch 18/30), loss = 0.734162 (0.837 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:44:00.803120: step 24080/40890 (epoch 18/30), loss = 0.445164 (1.123 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:44:18.801196: step 24100/40890 (epoch 18/30), loss = 0.215870 (0.885 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:44:36.711956: step 24120/40890 (epoch 18/30), loss = 0.241908 (0.960 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:44:54.470724: step 24140/40890 (epoch 18/30), loss = 0.263530 (0.846 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:45:11.085095: step 24160/40890 (epoch 18/30), loss = 0.296296 (0.809 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:45:28.483556: step 24180/40890 (epoch 18/30), loss = 0.270229 (0.986 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:45:46.199580: step 24200/40890 (epoch 18/30), loss = 0.400082 (0.853 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:46:02.536286: step 24220/40890 (epoch 18/30), loss = 0.163452 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:46:18.798928: step 24240/40890 (epoch 18/30), loss = 0.128465 (0.884 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:46:35.843905: step 24260/40890 (epoch 18/30), loss = 0.195717 (0.729 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:46:52.720002: step 24280/40890 (epoch 18/30), loss = 0.452331 (0.819 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:47:09.920465: step 24300/40890 (epoch 18/30), loss = 0.181640 (0.818 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:47:25.903436: step 24320/40890 (epoch 18/30), loss = 0.467085 (0.841 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:47:41.489303: step 24340/40890 (epoch 18/30), loss = 0.105570 (0.707 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:47:57.659469: step 24360/40890 (epoch 18/30), loss = 0.431227 (0.742 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:48:14.561240: step 24380/40890 (epoch 18/30), loss = 0.411344 (0.891 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:48:31.442129: step 24400/40890 (epoch 18/30), loss = 0.117778 (0.808 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:48:48.401846: step 24420/40890 (epoch 18/30), loss = 0.246274 (0.876 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:49:04.011345: step 24440/40890 (epoch 18/30), loss = 0.072993 (0.660 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:49:20.471851: step 24460/40890 (epoch 18/30), loss = 0.209622 (0.824 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:49:36.829019: step 24480/40890 (epoch 18/30), loss = 0.166789 (0.878 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:49:53.658084: step 24500/40890 (epoch 18/30), loss = 0.257673 (0.710 sec/batch), lr: 0.810000\n",
      "2020-10-11 18:50:10.183164: step 24520/40890 (epoch 18/30), loss = 0.159911 (0.803 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.934%\n",
      "   Recall (micro): 61.461%\n",
      "       F1 (micro): 64.535%\n",
      "epoch 18: train_loss = 0.264154, dev_loss = 0.456669, dev_f1 = 0.6454\n",
      "model saved to ./save_models/00/checkpoint_epoch_18.pt\n",
      "\n",
      "2020-10-11 18:51:02.001360: step 24540/40890 (epoch 19/30), loss = 0.216795 (0.975 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:51:18.168328: step 24560/40890 (epoch 19/30), loss = 0.227528 (0.671 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:51:34.570260: step 24580/40890 (epoch 19/30), loss = 0.331816 (0.849 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:51:50.523873: step 24600/40890 (epoch 19/30), loss = 0.180502 (0.839 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:52:07.259645: step 24620/40890 (epoch 19/30), loss = 0.226963 (0.851 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:52:24.039971: step 24640/40890 (epoch 19/30), loss = 0.244233 (0.863 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:52:40.724589: step 24660/40890 (epoch 19/30), loss = 0.317285 (0.780 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:52:58.735902: step 24680/40890 (epoch 19/30), loss = 0.335098 (0.806 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:53:15.735537: step 24700/40890 (epoch 19/30), loss = 0.232411 (0.833 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:53:32.754262: step 24720/40890 (epoch 19/30), loss = 0.425425 (0.820 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:53:49.724136: step 24740/40890 (epoch 19/30), loss = 0.173555 (0.787 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:54:06.979517: step 24760/40890 (epoch 19/30), loss = 0.208066 (0.801 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:54:25.920993: step 24780/40890 (epoch 19/30), loss = 0.072002 (0.901 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:54:42.733843: step 24800/40890 (epoch 19/30), loss = 0.337502 (0.902 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:54:59.292356: step 24820/40890 (epoch 19/30), loss = 0.267765 (0.844 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:55:16.186371: step 24840/40890 (epoch 19/30), loss = 0.270141 (0.884 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:55:32.820883: step 24860/40890 (epoch 19/30), loss = 0.346333 (0.848 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:55:49.631688: step 24880/40890 (epoch 19/30), loss = 0.162652 (0.892 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:56:07.365705: step 24900/40890 (epoch 19/30), loss = 0.282112 (0.967 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:56:24.194878: step 24920/40890 (epoch 19/30), loss = 0.122626 (0.693 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:56:40.498167: step 24940/40890 (epoch 19/30), loss = 0.121392 (0.807 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:56:57.421635: step 24960/40890 (epoch 19/30), loss = 0.437794 (0.876 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:57:14.816720: step 24980/40890 (epoch 19/30), loss = 0.379467 (0.817 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:57:31.660230: step 25000/40890 (epoch 19/30), loss = 0.246309 (0.877 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:57:47.813375: step 25020/40890 (epoch 19/30), loss = 0.278536 (0.643 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:58:04.074784: step 25040/40890 (epoch 19/30), loss = 0.530993 (0.639 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:58:20.505652: step 25060/40890 (epoch 19/30), loss = 0.214157 (0.854 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:58:37.503307: step 25080/40890 (epoch 19/30), loss = 0.279766 (0.843 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:58:54.594071: step 25100/40890 (epoch 19/30), loss = 0.120241 (0.950 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:59:10.725915: step 25120/40890 (epoch 19/30), loss = 0.351917 (0.813 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:59:26.536653: step 25140/40890 (epoch 19/30), loss = 0.496230 (0.719 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:59:42.551222: step 25160/40890 (epoch 19/30), loss = 0.094619 (0.733 sec/batch), lr: 0.729000\n",
      "2020-10-11 18:59:58.500358: step 25180/40890 (epoch 19/30), loss = 0.223304 (0.891 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:00:14.538880: step 25200/40890 (epoch 19/30), loss = 0.358003 (0.841 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:00:30.884021: step 25220/40890 (epoch 19/30), loss = 0.179595 (0.814 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:00:48.152887: step 25240/40890 (epoch 19/30), loss = 0.239847 (0.690 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:01:05.141269: step 25260/40890 (epoch 19/30), loss = 0.154737 (0.800 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:01:22.038760: step 25280/40890 (epoch 19/30), loss = 0.261302 (0.810 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:01:37.946290: step 25300/40890 (epoch 19/30), loss = 0.345445 (0.856 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:01:54.406861: step 25320/40890 (epoch 19/30), loss = 0.190758 (0.958 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:02:11.803566: step 25340/40890 (epoch 19/30), loss = 0.280373 (0.965 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:02:28.313027: step 25360/40890 (epoch 19/30), loss = 0.469629 (0.693 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:02:46.088881: step 25380/40890 (epoch 19/30), loss = 0.253454 (0.978 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:03:02.630350: step 25400/40890 (epoch 19/30), loss = 0.183714 (0.811 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:03:19.641464: step 25420/40890 (epoch 19/30), loss = 0.094149 (0.965 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:03:37.679663: step 25440/40890 (epoch 19/30), loss = 0.441681 (0.951 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:03:55.602684: step 25460/40890 (epoch 19/30), loss = 0.227542 (0.864 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:04:14.115572: step 25480/40890 (epoch 19/30), loss = 0.368324 (0.937 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:04:33.153070: step 25500/40890 (epoch 19/30), loss = 0.351112 (0.869 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:04:50.406600: step 25520/40890 (epoch 19/30), loss = 0.497962 (0.743 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:05:07.381259: step 25540/40890 (epoch 19/30), loss = 0.356123 (0.879 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:05:24.764496: step 25560/40890 (epoch 19/30), loss = 0.173585 (0.908 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:05:40.808854: step 25580/40890 (epoch 19/30), loss = 0.327331 (0.843 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:05:57.552905: step 25600/40890 (epoch 19/30), loss = 0.293219 (0.912 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:06:14.984002: step 25620/40890 (epoch 19/30), loss = 0.175830 (0.757 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:06:31.937344: step 25640/40890 (epoch 19/30), loss = 0.128008 (0.888 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:06:49.645313: step 25660/40890 (epoch 19/30), loss = 0.309312 (0.869 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:07:06.157662: step 25680/40890 (epoch 19/30), loss = 0.242178 (0.689 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:07:22.014108: step 25700/40890 (epoch 19/30), loss = 0.134020 (0.809 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:07:38.326680: step 25720/40890 (epoch 19/30), loss = 0.253227 (0.858 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:07:55.510422: step 25740/40890 (epoch 19/30), loss = 0.293393 (1.018 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:08:12.795638: step 25760/40890 (epoch 19/30), loss = 0.188822 (0.932 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:08:29.451289: step 25780/40890 (epoch 19/30), loss = 0.170319 (0.874 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:08:45.624647: step 25800/40890 (epoch 19/30), loss = 0.163435 (0.760 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:09:01.664197: step 25820/40890 (epoch 19/30), loss = 0.285172 (0.905 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:09:17.809318: step 25840/40890 (epoch 19/30), loss = 0.280989 (0.914 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:09:34.535191: step 25860/40890 (epoch 19/30), loss = 0.296682 (0.912 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:09:51.372201: step 25880/40890 (epoch 19/30), loss = 0.329217 (0.838 sec/batch), lr: 0.729000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.027%\n",
      "   Recall (micro): 61.957%\n",
      "       F1 (micro): 64.850%\n",
      "epoch 19: train_loss = 0.256365, dev_loss = 0.464478, dev_f1 = 0.6485\n",
      "model saved to ./save_models/00/checkpoint_epoch_19.pt\n",
      "\n",
      "2020-10-11 19:10:43.120099: step 25900/40890 (epoch 20/30), loss = 0.252088 (0.729 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:10:59.431115: step 25920/40890 (epoch 20/30), loss = 0.294996 (0.797 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:11:15.901102: step 25940/40890 (epoch 20/30), loss = 0.472107 (0.928 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:11:32.301476: step 25960/40890 (epoch 20/30), loss = 0.283478 (0.677 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:11:49.374315: step 25980/40890 (epoch 20/30), loss = 0.303740 (0.860 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:12:05.690222: step 26000/40890 (epoch 20/30), loss = 0.138378 (0.751 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:12:22.660675: step 26020/40890 (epoch 20/30), loss = 0.157969 (0.902 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:12:39.948127: step 26040/40890 (epoch 20/30), loss = 0.354428 (0.879 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:12:56.887536: step 26060/40890 (epoch 20/30), loss = 0.314923 (0.827 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:13:14.181784: step 26080/40890 (epoch 20/30), loss = 0.174554 (0.746 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:13:31.429204: step 26100/40890 (epoch 20/30), loss = 0.321594 (0.948 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:13:48.632537: step 26120/40890 (epoch 20/30), loss = 0.306551 (0.807 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:14:06.517710: step 26140/40890 (epoch 20/30), loss = 0.270709 (0.935 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:14:25.286469: step 26160/40890 (epoch 20/30), loss = 0.058334 (0.727 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:14:42.374958: step 26180/40890 (epoch 20/30), loss = 0.214271 (0.934 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:14:59.728303: step 26200/40890 (epoch 20/30), loss = 0.447100 (0.882 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:15:16.752447: step 26220/40890 (epoch 20/30), loss = 0.329476 (0.762 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:15:33.686698: step 26240/40890 (epoch 20/30), loss = 0.153145 (0.781 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:15:50.918159: step 26260/40890 (epoch 20/30), loss = 0.241757 (0.773 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:16:07.607065: step 26280/40890 (epoch 20/30), loss = 0.240279 (0.877 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:16:23.861089: step 26300/40890 (epoch 20/30), loss = 0.121056 (0.777 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:16:40.781541: step 26320/40890 (epoch 20/30), loss = 0.431295 (0.811 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:16:58.570655: step 26340/40890 (epoch 20/30), loss = 0.170459 (0.936 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:17:15.276451: step 26360/40890 (epoch 20/30), loss = 0.376809 (0.748 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:17:31.574699: step 26380/40890 (epoch 20/30), loss = 0.186410 (0.894 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:17:47.730487: step 26400/40890 (epoch 20/30), loss = 0.164440 (0.851 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:18:04.176717: step 26420/40890 (epoch 20/30), loss = 0.286350 (0.791 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:18:20.979238: step 26440/40890 (epoch 20/30), loss = 0.265737 (0.802 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:18:36.969383: step 26460/40890 (epoch 20/30), loss = 0.206744 (0.769 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:18:53.146029: step 26480/40890 (epoch 20/30), loss = 0.161697 (0.604 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:19:08.966032: step 26500/40890 (epoch 20/30), loss = 0.237377 (0.813 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:19:24.835395: step 26520/40890 (epoch 20/30), loss = 0.285838 (0.847 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:19:40.676514: step 26540/40890 (epoch 20/30), loss = 0.231212 (0.743 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:19:56.701480: step 26560/40890 (epoch 20/30), loss = 0.356372 (0.788 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:20:12.958773: step 26580/40890 (epoch 20/30), loss = 0.151219 (0.822 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:20:29.694067: step 26600/40890 (epoch 20/30), loss = 0.182843 (0.910 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:20:46.600919: step 26620/40890 (epoch 20/30), loss = 0.101967 (0.916 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:21:03.046430: step 26640/40890 (epoch 20/30), loss = 0.227258 (0.844 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:21:18.877464: step 26660/40890 (epoch 20/30), loss = 0.219147 (0.846 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:21:35.251748: step 26680/40890 (epoch 20/30), loss = 0.223419 (0.826 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:21:52.484658: step 26700/40890 (epoch 20/30), loss = 0.240055 (0.935 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:22:08.394865: step 26720/40890 (epoch 20/30), loss = 0.321668 (0.894 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:22:25.532250: step 26740/40890 (epoch 20/30), loss = 0.500370 (0.909 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:22:42.012688: step 26760/40890 (epoch 20/30), loss = 0.268148 (0.738 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:22:58.276191: step 26780/40890 (epoch 20/30), loss = 0.194359 (0.718 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:23:15.184755: step 26800/40890 (epoch 20/30), loss = 0.141875 (0.859 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:23:32.419680: step 26820/40890 (epoch 20/30), loss = 0.079223 (0.939 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:23:48.952229: step 26840/40890 (epoch 20/30), loss = 0.414060 (0.955 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:24:06.554124: step 26860/40890 (epoch 20/30), loss = 0.170358 (0.683 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:24:25.138754: step 26880/40890 (epoch 20/30), loss = 0.262845 (0.860 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:24:41.651420: step 26900/40890 (epoch 20/30), loss = 0.225167 (0.737 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:24:58.494287: step 26920/40890 (epoch 20/30), loss = 0.185785 (0.911 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:25:14.444568: step 26940/40890 (epoch 20/30), loss = 0.186428 (0.746 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:25:30.690603: step 26960/40890 (epoch 20/30), loss = 0.347784 (0.868 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:25:47.582665: step 26980/40890 (epoch 20/30), loss = 0.247756 (0.944 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:26:03.870896: step 27000/40890 (epoch 20/30), loss = 0.315651 (0.751 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:26:20.934096: step 27020/40890 (epoch 20/30), loss = 0.124077 (0.797 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:26:37.500293: step 27040/40890 (epoch 20/30), loss = 0.282740 (0.721 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:26:53.686229: step 27060/40890 (epoch 20/30), loss = 0.070780 (0.790 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:27:09.721845: step 27080/40890 (epoch 20/30), loss = 0.377567 (0.602 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:27:26.070731: step 27100/40890 (epoch 20/30), loss = 0.223353 (0.824 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:27:42.639038: step 27120/40890 (epoch 20/30), loss = 0.304990 (0.830 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:27:59.246930: step 27140/40890 (epoch 20/30), loss = 0.371773 (0.696 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:28:15.508189: step 27160/40890 (epoch 20/30), loss = 0.249764 (0.818 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:28:30.920033: step 27180/40890 (epoch 20/30), loss = 0.241126 (0.745 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:28:46.740087: step 27200/40890 (epoch 20/30), loss = 0.349247 (0.705 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:29:02.879700: step 27220/40890 (epoch 20/30), loss = 0.173051 (0.806 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:29:19.423902: step 27240/40890 (epoch 20/30), loss = 0.171474 (0.870 sec/batch), lr: 0.729000\n",
      "2020-10-11 19:29:35.330711: step 27260/40890 (epoch 20/30), loss = 0.016979 (0.439 sec/batch), lr: 0.729000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.832%\n",
      "   Recall (micro): 59.842%\n",
      "       F1 (micro): 64.023%\n",
      "epoch 20: train_loss = 0.249243, dev_loss = 0.464919, dev_f1 = 0.6402\n",
      "model saved to ./save_models/00/checkpoint_epoch_20.pt\n",
      "\n",
      "2020-10-11 19:30:25.788150: step 27280/40890 (epoch 21/30), loss = 0.195015 (0.809 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:30:41.231524: step 27300/40890 (epoch 21/30), loss = 0.111283 (0.798 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:30:57.394049: step 27320/40890 (epoch 21/30), loss = 0.179453 (0.756 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:31:13.664626: step 27340/40890 (epoch 21/30), loss = 0.120155 (0.793 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:31:29.826545: step 27360/40890 (epoch 21/30), loss = 0.091503 (0.846 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:31:45.713009: step 27380/40890 (epoch 21/30), loss = 0.281590 (0.906 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:32:01.634770: step 27400/40890 (epoch 21/30), loss = 0.335904 (0.892 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:32:18.121249: step 27420/40890 (epoch 21/30), loss = 0.117367 (0.936 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:32:35.056686: step 27440/40890 (epoch 21/30), loss = 0.090561 (0.852 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:32:51.829484: step 27460/40890 (epoch 21/30), loss = 0.263421 (0.894 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:33:08.453407: step 27480/40890 (epoch 21/30), loss = 0.246772 (0.733 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:33:25.010717: step 27500/40890 (epoch 21/30), loss = 0.186712 (0.681 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:33:42.075012: step 27520/40890 (epoch 21/30), loss = 0.202199 (0.732 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:33:58.587764: step 27540/40890 (epoch 21/30), loss = 0.149798 (0.689 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:34:17.413312: step 27560/40890 (epoch 21/30), loss = 0.270690 (1.113 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:34:34.552514: step 27580/40890 (epoch 21/30), loss = 0.135840 (0.838 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:34:51.031660: step 27600/40890 (epoch 21/30), loss = 0.246969 (0.691 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:35:07.801983: step 27620/40890 (epoch 21/30), loss = 0.654139 (0.762 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:35:24.377586: step 27640/40890 (epoch 21/30), loss = 0.169358 (0.866 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:35:40.775737: step 27660/40890 (epoch 21/30), loss = 0.356450 (0.912 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:35:56.964491: step 27680/40890 (epoch 21/30), loss = 0.072199 (0.863 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:36:13.817131: step 27700/40890 (epoch 21/30), loss = 0.388841 (0.834 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:36:30.766786: step 27720/40890 (epoch 21/30), loss = 0.352296 (0.949 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:36:47.743278: step 27740/40890 (epoch 21/30), loss = 0.199687 (0.964 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:37:04.156478: step 27760/40890 (epoch 21/30), loss = 0.350557 (0.831 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:37:20.849836: step 27780/40890 (epoch 21/30), loss = 0.361930 (0.929 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:37:38.545952: step 27800/40890 (epoch 21/30), loss = 0.267319 (0.868 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:37:54.549283: step 27820/40890 (epoch 21/30), loss = 0.432939 (0.778 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:38:10.924368: step 27840/40890 (epoch 21/30), loss = 0.211281 (0.757 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:38:26.511410: step 27860/40890 (epoch 21/30), loss = 0.323367 (0.723 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:38:42.235663: step 27880/40890 (epoch 21/30), loss = 0.274278 (0.750 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:38:58.222551: step 27900/40890 (epoch 21/30), loss = 0.288359 (0.619 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:39:13.965706: step 27920/40890 (epoch 21/30), loss = 0.091423 (0.704 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:39:30.280485: step 27940/40890 (epoch 21/30), loss = 0.309694 (0.759 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:39:46.876126: step 27960/40890 (epoch 21/30), loss = 0.344188 (0.840 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:40:03.428809: step 27980/40890 (epoch 21/30), loss = 0.228008 (0.829 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:40:19.803964: step 28000/40890 (epoch 21/30), loss = 0.158964 (0.665 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:40:35.815322: step 28020/40890 (epoch 21/30), loss = 0.369510 (0.971 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:40:51.928646: step 28040/40890 (epoch 21/30), loss = 0.199279 (0.940 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:41:08.875179: step 28060/40890 (epoch 21/30), loss = 0.474995 (0.726 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:41:25.005366: step 28080/40890 (epoch 21/30), loss = 0.274900 (0.923 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:41:41.343863: step 28100/40890 (epoch 21/30), loss = 0.318838 (0.795 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:41:57.656277: step 28120/40890 (epoch 21/30), loss = 0.280892 (0.789 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:42:13.675914: step 28140/40890 (epoch 21/30), loss = 0.142856 (0.924 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:42:30.190494: step 28160/40890 (epoch 21/30), loss = 0.195494 (0.735 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:42:47.303605: step 28180/40890 (epoch 21/30), loss = 0.288222 (0.767 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:43:04.040551: step 28200/40890 (epoch 21/30), loss = 0.100626 (0.889 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:43:21.281450: step 28220/40890 (epoch 21/30), loss = 0.334781 (0.723 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:43:38.074613: step 28240/40890 (epoch 21/30), loss = 0.281671 (0.725 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:43:54.601356: step 28260/40890 (epoch 21/30), loss = 0.305143 (0.919 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:44:12.229570: step 28280/40890 (epoch 21/30), loss = 0.182609 (0.944 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:44:29.502035: step 28300/40890 (epoch 21/30), loss = 0.320832 (0.713 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:44:45.430370: step 28320/40890 (epoch 21/30), loss = 0.351221 (0.647 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:45:02.033795: step 28340/40890 (epoch 21/30), loss = 0.304519 (0.826 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:45:18.663162: step 28360/40890 (epoch 21/30), loss = 0.291731 (0.893 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:45:35.599066: step 28380/40890 (epoch 21/30), loss = 0.372323 (0.813 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:45:52.178365: step 28400/40890 (epoch 21/30), loss = 0.142104 (0.737 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:46:08.249078: step 28420/40890 (epoch 21/30), loss = 0.167543 (0.738 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:46:24.429297: step 28440/40890 (epoch 21/30), loss = 0.192078 (0.899 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:46:41.003972: step 28460/40890 (epoch 21/30), loss = 0.167142 (0.832 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:46:57.871318: step 28480/40890 (epoch 21/30), loss = 0.302674 (0.824 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:47:14.702728: step 28500/40890 (epoch 21/30), loss = 0.180867 (0.831 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:47:31.045455: step 28520/40890 (epoch 21/30), loss = 0.502263 (0.901 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:47:46.507745: step 28540/40890 (epoch 21/30), loss = 0.048222 (0.863 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:48:02.792355: step 28560/40890 (epoch 21/30), loss = 0.274612 (0.632 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:48:18.569478: step 28580/40890 (epoch 21/30), loss = 0.106999 (0.697 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:48:34.970877: step 28600/40890 (epoch 21/30), loss = 0.191293 (0.745 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:48:51.727557: step 28620/40890 (epoch 21/30), loss = 0.102613 (0.841 sec/batch), lr: 0.656100\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.793%\n",
      "   Recall (micro): 64.606%\n",
      "       F1 (micro): 65.682%\n",
      "epoch 21: train_loss = 0.240092, dev_loss = 0.468049, dev_f1 = 0.6568\n",
      "model saved to ./save_models/00/checkpoint_epoch_21.pt\n",
      "\n",
      "2020-10-11 19:49:41.794242: step 28640/40890 (epoch 22/30), loss = 0.170320 (0.892 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:49:57.498573: step 28660/40890 (epoch 22/30), loss = 0.131644 (0.925 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:50:13.598182: step 28680/40890 (epoch 22/30), loss = 0.313993 (0.752 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:50:29.785301: step 28700/40890 (epoch 22/30), loss = 0.249303 (0.872 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:50:45.990732: step 28720/40890 (epoch 22/30), loss = 0.320927 (0.746 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:51:01.834363: step 28740/40890 (epoch 22/30), loss = 0.098627 (0.832 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:51:17.940972: step 28760/40890 (epoch 22/30), loss = 0.394923 (0.779 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:51:34.113942: step 28780/40890 (epoch 22/30), loss = 0.354871 (0.767 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:51:50.608029: step 28800/40890 (epoch 22/30), loss = 0.305396 (0.869 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:52:06.954635: step 28820/40890 (epoch 22/30), loss = 0.309695 (0.859 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:52:23.595498: step 28840/40890 (epoch 22/30), loss = 0.105161 (0.933 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:52:39.855688: step 28860/40890 (epoch 22/30), loss = 0.276809 (0.916 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:52:56.429793: step 28880/40890 (epoch 22/30), loss = 0.210112 (0.882 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:53:13.033681: step 28900/40890 (epoch 22/30), loss = 0.230886 (0.814 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:53:29.778534: step 28920/40890 (epoch 22/30), loss = 0.402956 (0.672 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:53:45.844880: step 28940/40890 (epoch 22/30), loss = 0.169100 (0.817 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:54:03.032736: step 28960/40890 (epoch 22/30), loss = 0.103315 (0.881 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:54:21.120265: step 28980/40890 (epoch 22/30), loss = 0.170281 (0.884 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:54:37.657076: step 29000/40890 (epoch 22/30), loss = 0.107523 (0.843 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:54:54.107450: step 29020/40890 (epoch 22/30), loss = 0.201168 (0.766 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:55:10.463667: step 29040/40890 (epoch 22/30), loss = 0.174703 (0.902 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:55:27.437001: step 29060/40890 (epoch 22/30), loss = 0.305852 (0.900 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:55:44.154904: step 29080/40890 (epoch 22/30), loss = 0.197738 (0.906 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:56:00.960156: step 29100/40890 (epoch 22/30), loss = 0.542711 (0.883 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:56:17.525746: step 29120/40890 (epoch 22/30), loss = 0.278442 (0.783 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:56:34.464953: step 29140/40890 (epoch 22/30), loss = 0.354816 (0.838 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:56:51.503163: step 29160/40890 (epoch 22/30), loss = 0.244020 (0.910 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:57:08.029915: step 29180/40890 (epoch 22/30), loss = 0.366960 (0.808 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:57:24.430294: step 29200/40890 (epoch 22/30), loss = 0.201400 (0.886 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:57:40.032929: step 29220/40890 (epoch 22/30), loss = 0.289595 (0.746 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:57:55.804071: step 29240/40890 (epoch 22/30), loss = 0.288046 (0.689 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:58:11.919459: step 29260/40890 (epoch 22/30), loss = 0.156505 (0.805 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:58:27.662027: step 29280/40890 (epoch 22/30), loss = 0.211284 (0.790 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:58:43.710373: step 29300/40890 (epoch 22/30), loss = 0.340617 (0.812 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:59:00.639850: step 29320/40890 (epoch 22/30), loss = 0.499245 (0.865 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:59:16.950654: step 29340/40890 (epoch 22/30), loss = 0.253628 (0.860 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:59:33.519451: step 29360/40890 (epoch 22/30), loss = 0.244673 (0.866 sec/batch), lr: 0.656100\n",
      "2020-10-11 19:59:49.408728: step 29380/40890 (epoch 22/30), loss = 0.404857 (0.812 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:00:05.568442: step 29400/40890 (epoch 22/30), loss = 0.131622 (0.760 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:00:22.378251: step 29420/40890 (epoch 22/30), loss = 0.198491 (0.869 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:00:38.697283: step 29440/40890 (epoch 22/30), loss = 0.170287 (0.713 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:00:54.639971: step 29460/40890 (epoch 22/30), loss = 0.128304 (0.829 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:01:11.315228: step 29480/40890 (epoch 22/30), loss = 0.205202 (0.747 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:01:26.762303: step 29500/40890 (epoch 22/30), loss = 0.202895 (0.738 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:01:43.202265: step 29520/40890 (epoch 22/30), loss = 0.154967 (0.764 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:01:59.719723: step 29540/40890 (epoch 22/30), loss = 0.175336 (0.773 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:02:15.896700: step 29560/40890 (epoch 22/30), loss = 0.136590 (0.829 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:02:33.224276: step 29580/40890 (epoch 22/30), loss = 0.162739 (0.919 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:02:50.098171: step 29600/40890 (epoch 22/30), loss = 0.332072 (0.884 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:03:06.686876: step 29620/40890 (epoch 22/30), loss = 0.201422 (0.937 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:03:23.303567: step 29640/40890 (epoch 22/30), loss = 0.047265 (0.732 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:03:39.968971: step 29660/40890 (epoch 22/30), loss = 0.203290 (0.724 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:03:55.741934: step 29680/40890 (epoch 22/30), loss = 0.141197 (0.736 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:04:13.497211: step 29700/40890 (epoch 22/30), loss = 0.139746 (1.002 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:04:31.379287: step 29720/40890 (epoch 22/30), loss = 0.169630 (0.704 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:04:48.288281: step 29740/40890 (epoch 22/30), loss = 0.135560 (0.821 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:05:05.005422: step 29760/40890 (epoch 22/30), loss = 0.280950 (0.690 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:05:21.227094: step 29780/40890 (epoch 22/30), loss = 0.344435 (0.711 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:05:37.112504: step 29800/40890 (epoch 22/30), loss = 0.116340 (0.935 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:05:53.862542: step 29820/40890 (epoch 22/30), loss = 0.227167 (0.747 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:06:10.652870: step 29840/40890 (epoch 22/30), loss = 0.190495 (0.775 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:06:27.642755: step 29860/40890 (epoch 22/30), loss = 0.240833 (0.769 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:06:44.499430: step 29880/40890 (epoch 22/30), loss = 0.252103 (0.766 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:07:00.550639: step 29900/40890 (epoch 22/30), loss = 0.205230 (1.003 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:07:16.942014: step 29920/40890 (epoch 22/30), loss = 0.103612 (0.856 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:07:32.527113: step 29940/40890 (epoch 22/30), loss = 0.362660 (0.736 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:07:48.868649: step 29960/40890 (epoch 22/30), loss = 0.422476 (0.681 sec/batch), lr: 0.656100\n",
      "2020-10-11 20:08:05.505177: step 29980/40890 (epoch 22/30), loss = 0.197158 (0.877 sec/batch), lr: 0.656100\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.305%\n",
      "   Recall (micro): 63.153%\n",
      "       F1 (micro): 65.628%\n",
      "epoch 22: train_loss = 0.237902, dev_loss = 0.465713, dev_f1 = 0.6563\n",
      "model saved to ./save_models/00/checkpoint_epoch_22.pt\n",
      "\n",
      "2020-10-11 20:08:55.664665: step 30000/40890 (epoch 23/30), loss = 0.215102 (0.859 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:09:11.571652: step 30020/40890 (epoch 23/30), loss = 0.217244 (0.685 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:09:27.536015: step 30040/40890 (epoch 23/30), loss = 0.347412 (0.603 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:09:43.316360: step 30060/40890 (epoch 23/30), loss = 0.253259 (0.767 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:09:59.960358: step 30080/40890 (epoch 23/30), loss = 0.201843 (0.817 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:10:15.706555: step 30100/40890 (epoch 23/30), loss = 0.324214 (0.896 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:10:31.655894: step 30120/40890 (epoch 23/30), loss = 0.128157 (0.761 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:10:47.707955: step 30140/40890 (epoch 23/30), loss = 0.297177 (0.913 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:11:03.948667: step 30160/40890 (epoch 23/30), loss = 0.220567 (0.851 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:11:20.364636: step 30180/40890 (epoch 23/30), loss = 0.410240 (0.726 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:11:36.918826: step 30200/40890 (epoch 23/30), loss = 0.124197 (0.902 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:11:52.649088: step 30220/40890 (epoch 23/30), loss = 0.416839 (0.800 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:12:08.826933: step 30240/40890 (epoch 23/30), loss = 0.205398 (0.885 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:12:25.639629: step 30260/40890 (epoch 23/30), loss = 0.285459 (0.836 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:12:42.286181: step 30280/40890 (epoch 23/30), loss = 0.163515 (0.833 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:12:58.463677: step 30300/40890 (epoch 23/30), loss = 0.136373 (0.922 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:13:15.019807: step 30320/40890 (epoch 23/30), loss = 0.269946 (0.935 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:13:31.405844: step 30340/40890 (epoch 23/30), loss = 0.182196 (0.721 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:13:47.915857: step 30360/40890 (epoch 23/30), loss = 0.071821 (0.811 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:14:04.821911: step 30380/40890 (epoch 23/30), loss = 0.234896 (0.980 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:14:22.775692: step 30400/40890 (epoch 23/30), loss = 0.131518 (0.998 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:14:39.595154: step 30420/40890 (epoch 23/30), loss = 0.416040 (0.913 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:14:56.649914: step 30440/40890 (epoch 23/30), loss = 0.445282 (0.938 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:15:13.591336: step 30460/40890 (epoch 23/30), loss = 0.322855 (0.885 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:15:30.326126: step 30480/40890 (epoch 23/30), loss = 0.284609 (0.797 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:15:47.086032: step 30500/40890 (epoch 23/30), loss = 0.167295 (0.841 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:16:03.892951: step 30520/40890 (epoch 23/30), loss = 0.109742 (0.943 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:16:20.538778: step 30540/40890 (epoch 23/30), loss = 0.219315 (0.708 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:16:37.541551: step 30560/40890 (epoch 23/30), loss = 0.129017 (0.968 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:16:53.896365: step 30580/40890 (epoch 23/30), loss = 0.109474 (0.893 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:17:09.774829: step 30600/40890 (epoch 23/30), loss = 0.349372 (0.949 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:17:25.826466: step 30620/40890 (epoch 23/30), loss = 0.069611 (0.928 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:17:41.693731: step 30640/40890 (epoch 23/30), loss = 0.236156 (0.908 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:17:57.755820: step 30660/40890 (epoch 23/30), loss = 0.366055 (0.810 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:18:14.416978: step 30680/40890 (epoch 23/30), loss = 0.191849 (0.877 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:18:30.785346: step 30700/40890 (epoch 23/30), loss = 0.334122 (0.838 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:18:47.619447: step 30720/40890 (epoch 23/30), loss = 0.225032 (0.862 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:19:03.833655: step 30740/40890 (epoch 23/30), loss = 0.281921 (0.908 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:19:19.742589: step 30760/40890 (epoch 23/30), loss = 0.168721 (0.865 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:19:36.517457: step 30780/40890 (epoch 23/30), loss = 0.143821 (0.780 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:19:53.041876: step 30800/40890 (epoch 23/30), loss = 0.283580 (0.758 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:20:09.031460: step 30820/40890 (epoch 23/30), loss = 0.300556 (0.751 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:20:25.685455: step 30840/40890 (epoch 23/30), loss = 0.287280 (0.793 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:20:41.133289: step 30860/40890 (epoch 23/30), loss = 0.338646 (0.869 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:20:58.437478: step 30880/40890 (epoch 23/30), loss = 0.182206 (0.880 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:21:16.226581: step 30900/40890 (epoch 23/30), loss = 0.175785 (0.833 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:21:33.667371: step 30920/40890 (epoch 23/30), loss = 0.282755 (0.811 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:21:50.996300: step 30940/40890 (epoch 23/30), loss = 0.216251 (0.756 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:22:07.416009: step 30960/40890 (epoch 23/30), loss = 0.144939 (0.767 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:22:23.973420: step 30980/40890 (epoch 23/30), loss = 0.324489 (0.791 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:22:40.609530: step 31000/40890 (epoch 23/30), loss = 0.197170 (0.744 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:22:57.443660: step 31020/40890 (epoch 23/30), loss = 0.054659 (0.880 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:23:13.253219: step 31040/40890 (epoch 23/30), loss = 0.085999 (0.728 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:23:29.542499: step 31060/40890 (epoch 23/30), loss = 0.210017 (0.902 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:23:46.171939: step 31080/40890 (epoch 23/30), loss = 0.249026 (0.706 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:24:03.213378: step 31100/40890 (epoch 23/30), loss = 0.138174 (0.875 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:24:21.748306: step 31120/40890 (epoch 23/30), loss = 0.322951 (0.996 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:24:38.746514: step 31140/40890 (epoch 23/30), loss = 0.355997 (0.911 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:24:54.765420: step 31160/40890 (epoch 23/30), loss = 0.248121 (0.763 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:25:11.704229: step 31180/40890 (epoch 23/30), loss = 0.414501 (0.846 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:25:28.694618: step 31200/40890 (epoch 23/30), loss = 0.320635 (0.682 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:25:46.249410: step 31220/40890 (epoch 23/30), loss = 0.224453 (1.017 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:26:03.950149: step 31240/40890 (epoch 23/30), loss = 0.176954 (0.790 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:26:19.896458: step 31260/40890 (epoch 23/30), loss = 0.098377 (0.894 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:26:37.067527: step 31280/40890 (epoch 23/30), loss = 0.140077 (0.840 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:26:53.970090: step 31300/40890 (epoch 23/30), loss = 0.324220 (0.847 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:27:11.054292: step 31320/40890 (epoch 23/30), loss = 0.042598 (0.846 sec/batch), lr: 0.590490\n",
      "2020-10-11 20:27:27.501700: step 31340/40890 (epoch 23/30), loss = 0.146758 (0.853 sec/batch), lr: 0.590490\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 65.927%\n",
      "   Recall (micro): 65.066%\n",
      "       F1 (micro): 65.494%\n",
      "epoch 23: train_loss = 0.227884, dev_loss = 0.477185, dev_f1 = 0.6549\n",
      "model saved to ./save_models/00/checkpoint_epoch_23.pt\n",
      "\n",
      "2020-10-11 20:28:17.985093: step 31360/40890 (epoch 24/30), loss = 0.137982 (0.930 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:28:34.190692: step 31380/40890 (epoch 24/30), loss = 0.194886 (0.648 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:28:50.497584: step 31400/40890 (epoch 24/30), loss = 0.125755 (0.879 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:29:06.046831: step 31420/40890 (epoch 24/30), loss = 0.144904 (0.804 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:29:22.842651: step 31440/40890 (epoch 24/30), loss = 0.158161 (0.868 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:29:38.685906: step 31460/40890 (epoch 24/30), loss = 0.268983 (0.673 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:29:54.652934: step 31480/40890 (epoch 24/30), loss = 0.400749 (0.747 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:30:10.921376: step 31500/40890 (epoch 24/30), loss = 0.221343 (0.918 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:30:27.144742: step 31520/40890 (epoch 24/30), loss = 0.313694 (0.720 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:30:43.834750: step 31540/40890 (epoch 24/30), loss = 0.161226 (0.816 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:31:00.055930: step 31560/40890 (epoch 24/30), loss = 0.164699 (0.837 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:31:16.112077: step 31580/40890 (epoch 24/30), loss = 0.324664 (0.893 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:31:32.189042: step 31600/40890 (epoch 24/30), loss = 0.155178 (0.890 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:31:48.636925: step 31620/40890 (epoch 24/30), loss = 0.235464 (0.857 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:32:05.012832: step 31640/40890 (epoch 24/30), loss = 0.191149 (0.850 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:32:20.986903: step 31660/40890 (epoch 24/30), loss = 0.126270 (0.905 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:32:37.743542: step 31680/40890 (epoch 24/30), loss = 0.157289 (0.797 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:32:54.491859: step 31700/40890 (epoch 24/30), loss = 0.312491 (0.905 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:33:11.565563: step 31720/40890 (epoch 24/30), loss = 0.225721 (0.877 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:33:27.641569: step 31740/40890 (epoch 24/30), loss = 0.158378 (0.774 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:33:43.920812: step 31760/40890 (epoch 24/30), loss = 0.232734 (0.744 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:34:00.768838: step 31780/40890 (epoch 24/30), loss = 0.193487 (1.065 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:34:19.921970: step 31800/40890 (epoch 24/30), loss = 0.344132 (0.954 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:34:37.490939: step 31820/40890 (epoch 24/30), loss = 0.437858 (0.698 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:34:54.435514: step 31840/40890 (epoch 24/30), loss = 0.231884 (0.984 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:35:11.333008: step 31860/40890 (epoch 24/30), loss = 0.246099 (0.900 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:35:28.491347: step 31880/40890 (epoch 24/30), loss = 0.366800 (0.912 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:35:45.533538: step 31900/40890 (epoch 24/30), loss = 0.136451 (0.922 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:36:02.194142: step 31920/40890 (epoch 24/30), loss = 0.087175 (0.785 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:36:19.027192: step 31940/40890 (epoch 24/30), loss = 0.242843 (0.929 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:36:35.051319: step 31960/40890 (epoch 24/30), loss = 0.321887 (0.918 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:36:51.530294: step 31980/40890 (epoch 24/30), loss = 0.090534 (0.762 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:37:07.904980: step 32000/40890 (epoch 24/30), loss = 0.155937 (0.855 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:37:24.218458: step 32020/40890 (epoch 24/30), loss = 0.361146 (0.685 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:37:41.248927: step 32040/40890 (epoch 24/30), loss = 0.174383 (0.859 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:37:58.519898: step 32060/40890 (epoch 24/30), loss = 0.147404 (0.867 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:38:15.155977: step 32080/40890 (epoch 24/30), loss = 0.328175 (0.814 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:38:31.562079: step 32100/40890 (epoch 24/30), loss = 0.198928 (0.730 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:38:47.676431: step 32120/40890 (epoch 24/30), loss = 0.178202 (0.978 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:39:04.429601: step 32140/40890 (epoch 24/30), loss = 0.221122 (0.952 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:39:21.229797: step 32160/40890 (epoch 24/30), loss = 0.380161 (0.788 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:39:37.211351: step 32180/40890 (epoch 24/30), loss = 0.162333 (0.607 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:39:54.240261: step 32200/40890 (epoch 24/30), loss = 0.073346 (0.744 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:40:09.406366: step 32220/40890 (epoch 24/30), loss = 0.149487 (0.592 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:40:26.077268: step 32240/40890 (epoch 24/30), loss = 0.148531 (0.702 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:40:42.790374: step 32260/40890 (epoch 24/30), loss = 0.161762 (0.865 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:40:58.851699: step 32280/40890 (epoch 24/30), loss = 0.232950 (0.700 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:41:15.718825: step 32300/40890 (epoch 24/30), loss = 0.326400 (0.864 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:41:32.295813: step 32320/40890 (epoch 24/30), loss = 0.248888 (0.792 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:41:48.516859: step 32340/40890 (epoch 24/30), loss = 0.149654 (0.926 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:42:04.791484: step 32360/40890 (epoch 24/30), loss = 0.184786 (0.782 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:42:21.755198: step 32380/40890 (epoch 24/30), loss = 0.166806 (1.000 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:42:37.501284: step 32400/40890 (epoch 24/30), loss = 0.239904 (0.894 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:42:53.551009: step 32420/40890 (epoch 24/30), loss = 0.198145 (0.908 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:43:10.317167: step 32440/40890 (epoch 24/30), loss = 0.078159 (0.655 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:43:27.252426: step 32460/40890 (epoch 24/30), loss = 0.218674 (0.891 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:43:44.139826: step 32480/40890 (epoch 24/30), loss = 0.244636 (0.763 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:44:00.724631: step 32500/40890 (epoch 24/30), loss = 0.279812 (0.896 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:44:18.167095: step 32520/40890 (epoch 24/30), loss = 0.333656 (0.983 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:44:35.233745: step 32540/40890 (epoch 24/30), loss = 0.304933 (0.911 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:44:52.420468: step 32560/40890 (epoch 24/30), loss = 0.188800 (0.753 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:45:09.358351: step 32580/40890 (epoch 24/30), loss = 0.280015 (0.655 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:45:26.668301: step 32600/40890 (epoch 24/30), loss = 0.140887 (0.879 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:45:42.238946: step 32620/40890 (epoch 24/30), loss = 0.243922 (0.611 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:45:58.987131: step 32640/40890 (epoch 24/30), loss = 0.291605 (0.857 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:46:15.255801: step 32660/40890 (epoch 24/30), loss = 0.176459 (0.804 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:46:32.308264: step 32680/40890 (epoch 24/30), loss = 0.407526 (0.867 sec/batch), lr: 0.531441\n",
      "2020-10-11 20:46:48.896872: step 32700/40890 (epoch 24/30), loss = 0.113589 (0.886 sec/batch), lr: 0.531441\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.928%\n",
      "   Recall (micro): 62.730%\n",
      "       F1 (micro): 64.761%\n",
      "epoch 24: train_loss = 0.221151, dev_loss = 0.486185, dev_f1 = 0.6476\n",
      "model saved to ./save_models/00/checkpoint_epoch_24.pt\n",
      "\n",
      "2020-10-11 20:47:40.529468: step 32720/40890 (epoch 25/30), loss = 0.274166 (0.707 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:47:56.578436: step 32740/40890 (epoch 25/30), loss = 0.162377 (0.927 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:48:12.788999: step 32760/40890 (epoch 25/30), loss = 0.271488 (0.909 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:48:28.231651: step 32780/40890 (epoch 25/30), loss = 0.444166 (0.656 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:48:44.905162: step 32800/40890 (epoch 25/30), loss = 0.217553 (0.924 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:49:01.126490: step 32820/40890 (epoch 25/30), loss = 0.143770 (0.804 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:49:16.987773: step 32840/40890 (epoch 25/30), loss = 0.293497 (0.867 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:49:33.229378: step 32860/40890 (epoch 25/30), loss = 0.163734 (0.775 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:49:49.420711: step 32880/40890 (epoch 25/30), loss = 0.204790 (0.693 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:50:06.001698: step 32900/40890 (epoch 25/30), loss = 0.142098 (0.664 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:50:22.417146: step 32920/40890 (epoch 25/30), loss = 0.217205 (0.823 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:50:38.316815: step 32940/40890 (epoch 25/30), loss = 0.302346 (0.786 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:50:54.829151: step 32960/40890 (epoch 25/30), loss = 0.297814 (0.886 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:51:11.152329: step 32980/40890 (epoch 25/30), loss = 0.059708 (0.889 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:51:27.196326: step 33000/40890 (epoch 25/30), loss = 0.266092 (0.849 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:51:43.199227: step 33020/40890 (epoch 25/30), loss = 0.280601 (0.801 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:51:59.209420: step 33040/40890 (epoch 25/30), loss = 0.124936 (0.667 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:52:15.221833: step 33060/40890 (epoch 25/30), loss = 0.110521 (0.707 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:52:32.468253: step 33080/40890 (epoch 25/30), loss = 0.182075 (0.920 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:52:48.747339: step 33100/40890 (epoch 25/30), loss = 0.092433 (0.671 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:53:04.868588: step 33120/40890 (epoch 25/30), loss = 0.128285 (0.964 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:53:21.281317: step 33140/40890 (epoch 25/30), loss = 0.102974 (0.753 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:53:38.590040: step 33160/40890 (epoch 25/30), loss = 0.147791 (0.801 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:53:55.651838: step 33180/40890 (epoch 25/30), loss = 0.195135 (0.693 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:54:13.503687: step 33200/40890 (epoch 25/30), loss = 0.342800 (0.922 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:54:31.007197: step 33220/40890 (epoch 25/30), loss = 0.223322 (0.859 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:54:47.937496: step 33240/40890 (epoch 25/30), loss = 0.231806 (0.768 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:55:05.338468: step 33260/40890 (epoch 25/30), loss = 0.134007 (0.647 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:55:22.158372: step 33280/40890 (epoch 25/30), loss = 0.664896 (0.642 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:55:38.917120: step 33300/40890 (epoch 25/30), loss = 0.152939 (0.822 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:55:54.970658: step 33320/40890 (epoch 25/30), loss = 0.300597 (0.689 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:56:11.448964: step 33340/40890 (epoch 25/30), loss = 0.262071 (0.767 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:56:27.755409: step 33360/40890 (epoch 25/30), loss = 0.182410 (0.743 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:56:44.302370: step 33380/40890 (epoch 25/30), loss = 0.310566 (0.726 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:57:01.615942: step 33400/40890 (epoch 25/30), loss = 0.286926 (1.033 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:57:18.637412: step 33420/40890 (epoch 25/30), loss = 0.211962 (0.884 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:57:35.262769: step 33440/40890 (epoch 25/30), loss = 0.180213 (0.765 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:57:51.834504: step 33460/40890 (epoch 25/30), loss = 0.375092 (0.756 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:58:07.725268: step 33480/40890 (epoch 25/30), loss = 0.244769 (0.601 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:58:24.433581: step 33500/40890 (epoch 25/30), loss = 0.345953 (0.825 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:58:41.471852: step 33520/40890 (epoch 25/30), loss = 0.246853 (0.773 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:58:57.340607: step 33540/40890 (epoch 25/30), loss = 0.327574 (0.817 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:59:14.320595: step 33560/40890 (epoch 25/30), loss = 0.248955 (0.747 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:59:30.036638: step 33580/40890 (epoch 25/30), loss = 0.170214 (0.868 sec/batch), lr: 0.478297\n",
      "2020-10-11 20:59:46.319139: step 33600/40890 (epoch 25/30), loss = 0.287571 (0.835 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:00:02.925259: step 33620/40890 (epoch 25/30), loss = 0.156315 (0.872 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:00:19.151360: step 33640/40890 (epoch 25/30), loss = 0.241456 (0.829 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:00:35.688654: step 33660/40890 (epoch 25/30), loss = 0.527280 (0.908 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:00:52.457615: step 33680/40890 (epoch 25/30), loss = 0.155523 (0.864 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:01:08.544622: step 33700/40890 (epoch 25/30), loss = 0.162299 (0.664 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:01:24.826045: step 33720/40890 (epoch 25/30), loss = 0.374289 (0.758 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:01:41.541578: step 33740/40890 (epoch 25/30), loss = 0.240391 (0.869 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:01:56.764795: step 33760/40890 (epoch 25/30), loss = 0.216514 (0.872 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:02:12.403226: step 33780/40890 (epoch 25/30), loss = 0.206229 (0.642 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:02:29.412545: step 33800/40890 (epoch 25/30), loss = 0.277873 (0.852 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:02:45.803336: step 33820/40890 (epoch 25/30), loss = 0.149012 (0.746 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:03:03.062814: step 33840/40890 (epoch 25/30), loss = 0.338832 (0.859 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:03:19.486219: step 33860/40890 (epoch 25/30), loss = 0.293614 (0.817 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:03:35.499654: step 33880/40890 (epoch 25/30), loss = 0.183460 (0.725 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:03:51.977334: step 33900/40890 (epoch 25/30), loss = 0.229456 (0.839 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:04:09.995842: step 33920/40890 (epoch 25/30), loss = 0.212400 (0.870 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:04:29.329200: step 33940/40890 (epoch 25/30), loss = 0.051771 (0.890 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:04:46.323092: step 33960/40890 (epoch 25/30), loss = 0.163588 (0.936 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:05:02.491041: step 33980/40890 (epoch 25/30), loss = 0.294694 (0.800 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:05:18.871483: step 34000/40890 (epoch 25/30), loss = 0.339958 (0.924 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:05:35.118559: step 34020/40890 (epoch 25/30), loss = 0.109361 (0.739 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:05:52.200878: step 34040/40890 (epoch 25/30), loss = 0.148628 (0.897 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:06:08.897771: step 34060/40890 (epoch 25/30), loss = 0.200387 (0.707 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.042%\n",
      "   Recall (micro): 62.785%\n",
      "       F1 (micro): 65.308%\n",
      "epoch 25: train_loss = 0.216068, dev_loss = 0.477942, dev_f1 = 0.6531\n",
      "model saved to ./save_models/00/checkpoint_epoch_25.pt\n",
      "\n",
      "2020-10-11 21:07:01.370809: step 34080/40890 (epoch 26/30), loss = 0.168796 (0.898 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:07:17.703216: step 34100/40890 (epoch 26/30), loss = 0.247131 (0.827 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:07:33.713347: step 34120/40890 (epoch 26/30), loss = 0.121904 (0.882 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:07:49.499504: step 34140/40890 (epoch 26/30), loss = 0.168713 (0.767 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:08:06.234765: step 34160/40890 (epoch 26/30), loss = 0.188026 (0.700 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:08:22.375189: step 34180/40890 (epoch 26/30), loss = 0.261785 (0.835 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:08:38.331363: step 34200/40890 (epoch 26/30), loss = 0.202597 (0.789 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:08:54.644069: step 34220/40890 (epoch 26/30), loss = 0.286489 (0.860 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:09:10.857852: step 34240/40890 (epoch 26/30), loss = 0.155486 (0.716 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:09:27.465391: step 34260/40890 (epoch 26/30), loss = 0.409266 (0.816 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:09:43.854774: step 34280/40890 (epoch 26/30), loss = 0.175136 (0.958 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:09:59.968492: step 34300/40890 (epoch 26/30), loss = 0.273956 (0.921 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:10:16.314883: step 34320/40890 (epoch 26/30), loss = 0.160649 (0.697 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:10:32.406852: step 34340/40890 (epoch 26/30), loss = 0.090211 (0.757 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:10:48.486171: step 34360/40890 (epoch 26/30), loss = 0.402599 (0.775 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:11:04.823600: step 34380/40890 (epoch 26/30), loss = 0.151311 (0.692 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:11:20.782100: step 34400/40890 (epoch 26/30), loss = 0.235048 (0.769 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:11:36.725775: step 34420/40890 (epoch 26/30), loss = 0.165701 (0.750 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:11:53.499983: step 34440/40890 (epoch 26/30), loss = 0.137431 (0.806 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:12:09.417446: step 34460/40890 (epoch 26/30), loss = 0.046492 (0.871 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:12:25.109068: step 34480/40890 (epoch 26/30), loss = 0.197764 (0.759 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:12:41.562251: step 34500/40890 (epoch 26/30), loss = 0.044026 (0.763 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:12:58.883069: step 34520/40890 (epoch 26/30), loss = 0.464826 (0.849 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:13:16.023472: step 34540/40890 (epoch 26/30), loss = 0.109960 (0.835 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:13:32.764173: step 34560/40890 (epoch 26/30), loss = 0.253962 (0.926 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:13:49.363540: step 34580/40890 (epoch 26/30), loss = 0.134248 (0.864 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:14:06.641643: step 34600/40890 (epoch 26/30), loss = 0.411949 (0.732 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:14:25.615772: step 34620/40890 (epoch 26/30), loss = 0.191421 (0.931 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:14:42.023776: step 34640/40890 (epoch 26/30), loss = 0.097235 (0.945 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:14:58.826324: step 34660/40890 (epoch 26/30), loss = 0.371699 (0.751 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:15:15.299507: step 34680/40890 (epoch 26/30), loss = 0.333727 (0.869 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:15:31.874210: step 34700/40890 (epoch 26/30), loss = 0.193253 (0.881 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:15:48.099645: step 34720/40890 (epoch 26/30), loss = 0.206009 (0.784 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:16:04.728764: step 34740/40890 (epoch 26/30), loss = 0.181798 (0.821 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:16:21.645655: step 34760/40890 (epoch 26/30), loss = 0.458258 (0.792 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:16:38.966990: step 34780/40890 (epoch 26/30), loss = 0.204216 (0.922 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:16:56.214476: step 34800/40890 (epoch 26/30), loss = 0.324413 (0.801 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:17:13.366335: step 34820/40890 (epoch 26/30), loss = 0.277042 (0.874 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:17:29.253474: step 34840/40890 (epoch 26/30), loss = 0.255542 (0.754 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:17:45.586715: step 34860/40890 (epoch 26/30), loss = 0.235848 (0.684 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:18:02.730844: step 34880/40890 (epoch 26/30), loss = 0.199836 (0.810 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:18:18.777363: step 34900/40890 (epoch 26/30), loss = 0.172772 (0.802 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:18:35.662448: step 34920/40890 (epoch 26/30), loss = 0.275923 (0.933 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:18:51.417644: step 34940/40890 (epoch 26/30), loss = 0.325910 (0.700 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:19:07.559297: step 34960/40890 (epoch 26/30), loss = 0.340190 (0.854 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:19:24.126497: step 34980/40890 (epoch 26/30), loss = 0.180212 (0.903 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:19:40.880122: step 35000/40890 (epoch 26/30), loss = 0.059007 (0.782 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:19:57.055612: step 35020/40890 (epoch 26/30), loss = 0.325699 (0.814 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:20:13.954046: step 35040/40890 (epoch 26/30), loss = 0.284046 (0.905 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:20:30.319635: step 35060/40890 (epoch 26/30), loss = 0.316513 (0.706 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:20:46.511478: step 35080/40890 (epoch 26/30), loss = 0.333246 (0.874 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:21:03.473973: step 35100/40890 (epoch 26/30), loss = 0.120506 (0.925 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:21:19.865239: step 35120/40890 (epoch 26/30), loss = 0.235074 (0.843 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:21:36.396879: step 35140/40890 (epoch 26/30), loss = 0.457534 (0.852 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:21:55.833767: step 35160/40890 (epoch 26/30), loss = 0.282793 (0.768 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:22:12.804388: step 35180/40890 (epoch 26/30), loss = 0.283210 (0.937 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:22:30.140549: step 35200/40890 (epoch 26/30), loss = 0.229764 (0.891 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:22:46.586574: step 35220/40890 (epoch 26/30), loss = 0.148465 (0.792 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:23:02.827179: step 35240/40890 (epoch 26/30), loss = 0.375575 (0.790 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:23:20.368278: step 35260/40890 (epoch 26/30), loss = 0.385923 (0.832 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:23:37.479172: step 35280/40890 (epoch 26/30), loss = 0.130058 (0.796 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:23:54.801452: step 35300/40890 (epoch 26/30), loss = 0.302321 (1.013 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:24:12.589887: step 35320/40890 (epoch 26/30), loss = 0.288773 (0.910 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:24:29.860706: step 35340/40890 (epoch 26/30), loss = 0.250859 (0.906 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:24:46.010522: step 35360/40890 (epoch 26/30), loss = 0.237777 (0.853 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:25:02.314984: step 35380/40890 (epoch 26/30), loss = 0.139286 (0.855 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:25:19.137317: step 35400/40890 (epoch 26/30), loss = 0.194396 (0.900 sec/batch), lr: 0.478297\n",
      "2020-10-11 21:25:36.914400: step 35420/40890 (epoch 26/30), loss = 0.190072 (0.862 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.423%\n",
      "   Recall (micro): 63.576%\n",
      "       F1 (micro): 64.969%\n",
      "epoch 26: train_loss = 0.211499, dev_loss = 0.493388, dev_f1 = 0.6497\n",
      "model saved to ./save_models/00/checkpoint_epoch_26.pt\n",
      "\n",
      "2020-10-11 21:26:32.928354: step 35440/40890 (epoch 27/30), loss = 0.120967 (0.958 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:26:49.885013: step 35460/40890 (epoch 27/30), loss = 0.195876 (0.750 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:27:06.127581: step 35480/40890 (epoch 27/30), loss = 0.221800 (0.745 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:27:23.044352: step 35500/40890 (epoch 27/30), loss = 0.233841 (0.728 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:27:39.527181: step 35520/40890 (epoch 27/30), loss = 0.115667 (0.736 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:27:55.653061: step 35540/40890 (epoch 27/30), loss = 0.253853 (0.874 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:28:11.644301: step 35560/40890 (epoch 27/30), loss = 0.165856 (0.898 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:28:27.793119: step 35580/40890 (epoch 27/30), loss = 0.229318 (0.861 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:28:44.553092: step 35600/40890 (epoch 27/30), loss = 0.122348 (0.841 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:29:01.298788: step 35620/40890 (epoch 27/30), loss = 0.160499 (0.723 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:29:17.658654: step 35640/40890 (epoch 27/30), loss = 0.182462 (0.827 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:29:34.092329: step 35660/40890 (epoch 27/30), loss = 0.138664 (0.863 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:29:50.626307: step 35680/40890 (epoch 27/30), loss = 0.179153 (0.889 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:30:06.801106: step 35700/40890 (epoch 27/30), loss = 0.137812 (0.747 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:30:22.931360: step 35720/40890 (epoch 27/30), loss = 0.254641 (0.776 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:30:39.771689: step 35740/40890 (epoch 27/30), loss = 0.133652 (0.874 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:30:56.232425: step 35760/40890 (epoch 27/30), loss = 0.222640 (0.940 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:31:12.735386: step 35780/40890 (epoch 27/30), loss = 0.138647 (0.909 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:31:29.770832: step 35800/40890 (epoch 27/30), loss = 0.224085 (0.913 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:31:45.746116: step 35820/40890 (epoch 27/30), loss = 0.184600 (0.782 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:32:01.641611: step 35840/40890 (epoch 27/30), loss = 0.145874 (0.843 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:32:17.540155: step 35860/40890 (epoch 27/30), loss = 0.244160 (0.822 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:32:34.621484: step 35880/40890 (epoch 27/30), loss = 0.132234 (0.937 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:32:51.748272: step 35900/40890 (epoch 27/30), loss = 0.228915 (0.920 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:33:09.352287: step 35920/40890 (epoch 27/30), loss = 0.139935 (0.746 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:33:26.223782: step 35940/40890 (epoch 27/30), loss = 0.345957 (0.837 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:33:43.762396: step 35960/40890 (epoch 27/30), loss = 0.111459 (0.958 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:34:01.354918: step 35980/40890 (epoch 27/30), loss = 0.286556 (1.032 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:34:19.997978: step 36000/40890 (epoch 27/30), loss = 0.201172 (1.016 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:34:37.429537: step 36020/40890 (epoch 27/30), loss = 0.136849 (0.723 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:34:54.119493: step 36040/40890 (epoch 27/30), loss = 0.184550 (0.791 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:35:11.281845: step 36060/40890 (epoch 27/30), loss = 0.103967 (0.842 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:35:29.377477: step 36080/40890 (epoch 27/30), loss = 0.313510 (0.749 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:35:45.896307: step 36100/40890 (epoch 27/30), loss = 0.047033 (0.928 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:36:02.748245: step 36120/40890 (epoch 27/30), loss = 0.481784 (0.776 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:36:20.702406: step 36140/40890 (epoch 27/30), loss = 0.307516 (0.781 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:36:38.259803: step 36160/40890 (epoch 27/30), loss = 0.290470 (0.958 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:36:55.660728: step 36180/40890 (epoch 27/30), loss = 0.178836 (0.935 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:37:11.930730: step 36200/40890 (epoch 27/30), loss = 0.225917 (0.779 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:37:28.590187: step 36220/40890 (epoch 27/30), loss = 0.105410 (0.896 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:37:45.439135: step 36240/40890 (epoch 27/30), loss = 0.248395 (0.751 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:38:02.751862: step 36260/40890 (epoch 27/30), loss = 0.344007 (0.776 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:38:19.773422: step 36280/40890 (epoch 27/30), loss = 0.116978 (0.941 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:38:36.237583: step 36300/40890 (epoch 27/30), loss = 0.195229 (0.688 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:38:52.390391: step 36320/40890 (epoch 27/30), loss = 0.240794 (0.663 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:39:08.853370: step 36340/40890 (epoch 27/30), loss = 0.221357 (0.883 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:39:25.576653: step 36360/40890 (epoch 27/30), loss = 0.081405 (0.771 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:39:41.844154: step 36380/40890 (epoch 27/30), loss = 0.056756 (0.987 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:39:59.182196: step 36400/40890 (epoch 27/30), loss = 0.176145 (0.915 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:40:16.102537: step 36420/40890 (epoch 27/30), loss = 0.308203 (0.840 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:40:32.612364: step 36440/40890 (epoch 27/30), loss = 0.217493 (0.903 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:40:49.239365: step 36460/40890 (epoch 27/30), loss = 0.050450 (0.786 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:41:05.367341: step 36480/40890 (epoch 27/30), loss = 0.156523 (0.615 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:41:21.615303: step 36500/40890 (epoch 27/30), loss = 0.451480 (0.849 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:41:38.768835: step 36520/40890 (epoch 27/30), loss = 0.097718 (0.840 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:41:55.777910: step 36540/40890 (epoch 27/30), loss = 0.124969 (0.805 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:42:12.403527: step 36560/40890 (epoch 27/30), loss = 0.264136 (0.855 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:42:28.869498: step 36580/40890 (epoch 27/30), loss = 0.297685 (0.911 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:42:44.831398: step 36600/40890 (epoch 27/30), loss = 0.088414 (0.944 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:43:01.310726: step 36620/40890 (epoch 27/30), loss = 0.250146 (0.872 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:43:21.109613: step 36640/40890 (epoch 27/30), loss = 0.189470 (1.063 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:43:38.449757: step 36660/40890 (epoch 27/30), loss = 0.169119 (0.811 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:43:56.698050: step 36680/40890 (epoch 27/30), loss = 0.076379 (0.952 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:44:15.439606: step 36700/40890 (epoch 27/30), loss = 0.280664 (1.054 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:44:33.790151: step 36720/40890 (epoch 27/30), loss = 0.214083 (0.859 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:44:50.305989: step 36740/40890 (epoch 27/30), loss = 0.069316 (0.682 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:45:07.086471: step 36760/40890 (epoch 27/30), loss = 0.134196 (0.943 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:45:26.323913: step 36780/40890 (epoch 27/30), loss = 0.187860 (1.223 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:45:43.736039: step 36800/40890 (epoch 27/30), loss = 0.253487 (0.715 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.623%\n",
      "   Recall (micro): 62.896%\n",
      "       F1 (micro): 65.173%\n",
      "epoch 27: train_loss = 0.208247, dev_loss = 0.488223, dev_f1 = 0.6517\n",
      "model saved to ./save_models/00/checkpoint_epoch_27.pt\n",
      "\n",
      "2020-10-11 21:46:36.512361: step 36820/40890 (epoch 28/30), loss = 0.195112 (0.830 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:46:52.628775: step 36840/40890 (epoch 28/30), loss = 0.166858 (0.769 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:47:09.378985: step 36860/40890 (epoch 28/30), loss = 0.265493 (0.752 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:47:25.985914: step 36880/40890 (epoch 28/30), loss = 0.209605 (0.900 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:47:42.196083: step 36900/40890 (epoch 28/30), loss = 0.106452 (0.862 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:47:58.137456: step 36920/40890 (epoch 28/30), loss = 0.288447 (0.780 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:48:14.081822: step 36940/40890 (epoch 28/30), loss = 0.302572 (0.707 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:48:30.188752: step 36960/40890 (epoch 28/30), loss = 0.166922 (0.804 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:48:47.036702: step 36980/40890 (epoch 28/30), loss = 0.082671 (0.871 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:49:03.391976: step 37000/40890 (epoch 28/30), loss = 0.321667 (0.710 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:49:20.332678: step 37020/40890 (epoch 28/30), loss = 0.318645 (0.787 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:49:37.106823: step 37040/40890 (epoch 28/30), loss = 0.157298 (0.919 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:49:53.673526: step 37060/40890 (epoch 28/30), loss = 0.198433 (0.905 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:50:09.778462: step 37080/40890 (epoch 28/30), loss = 0.165462 (0.857 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:50:25.764715: step 37100/40890 (epoch 28/30), loss = 0.294711 (0.685 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:50:41.575438: step 37120/40890 (epoch 28/30), loss = 0.318837 (0.902 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:50:57.654444: step 37140/40890 (epoch 28/30), loss = 0.182530 (0.689 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:51:14.032649: step 37160/40890 (epoch 28/30), loss = 0.164499 (0.927 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:51:29.921164: step 37180/40890 (epoch 28/30), loss = 0.315747 (0.907 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:51:46.659915: step 37200/40890 (epoch 28/30), loss = 0.134621 (1.015 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:52:03.404142: step 37220/40890 (epoch 28/30), loss = 0.127382 (0.875 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:52:21.716176: step 37240/40890 (epoch 28/30), loss = 0.196333 (0.935 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:52:39.547496: step 37260/40890 (epoch 28/30), loss = 0.143921 (0.773 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:52:56.586933: step 37280/40890 (epoch 28/30), loss = 0.238272 (0.879 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:53:14.414851: step 37300/40890 (epoch 28/30), loss = 0.100274 (0.891 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:53:31.693421: step 37320/40890 (epoch 28/30), loss = 0.160167 (0.883 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:53:49.616571: step 37340/40890 (epoch 28/30), loss = 0.277152 (0.794 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:54:07.874478: step 37360/40890 (epoch 28/30), loss = 0.275948 (1.071 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:54:27.046220: step 37380/40890 (epoch 28/30), loss = 0.245875 (0.943 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:54:44.247473: step 37400/40890 (epoch 28/30), loss = 0.106647 (0.771 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:55:00.593424: step 37420/40890 (epoch 28/30), loss = 0.232774 (0.897 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:55:20.541597: step 37440/40890 (epoch 28/30), loss = 0.273487 (0.874 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:55:38.062675: step 37460/40890 (epoch 28/30), loss = 0.191182 (1.339 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:55:56.765665: step 37480/40890 (epoch 28/30), loss = 0.203839 (1.002 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:56:15.847640: step 37500/40890 (epoch 28/30), loss = 0.177649 (0.933 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:56:34.518344: step 37520/40890 (epoch 28/30), loss = 0.234996 (0.914 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:56:53.895030: step 37540/40890 (epoch 28/30), loss = 0.131560 (0.958 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:57:11.300488: step 37560/40890 (epoch 28/30), loss = 0.079124 (0.746 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:57:27.424383: step 37580/40890 (epoch 28/30), loss = 0.137377 (0.617 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:57:44.345138: step 37600/40890 (epoch 28/30), loss = 0.198643 (0.879 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:58:00.659204: step 37620/40890 (epoch 28/30), loss = 0.231139 (0.772 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:58:17.027618: step 37640/40890 (epoch 28/30), loss = 0.229906 (0.858 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:58:33.228298: step 37660/40890 (epoch 28/30), loss = 0.388602 (0.679 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:58:48.908455: step 37680/40890 (epoch 28/30), loss = 0.237050 (0.865 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:59:05.012439: step 37700/40890 (epoch 28/30), loss = 0.146772 (0.696 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:59:21.461294: step 37720/40890 (epoch 28/30), loss = 0.125776 (0.761 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:59:37.334849: step 37740/40890 (epoch 28/30), loss = 0.269142 (0.831 sec/batch), lr: 0.430467\n",
      "2020-10-11 21:59:53.961391: step 37760/40890 (epoch 28/30), loss = 0.178123 (0.808 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:00:10.257669: step 37780/40890 (epoch 28/30), loss = 0.106953 (0.844 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:00:26.062917: step 37800/40890 (epoch 28/30), loss = 0.233162 (0.811 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:00:42.178799: step 37820/40890 (epoch 28/30), loss = 0.031229 (0.759 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:00:57.891838: step 37840/40890 (epoch 28/30), loss = 0.096459 (0.705 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:01:13.111169: step 37860/40890 (epoch 28/30), loss = 0.304658 (0.688 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:01:28.930218: step 37880/40890 (epoch 28/30), loss = 0.229501 (0.858 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:01:44.710415: step 37900/40890 (epoch 28/30), loss = 0.220129 (0.875 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:02:01.007845: step 37920/40890 (epoch 28/30), loss = 0.165683 (0.875 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:02:16.999086: step 37940/40890 (epoch 28/30), loss = 0.103333 (0.888 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:02:32.783894: step 37960/40890 (epoch 28/30), loss = 0.243134 (0.818 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:02:48.600955: step 37980/40890 (epoch 28/30), loss = 0.124042 (0.870 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:03:05.081471: step 38000/40890 (epoch 28/30), loss = 0.164948 (0.834 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:03:21.828265: step 38020/40890 (epoch 28/30), loss = 0.196784 (0.925 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:03:38.701173: step 38040/40890 (epoch 28/30), loss = 0.207252 (0.903 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:03:55.122326: step 38060/40890 (epoch 28/30), loss = 0.164732 (0.772 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:04:11.992233: step 38080/40890 (epoch 28/30), loss = 0.210869 (0.852 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:04:29.789642: step 38100/40890 (epoch 28/30), loss = 0.095473 (0.861 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:04:45.739997: step 38120/40890 (epoch 28/30), loss = 0.326100 (0.878 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:05:02.206474: step 38140/40890 (epoch 28/30), loss = 0.127991 (0.635 sec/batch), lr: 0.430467\n",
      "2020-10-11 22:05:19.184077: step 38160/40890 (epoch 28/30), loss = 0.258118 (0.817 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.336%\n",
      "   Recall (micro): 64.018%\n",
      "       F1 (micro): 65.156%\n",
      "epoch 28: train_loss = 0.204725, dev_loss = 0.494777, dev_f1 = 0.6516\n",
      "model saved to ./save_models/00/checkpoint_epoch_28.pt\n",
      "\n",
      "2020-10-11 22:06:09.557977: step 38180/40890 (epoch 29/30), loss = 0.161669 (0.786 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:06:25.269971: step 38200/40890 (epoch 29/30), loss = 0.263939 (0.635 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:06:41.705540: step 38220/40890 (epoch 29/30), loss = 0.060085 (0.766 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:06:57.853784: step 38240/40890 (epoch 29/30), loss = 0.350721 (0.916 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:07:14.361643: step 38260/40890 (epoch 29/30), loss = 0.078607 (0.671 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:07:29.990399: step 38280/40890 (epoch 29/30), loss = 0.203397 (0.711 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:07:45.859965: step 38300/40890 (epoch 29/30), loss = 0.185780 (0.848 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:08:01.822171: step 38320/40890 (epoch 29/30), loss = 0.236911 (0.789 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:08:19.007888: step 38340/40890 (epoch 29/30), loss = 0.221282 (0.929 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:08:36.181429: step 38360/40890 (epoch 29/30), loss = 0.084289 (0.924 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:08:52.909429: step 38380/40890 (epoch 29/30), loss = 0.142017 (0.824 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:09:09.558660: step 38400/40890 (epoch 29/30), loss = 0.351090 (0.844 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:09:26.985960: step 38420/40890 (epoch 29/30), loss = 0.183235 (0.854 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:09:45.361361: step 38440/40890 (epoch 29/30), loss = 0.174313 (0.674 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:10:02.214532: step 38460/40890 (epoch 29/30), loss = 0.282734 (0.764 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:10:17.891317: step 38480/40890 (epoch 29/30), loss = 0.169892 (0.736 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:10:34.033246: step 38500/40890 (epoch 29/30), loss = 0.118170 (0.655 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:10:50.120276: step 38520/40890 (epoch 29/30), loss = 0.232779 (0.792 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:11:06.217937: step 38540/40890 (epoch 29/30), loss = 0.147913 (0.819 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:11:22.828673: step 38560/40890 (epoch 29/30), loss = 0.229425 (0.837 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:11:38.657102: step 38580/40890 (epoch 29/30), loss = 0.090599 (0.833 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:11:55.118885: step 38600/40890 (epoch 29/30), loss = 0.162364 (0.918 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:12:11.220196: step 38620/40890 (epoch 29/30), loss = 0.092532 (0.660 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:12:27.708118: step 38640/40890 (epoch 29/30), loss = 0.178089 (0.874 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:12:44.066291: step 38660/40890 (epoch 29/30), loss = 0.110534 (0.745 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:13:02.106485: step 38680/40890 (epoch 29/30), loss = 0.118843 (0.769 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:13:20.410685: step 38700/40890 (epoch 29/30), loss = 0.283780 (0.839 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:13:37.005201: step 38720/40890 (epoch 29/30), loss = 0.356396 (0.867 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:13:55.127628: step 38740/40890 (epoch 29/30), loss = 0.261883 (0.757 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:14:13.967952: step 38760/40890 (epoch 29/30), loss = 0.275135 (0.811 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:14:32.631054: step 38780/40890 (epoch 29/30), loss = 0.131386 (0.902 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:14:51.025868: step 38800/40890 (epoch 29/30), loss = 0.158656 (0.837 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:15:08.199430: step 38820/40890 (epoch 29/30), loss = 0.372533 (0.719 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:15:26.195001: step 38840/40890 (epoch 29/30), loss = 0.174579 (0.799 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:15:44.794287: step 38860/40890 (epoch 29/30), loss = 0.243781 (0.983 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:16:02.419165: step 38880/40890 (epoch 29/30), loss = 0.299924 (1.035 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:16:19.846650: step 38900/40890 (epoch 29/30), loss = 0.175262 (0.770 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:16:36.249512: step 38920/40890 (epoch 29/30), loss = 0.411997 (0.664 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:16:53.062048: step 38940/40890 (epoch 29/30), loss = 0.135343 (0.802 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:17:10.235106: step 38960/40890 (epoch 29/30), loss = 0.205645 (0.807 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:17:27.034607: step 38980/40890 (epoch 29/30), loss = 0.221184 (0.891 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:17:42.912026: step 39000/40890 (epoch 29/30), loss = 0.215162 (0.773 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:17:59.786396: step 39020/40890 (epoch 29/30), loss = 0.155951 (0.907 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:18:15.383067: step 39040/40890 (epoch 29/30), loss = 0.242744 (0.865 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:18:32.055268: step 39060/40890 (epoch 29/30), loss = 0.129035 (0.778 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:18:48.800267: step 39080/40890 (epoch 29/30), loss = 0.229132 (0.709 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:19:04.737120: step 39100/40890 (epoch 29/30), loss = 0.103979 (0.890 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:19:21.492354: step 39120/40890 (epoch 29/30), loss = 0.109151 (0.783 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:19:37.840910: step 39140/40890 (epoch 29/30), loss = 0.144215 (0.908 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:19:53.967720: step 39160/40890 (epoch 29/30), loss = 0.264031 (0.786 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:20:10.364732: step 39180/40890 (epoch 29/30), loss = 0.080660 (0.806 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:20:26.513926: step 39200/40890 (epoch 29/30), loss = 0.155449 (0.677 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:20:41.596633: step 39220/40890 (epoch 29/30), loss = 0.174153 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:20:57.116282: step 39240/40890 (epoch 29/30), loss = 0.198573 (0.846 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:21:13.095559: step 39260/40890 (epoch 29/30), loss = 0.154077 (0.771 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:21:28.974106: step 39280/40890 (epoch 29/30), loss = 0.120374 (0.863 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:21:45.192824: step 39300/40890 (epoch 29/30), loss = 0.149044 (0.771 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:22:01.429010: step 39320/40890 (epoch 29/30), loss = 0.158921 (0.811 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:22:19.051886: step 39340/40890 (epoch 29/30), loss = 0.288554 (0.863 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:22:38.083268: step 39360/40890 (epoch 29/30), loss = 0.108437 (0.920 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:23:00.069812: step 39380/40890 (epoch 29/30), loss = 0.152572 (0.860 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:23:20.774708: step 39400/40890 (epoch 29/30), loss = 0.241860 (0.759 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:23:37.911882: step 39420/40890 (epoch 29/30), loss = 0.145157 (0.751 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:23:53.625133: step 39440/40890 (epoch 29/30), loss = 0.064505 (0.908 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:24:10.826004: step 39460/40890 (epoch 29/30), loss = 0.324797 (0.954 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:24:27.862468: step 39480/40890 (epoch 29/30), loss = 0.075295 (0.674 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:24:44.848907: step 39500/40890 (epoch 29/30), loss = 0.240347 (0.861 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:25:01.668450: step 39520/40890 (epoch 29/30), loss = 0.153934 (0.901 sec/batch), lr: 0.387420\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.155%\n",
      "   Recall (micro): 64.054%\n",
      "       F1 (micro): 65.568%\n",
      "epoch 29: train_loss = 0.197965, dev_loss = 0.503728, dev_f1 = 0.6557\n",
      "model saved to ./save_models/00/checkpoint_epoch_29.pt\n",
      "\n",
      "2020-10-11 22:25:54.060430: step 39540/40890 (epoch 30/30), loss = 0.234421 (0.930 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:26:10.496481: step 39560/40890 (epoch 30/30), loss = 0.234584 (0.925 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:26:27.072770: step 39580/40890 (epoch 30/30), loss = 0.105171 (0.895 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:26:43.079968: step 39600/40890 (epoch 30/30), loss = 0.182043 (0.810 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:27:00.829738: step 39620/40890 (epoch 30/30), loss = 0.180041 (0.919 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:27:17.022668: step 39640/40890 (epoch 30/30), loss = 0.149794 (0.787 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:27:33.463110: step 39660/40890 (epoch 30/30), loss = 0.235959 (1.084 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:27:49.497795: step 39680/40890 (epoch 30/30), loss = 0.346403 (0.660 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:28:05.674539: step 39700/40890 (epoch 30/30), loss = 0.077707 (0.838 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:28:22.071694: step 39720/40890 (epoch 30/30), loss = 0.307426 (0.679 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:28:38.353160: step 39740/40890 (epoch 30/30), loss = 0.049424 (0.883 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:28:53.966409: step 39760/40890 (epoch 30/30), loss = 0.073535 (0.633 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:29:09.917756: step 39780/40890 (epoch 30/30), loss = 0.065887 (0.877 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:29:26.097493: step 39800/40890 (epoch 30/30), loss = 0.131298 (0.799 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:29:42.048839: step 39820/40890 (epoch 30/30), loss = 0.184131 (0.880 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:29:57.505531: step 39840/40890 (epoch 30/30), loss = 0.351793 (0.716 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:30:13.431321: step 39860/40890 (epoch 30/30), loss = 0.309818 (0.824 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:30:29.982062: step 39880/40890 (epoch 30/30), loss = 0.102567 (0.904 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:30:46.024033: step 39900/40890 (epoch 30/30), loss = 0.218126 (0.674 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:31:01.736020: step 39920/40890 (epoch 30/30), loss = 0.195153 (0.719 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:31:17.488898: step 39940/40890 (epoch 30/30), loss = 0.155668 (0.793 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:31:34.355796: step 39960/40890 (epoch 30/30), loss = 0.197912 (0.901 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:31:50.906563: step 39980/40890 (epoch 30/30), loss = 0.109068 (0.756 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:32:07.765369: step 40000/40890 (epoch 30/30), loss = 0.167653 (0.686 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:32:24.379943: step 40020/40890 (epoch 30/30), loss = 0.376503 (0.784 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:32:42.687948: step 40040/40890 (epoch 30/30), loss = 0.292653 (0.979 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:33:00.235694: step 40060/40890 (epoch 30/30), loss = 0.272642 (0.784 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:33:17.292086: step 40080/40890 (epoch 30/30), loss = 0.206517 (0.728 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:33:34.142030: step 40100/40890 (epoch 30/30), loss = 0.217263 (1.032 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:33:50.791436: step 40120/40890 (epoch 30/30), loss = 0.439112 (0.854 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:34:06.875447: step 40140/40890 (epoch 30/30), loss = 0.375285 (0.784 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:34:25.880479: step 40160/40890 (epoch 30/30), loss = 0.175022 (0.815 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:34:42.726843: step 40180/40890 (epoch 30/30), loss = 0.195922 (0.920 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:34:59.659931: step 40200/40890 (epoch 30/30), loss = 0.138585 (0.943 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:35:17.228689: step 40220/40890 (epoch 30/30), loss = 0.191896 (0.760 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:35:34.252588: step 40240/40890 (epoch 30/30), loss = 0.157033 (0.889 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:35:51.641462: step 40260/40890 (epoch 30/30), loss = 0.251481 (0.915 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:36:08.150323: step 40280/40890 (epoch 30/30), loss = 0.324670 (0.804 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:36:24.550564: step 40300/40890 (epoch 30/30), loss = 0.218760 (0.771 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:36:41.648845: step 40320/40890 (epoch 30/30), loss = 0.231144 (0.949 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:36:58.327247: step 40340/40890 (epoch 30/30), loss = 0.142021 (0.733 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:37:14.637850: step 40360/40890 (epoch 30/30), loss = 0.289879 (0.794 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:37:31.212704: step 40380/40890 (epoch 30/30), loss = 0.328868 (0.673 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:37:46.361561: step 40400/40890 (epoch 30/30), loss = 0.174571 (0.795 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:38:03.459460: step 40420/40890 (epoch 30/30), loss = 0.149796 (0.790 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:38:20.892844: step 40440/40890 (epoch 30/30), loss = 0.100003 (0.856 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:38:36.773380: step 40460/40890 (epoch 30/30), loss = 0.192080 (0.814 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:38:53.482262: step 40480/40890 (epoch 30/30), loss = 0.241744 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:39:10.047276: step 40500/40890 (epoch 30/30), loss = 0.222045 (0.711 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:39:26.648957: step 40520/40890 (epoch 30/30), loss = 0.179539 (0.788 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:39:42.899468: step 40540/40890 (epoch 30/30), loss = 0.063818 (0.848 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:40:00.136279: step 40560/40890 (epoch 30/30), loss = 0.118470 (0.589 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:40:15.915968: step 40580/40890 (epoch 30/30), loss = 0.036610 (0.871 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:40:31.892869: step 40600/40890 (epoch 30/30), loss = 0.184808 (0.789 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:40:48.099869: step 40620/40890 (epoch 30/30), loss = 0.171014 (0.882 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:41:03.865745: step 40640/40890 (epoch 30/30), loss = 0.205006 (0.717 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:41:20.042881: step 40660/40890 (epoch 30/30), loss = 0.317865 (0.682 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:41:35.516505: step 40680/40890 (epoch 30/30), loss = 0.145877 (0.722 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:41:50.569590: step 40700/40890 (epoch 30/30), loss = 0.222067 (0.848 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:42:06.368349: step 40720/40890 (epoch 30/30), loss = 0.279867 (0.783 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:42:22.694209: step 40740/40890 (epoch 30/30), loss = 0.180584 (0.890 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:42:39.196472: step 40760/40890 (epoch 30/30), loss = 0.100215 (0.889 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:42:55.749567: step 40780/40890 (epoch 30/30), loss = 0.165326 (0.739 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:43:10.963522: step 40800/40890 (epoch 30/30), loss = 0.181349 (0.889 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:43:27.125928: step 40820/40890 (epoch 30/30), loss = 0.317851 (0.728 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:43:44.326012: step 40840/40890 (epoch 30/30), loss = 0.107242 (0.891 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:44:00.877818: step 40860/40890 (epoch 30/30), loss = 0.098303 (1.036 sec/batch), lr: 0.387420\n",
      "2020-10-11 22:44:18.316063: step 40880/40890 (epoch 30/30), loss = 0.293793 (0.992 sec/batch), lr: 0.387420\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.245%\n",
      "   Recall (micro): 63.650%\n",
      "       F1 (micro): 64.922%\n",
      "epoch 30: train_loss = 0.193845, dev_loss = 0.503002, dev_f1 = 0.6492\n",
      "model saved to ./save_models/00/checkpoint_epoch_30.pt\n",
      "\n",
      "Training ended with 30 epochs.\n",
      "{'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'per:employee_of': 3, 'org:alternate_names': 4, 'org:country_of_headquarters': 5, 'per:countries_of_residence': 6, 'org:city_of_headquarters': 7, 'per:cities_of_residence': 8, 'per:age': 9, 'per:stateorprovinces_of_residence': 10, 'per:origin': 11, 'org:subsidiaries': 12, 'org:parents': 13, 'per:spouse': 14, 'org:stateorprovince_of_headquarters': 15, 'per:children': 16, 'per:other_family': 17, 'per:alternate_names': 18, 'org:members': 19, 'per:siblings': 20, 'per:schools_attended': 21, 'per:parents': 22, 'per:date_of_death': 23, 'org:member_of': 24, 'org:founded_by': 25, 'org:website': 26, 'per:cause_of_death': 27, 'org:political/religious_affiliation': 28, 'org:founded': 29, 'per:city_of_death': 30, 'org:shareholders': 31, 'org:number_of_employees/members': 32, 'per:date_of_birth': 33, 'per:city_of_birth': 34, 'per:charges': 35, 'per:stateorprovince_of_death': 36, 'per:religion': 37, 'per:stateorprovince_of_birth': 38, 'per:country_of_birth': 39, 'org:dissolved': 40, 'per:country_of_death': 41}\n",
      "{'data_dir': './dataset/tacred', 'vocab_dir': './dataset/vocab', 'glove_dir': './dataset/glove', 'emb_dim': 300, 'vocab_file': '/vocab.pkl', 'embed_file': '/embedding.npy', 'glove_text_file': 'glove.840B.300d.txt', 'lower': False, 'min_freq': 0, 'num_class': 42, 'ner_dim': 30, 'pos_dim': 30, 'hidden_dim': 200, 'num_layers': 2, 'dropout': 0.5, 'word_dropout': 0.04, 'topn': 10000000000.0, 'lower_dest': 'lower', 'lower_action': 'store_true', 'no_lower_dest': 'lower', 'no_lower_action': 'store_false', 'attn_dest': 'attn', 'attn_action': 'store_true', 'no_attn_dest': 'attn', 'no_attn_action': 'store_false', 'attn': True, 'attn_dim': 200, 'pe_dim': 30, 'lr': 1.0, 'lr_decay': 0.9, 'optim': 'sgd', 'num_epoch': 30, 'batch_size': 50, 'max_grad_norm': 5, 'log_step': 20, 'log': 'logs.txt', 'save_epoch': 5, 'save_dir': './save_models', 'id': '00', 'info': '', 'seed': 1234, 'cuda': False, 'cpu_action': 'store_true', 'cpu': True, 'vocab_size': 55950, 'model_save_dir': './save_models/00'}\n",
      "Vocab size 55950 loaded from file\n",
      "Config saved to file ./save_models/01/config.json\n",
      "Overwriting old vocab file at ./save_models/01/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : ./dataset/tacred\n",
      "\tvocab_dir : ./dataset/vocab\n",
      "\tglove_dir : ./dataset/glove\n",
      "\temb_dim : 300\n",
      "\tvocab_file : /vocab.pkl\n",
      "\tembed_file : /embedding.npy\n",
      "\tglove_text_file : glove.840B.300d.txt\n",
      "\tlower : False\n",
      "\tmin_freq : 0\n",
      "\tnum_class : 42\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 200\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower_dest : lower\n",
      "\tlower_action : store_true\n",
      "\tno_lower_dest : lower\n",
      "\tno_lower_action : store_false\n",
      "\tattn_dest : attn\n",
      "\tattn_action : store_true\n",
      "\tno_attn_dest : attn\n",
      "\tno_attn_action : store_false\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.9\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 30\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./save_models\n",
      "\tid : 00\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : False\n",
      "\tcpu_action : store_true\n",
      "\tcpu : True\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./save_models/01\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n",
      "2020-10-11 22:45:22.843474: step 20/40890 (epoch 1/30), loss = 1.258212 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:45:39.660973: step 40/40890 (epoch 1/30), loss = 0.754567 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:45:56.163845: step 60/40890 (epoch 1/30), loss = 0.819642 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:46:12.826809: step 80/40890 (epoch 1/30), loss = 0.545918 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:46:29.076266: step 100/40890 (epoch 1/30), loss = 0.752670 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:46:45.193153: step 120/40890 (epoch 1/30), loss = 0.977101 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:47:01.278507: step 140/40890 (epoch 1/30), loss = 1.168537 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:47:17.586416: step 160/40890 (epoch 1/30), loss = 0.805399 (0.913 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:47:34.103251: step 180/40890 (epoch 1/30), loss = 1.020030 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:47:50.820550: step 200/40890 (epoch 1/30), loss = 0.931382 (0.924 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:48:07.230516: step 220/40890 (epoch 1/30), loss = 0.870774 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:48:23.818163: step 240/40890 (epoch 1/30), loss = 1.185092 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:48:39.874561: step 260/40890 (epoch 1/30), loss = 0.818716 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:48:55.805474: step 280/40890 (epoch 1/30), loss = 0.741087 (0.651 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:49:12.241610: step 300/40890 (epoch 1/30), loss = 0.627197 (0.860 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:49:28.443271: step 320/40890 (epoch 1/30), loss = 0.631486 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:49:44.204518: step 340/40890 (epoch 1/30), loss = 1.038404 (0.640 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:50:00.198756: step 360/40890 (epoch 1/30), loss = 1.245947 (0.690 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:50:15.937670: step 380/40890 (epoch 1/30), loss = 1.349759 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:50:31.535961: step 400/40890 (epoch 1/30), loss = 0.723725 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:50:46.984551: step 420/40890 (epoch 1/30), loss = 0.506739 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:51:03.003717: step 440/40890 (epoch 1/30), loss = 1.057719 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:51:19.096685: step 460/40890 (epoch 1/30), loss = 0.891380 (0.889 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:51:35.143776: step 480/40890 (epoch 1/30), loss = 0.796642 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:51:50.801417: step 500/40890 (epoch 1/30), loss = 0.469336 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:52:06.947244: step 520/40890 (epoch 1/30), loss = 0.754350 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:52:23.276580: step 540/40890 (epoch 1/30), loss = 0.402274 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:52:39.472685: step 560/40890 (epoch 1/30), loss = 0.741191 (0.736 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:52:55.918709: step 580/40890 (epoch 1/30), loss = 0.499467 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:53:11.886052: step 600/40890 (epoch 1/30), loss = 0.882869 (0.809 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:53:27.714727: step 620/40890 (epoch 1/30), loss = 0.590867 (0.804 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:53:43.783729: step 640/40890 (epoch 1/30), loss = 0.758468 (0.666 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:53:59.583482: step 660/40890 (epoch 1/30), loss = 0.414785 (0.741 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:54:17.206438: step 680/40890 (epoch 1/30), loss = 0.521635 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:54:34.501192: step 700/40890 (epoch 1/30), loss = 0.665271 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:54:51.640383: step 720/40890 (epoch 1/30), loss = 0.631263 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:55:08.051501: step 740/40890 (epoch 1/30), loss = 0.448211 (0.635 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:55:24.344933: step 760/40890 (epoch 1/30), loss = 0.514535 (0.927 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:55:40.592487: step 780/40890 (epoch 1/30), loss = 0.500477 (0.977 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:55:57.725674: step 800/40890 (epoch 1/30), loss = 1.101705 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:56:13.963342: step 820/40890 (epoch 1/30), loss = 0.539694 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:56:30.356508: step 840/40890 (epoch 1/30), loss = 0.408042 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:56:46.835444: step 860/40890 (epoch 1/30), loss = 0.404977 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:57:02.822701: step 880/40890 (epoch 1/30), loss = 0.429124 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:57:19.016394: step 900/40890 (epoch 1/30), loss = 0.816981 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:57:35.364679: step 920/40890 (epoch 1/30), loss = 0.578622 (0.704 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:57:51.226266: step 940/40890 (epoch 1/30), loss = 0.332286 (0.826 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:58:07.758061: step 960/40890 (epoch 1/30), loss = 0.711051 (0.739 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:58:23.873968: step 980/40890 (epoch 1/30), loss = 0.654821 (0.743 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:58:39.692180: step 1000/40890 (epoch 1/30), loss = 0.482031 (0.907 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:58:55.743260: step 1020/40890 (epoch 1/30), loss = 0.444282 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:59:11.252297: step 1040/40890 (epoch 1/30), loss = 0.511974 (0.634 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:59:26.334966: step 1060/40890 (epoch 1/30), loss = 0.596279 (0.557 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:59:42.295798: step 1080/40890 (epoch 1/30), loss = 0.530833 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 22:59:58.111508: step 1100/40890 (epoch 1/30), loss = 0.647643 (0.827 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:00:14.269169: step 1120/40890 (epoch 1/30), loss = 0.384513 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:00:30.144719: step 1140/40890 (epoch 1/30), loss = 0.644153 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:00:45.516615: step 1160/40890 (epoch 1/30), loss = 0.297451 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:01:00.901477: step 1180/40890 (epoch 1/30), loss = 0.551875 (0.909 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:01:16.807952: step 1200/40890 (epoch 1/30), loss = 0.217051 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:01:32.958766: step 1220/40890 (epoch 1/30), loss = 0.665334 (0.789 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:01:49.199339: step 1240/40890 (epoch 1/30), loss = 0.389289 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:02:05.298803: step 1260/40890 (epoch 1/30), loss = 0.627109 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:02:20.548883: step 1280/40890 (epoch 1/30), loss = 0.203175 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:02:36.665787: step 1300/40890 (epoch 1/30), loss = 0.481843 (0.656 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:02:52.458068: step 1320/40890 (epoch 1/30), loss = 0.317873 (0.635 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:03:09.077091: step 1340/40890 (epoch 1/30), loss = 0.236476 (0.748 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:03:26.063670: step 1360/40890 (epoch 1/30), loss = 0.381254 (0.827 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 62.234%\n",
      "   Recall (micro): 41.501%\n",
      "       F1 (micro): 49.796%\n",
      "epoch 1: train_loss = 0.691695, dev_loss = 0.599954, dev_f1 = 0.4980\n",
      "model saved to ./save_models/01/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 23:04:18.499447: step 1380/40890 (epoch 2/30), loss = 0.351459 (0.913 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:04:34.747999: step 1400/40890 (epoch 2/30), loss = 0.222033 (0.938 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:04:51.031458: step 1420/40890 (epoch 2/30), loss = 0.553333 (0.733 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:05:07.295853: step 1440/40890 (epoch 2/30), loss = 0.569818 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:05:23.759842: step 1460/40890 (epoch 2/30), loss = 0.438725 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:05:39.614531: step 1480/40890 (epoch 2/30), loss = 0.485853 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:05:55.851122: step 1500/40890 (epoch 2/30), loss = 0.554091 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:06:12.133584: step 1520/40890 (epoch 2/30), loss = 0.796463 (0.793 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:06:28.729208: step 1540/40890 (epoch 2/30), loss = 0.596498 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:06:45.126370: step 1560/40890 (epoch 2/30), loss = 0.427094 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:07:01.451718: step 1580/40890 (epoch 2/30), loss = 0.451356 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:07:17.438482: step 1600/40890 (epoch 2/30), loss = 0.528265 (0.839 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:07:33.161439: step 1620/40890 (epoch 2/30), loss = 0.338092 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:07:49.115640: step 1640/40890 (epoch 2/30), loss = 0.425448 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:08:05.092927: step 1660/40890 (epoch 2/30), loss = 0.571385 (0.626 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:08:20.488269: step 1680/40890 (epoch 2/30), loss = 0.192519 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:08:36.579243: step 1700/40890 (epoch 2/30), loss = 0.251841 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:08:52.341160: step 1720/40890 (epoch 2/30), loss = 0.362702 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:09:07.902549: step 1740/40890 (epoch 2/30), loss = 0.540871 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:09:23.590659: step 1760/40890 (epoch 2/30), loss = 0.561662 (0.783 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:09:39.092209: step 1780/40890 (epoch 2/30), loss = 0.384051 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:09:55.165230: step 1800/40890 (epoch 2/30), loss = 0.716035 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:10:11.217308: step 1820/40890 (epoch 2/30), loss = 0.440870 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:10:27.259412: step 1840/40890 (epoch 2/30), loss = 0.730007 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:10:43.033240: step 1860/40890 (epoch 2/30), loss = 0.512894 (0.718 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:10:59.223946: step 1880/40890 (epoch 2/30), loss = 0.730003 (0.776 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:11:15.585197: step 1900/40890 (epoch 2/30), loss = 0.637030 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:11:31.354033: step 1920/40890 (epoch 2/30), loss = 0.522554 (0.758 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:11:47.395140: step 1940/40890 (epoch 2/30), loss = 0.461238 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:12:02.853662: step 1960/40890 (epoch 2/30), loss = 0.502616 (0.763 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:12:18.263511: step 1980/40890 (epoch 2/30), loss = 0.517018 (0.618 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:12:34.430281: step 2000/40890 (epoch 2/30), loss = 0.357891 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:12:50.185153: step 2020/40890 (epoch 2/30), loss = 0.319641 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:13:06.455008: step 2040/40890 (epoch 2/30), loss = 0.530992 (0.781 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:13:23.498787: step 2060/40890 (epoch 2/30), loss = 0.579451 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:13:39.963760: step 2080/40890 (epoch 2/30), loss = 0.303996 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:13:56.594255: step 2100/40890 (epoch 2/30), loss = 0.467466 (0.860 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:14:13.806740: step 2120/40890 (epoch 2/30), loss = 0.417209 (0.918 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:14:30.765393: step 2140/40890 (epoch 2/30), loss = 0.408501 (0.788 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:14:47.818306: step 2160/40890 (epoch 2/30), loss = 0.554758 (0.927 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:15:04.304564: step 2180/40890 (epoch 2/30), loss = 0.416831 (0.666 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:15:20.655351: step 2200/40890 (epoch 2/30), loss = 0.232025 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:15:37.564138: step 2220/40890 (epoch 2/30), loss = 0.486383 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:15:53.097602: step 2240/40890 (epoch 2/30), loss = 0.493884 (0.697 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:16:09.739959: step 2260/40890 (epoch 2/30), loss = 0.470942 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:16:26.246821: step 2280/40890 (epoch 2/30), loss = 0.314199 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:16:42.371704: step 2300/40890 (epoch 2/30), loss = 0.363959 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:16:59.340330: step 2320/40890 (epoch 2/30), loss = 0.372695 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:17:15.902045: step 2340/40890 (epoch 2/30), loss = 0.684661 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:17:31.787568: step 2360/40890 (epoch 2/30), loss = 0.281405 (0.906 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:17:47.681071: step 2380/40890 (epoch 2/30), loss = 0.318281 (0.759 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:18:03.624438: step 2400/40890 (epoch 2/30), loss = 0.832487 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:18:18.726563: step 2420/40890 (epoch 2/30), loss = 0.234721 (0.715 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:18:34.295932: step 2440/40890 (epoch 2/30), loss = 0.398276 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:18:50.006492: step 2460/40890 (epoch 2/30), loss = 0.337867 (0.648 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:19:05.963823: step 2480/40890 (epoch 2/30), loss = 0.432942 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:19:21.945090: step 2500/40890 (epoch 2/30), loss = 0.546257 (0.674 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:19:37.418714: step 2520/40890 (epoch 2/30), loss = 0.696190 (0.683 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:19:52.550624: step 2540/40890 (epoch 2/30), loss = 0.249179 (0.895 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:20:08.590740: step 2560/40890 (epoch 2/30), loss = 0.370627 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:20:24.592958: step 2580/40890 (epoch 2/30), loss = 0.337852 (0.738 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:20:40.780726: step 2600/40890 (epoch 2/30), loss = 0.307138 (0.754 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:20:56.824825: step 2620/40890 (epoch 2/30), loss = 0.763068 (0.718 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:21:12.127233: step 2640/40890 (epoch 2/30), loss = 0.400643 (0.899 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:21:27.972862: step 2660/40890 (epoch 2/30), loss = 0.260900 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:21:43.317831: step 2680/40890 (epoch 2/30), loss = 0.653163 (0.656 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:21:59.413791: step 2700/40890 (epoch 2/30), loss = 0.899716 (0.642 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:22:15.554479: step 2720/40890 (epoch 2/30), loss = 0.492710 (0.838 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 62.720%\n",
      "   Recall (micro): 49.117%\n",
      "       F1 (micro): 55.091%\n",
      "epoch 2: train_loss = 0.461492, dev_loss = 0.522418, dev_f1 = 0.5509\n",
      "model saved to ./save_models/01/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 23:23:05.396715: step 2740/40890 (epoch 3/30), loss = 0.475547 (0.777 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:23:21.369096: step 2760/40890 (epoch 3/30), loss = 0.437721 (0.610 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:23:37.462064: step 2780/40890 (epoch 3/30), loss = 0.543649 (0.569 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:23:53.345593: step 2800/40890 (epoch 3/30), loss = 0.442456 (0.763 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:24:10.916608: step 2820/40890 (epoch 3/30), loss = 0.439783 (0.925 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:24:27.746606: step 2840/40890 (epoch 3/30), loss = 0.371032 (0.903 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:24:43.913378: step 2860/40890 (epoch 3/30), loss = 0.227484 (0.763 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:25:00.014338: step 2880/40890 (epoch 3/30), loss = 0.383391 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:25:16.668814: step 2900/40890 (epoch 3/30), loss = 0.351658 (0.911 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:25:33.080929: step 2920/40890 (epoch 3/30), loss = 0.417486 (0.727 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:25:49.748875: step 2940/40890 (epoch 3/30), loss = 0.138385 (0.917 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:26:05.478814: step 2960/40890 (epoch 3/30), loss = 0.432290 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:26:21.806161: step 2980/40890 (epoch 3/30), loss = 0.496090 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:26:38.122532: step 3000/40890 (epoch 3/30), loss = 0.247792 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:26:54.423942: step 3020/40890 (epoch 3/30), loss = 0.333668 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:27:10.002357: step 3040/40890 (epoch 3/30), loss = 0.304283 (0.891 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:27:26.142201: step 3060/40890 (epoch 3/30), loss = 0.674939 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:27:41.775398: step 3080/40890 (epoch 3/30), loss = 0.275935 (0.709 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:27:57.545101: step 3100/40890 (epoch 3/30), loss = 0.274411 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:28:13.100506: step 3120/40890 (epoch 3/30), loss = 0.475181 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:28:28.635964: step 3140/40890 (epoch 3/30), loss = 0.321239 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:28:44.670091: step 3160/40890 (epoch 3/30), loss = 0.389389 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:29:00.750104: step 3180/40890 (epoch 3/30), loss = 0.587744 (0.906 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:29:16.764283: step 3200/40890 (epoch 3/30), loss = 0.691101 (0.846 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:29:32.698674: step 3220/40890 (epoch 3/30), loss = 0.456050 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:29:48.719889: step 3240/40890 (epoch 3/30), loss = 0.266487 (0.827 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:30:04.885174: step 3260/40890 (epoch 3/30), loss = 0.270900 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:30:20.761721: step 3280/40890 (epoch 3/30), loss = 0.567984 (0.665 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:30:36.912535: step 3300/40890 (epoch 3/30), loss = 0.414305 (0.925 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:30:52.641476: step 3320/40890 (epoch 3/30), loss = 0.346561 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:31:07.811422: step 3340/40890 (epoch 3/30), loss = 0.547545 (0.924 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:31:23.487561: step 3360/40890 (epoch 3/30), loss = 0.258181 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:31:38.967676: step 3380/40890 (epoch 3/30), loss = 0.278401 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:31:54.717609: step 3400/40890 (epoch 3/30), loss = 0.447350 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:32:11.007420: step 3420/40890 (epoch 3/30), loss = 0.345186 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:32:27.303844: step 3440/40890 (epoch 3/30), loss = 0.443105 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:32:44.032467: step 3460/40890 (epoch 3/30), loss = 0.459025 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:33:00.372774: step 3480/40890 (epoch 3/30), loss = 0.581531 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:33:16.409898: step 3500/40890 (epoch 3/30), loss = 0.536138 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:33:33.079832: step 3520/40890 (epoch 3/30), loss = 0.386719 (0.752 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:33:49.814630: step 3540/40890 (epoch 3/30), loss = 0.665045 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:34:06.274677: step 3560/40890 (epoch 3/30), loss = 0.384977 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:34:25.104328: step 3580/40890 (epoch 3/30), loss = 0.624420 (0.759 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:34:40.869173: step 3600/40890 (epoch 3/30), loss = 0.512193 (0.923 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:34:57.512669: step 3620/40890 (epoch 3/30), loss = 0.298791 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:35:14.271864: step 3640/40890 (epoch 3/30), loss = 0.413408 (0.761 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:35:30.433741: step 3660/40890 (epoch 3/30), loss = 0.498477 (0.694 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:35:47.352502: step 3680/40890 (epoch 3/30), loss = 0.392290 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:36:03.784570: step 3700/40890 (epoch 3/30), loss = 0.313288 (0.789 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:36:19.986247: step 3720/40890 (epoch 3/30), loss = 0.415201 (0.791 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:36:36.196901: step 3740/40890 (epoch 3/30), loss = 0.442739 (0.656 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:36:52.634946: step 3760/40890 (epoch 3/30), loss = 0.137002 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:37:08.048731: step 3780/40890 (epoch 3/30), loss = 0.266545 (0.709 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:37:23.767209: step 3800/40890 (epoch 3/30), loss = 0.470729 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:37:39.573460: step 3820/40890 (epoch 3/30), loss = 0.548967 (0.716 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:37:55.523809: step 3840/40890 (epoch 3/30), loss = 0.516246 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:38:11.830720: step 3860/40890 (epoch 3/30), loss = 0.412175 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:38:27.620499: step 3880/40890 (epoch 3/30), loss = 0.381912 (0.929 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:38:42.958486: step 3900/40890 (epoch 3/30), loss = 0.391199 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:38:58.957705: step 3920/40890 (epoch 3/30), loss = 0.479034 (0.737 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:39:15.059721: step 3940/40890 (epoch 3/30), loss = 0.416537 (0.636 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:39:31.360134: step 3960/40890 (epoch 3/30), loss = 0.436230 (0.680 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:39:47.791041: step 3980/40890 (epoch 3/30), loss = 0.353541 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:40:02.825839: step 4000/40890 (epoch 3/30), loss = 0.488590 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:40:18.501991: step 4020/40890 (epoch 3/30), loss = 0.399689 (0.783 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:40:34.035456: step 4040/40890 (epoch 3/30), loss = 0.532768 (0.776 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:40:50.075417: step 4060/40890 (epoch 3/30), loss = 0.117878 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:41:05.898107: step 4080/40890 (epoch 3/30), loss = 0.328493 (0.832 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.299%\n",
      "   Recall (micro): 49.724%\n",
      "       F1 (micro): 56.827%\n",
      "epoch 3: train_loss = 0.421486, dev_loss = 0.493718, dev_f1 = 0.5683\n",
      "model saved to ./save_models/01/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-11 23:41:53.937643: step 4100/40890 (epoch 4/30), loss = 0.275601 (0.941 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:42:09.767324: step 4120/40890 (epoch 4/30), loss = 0.374733 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:42:25.620932: step 4140/40890 (epoch 4/30), loss = 0.393457 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:42:41.185314: step 4160/40890 (epoch 4/30), loss = 0.223970 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:42:58.200879: step 4180/40890 (epoch 4/30), loss = 0.381992 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:43:14.054487: step 4200/40890 (epoch 4/30), loss = 0.504645 (0.633 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:43:30.005834: step 4220/40890 (epoch 4/30), loss = 0.542174 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:43:46.270848: step 4240/40890 (epoch 4/30), loss = 0.291783 (0.898 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:44:02.892549: step 4260/40890 (epoch 4/30), loss = 0.580566 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:44:20.663031: step 4280/40890 (epoch 4/30), loss = 0.325113 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:44:37.136987: step 4300/40890 (epoch 4/30), loss = 0.407814 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:44:53.361111: step 4320/40890 (epoch 4/30), loss = 0.780919 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:45:09.457088: step 4340/40890 (epoch 4/30), loss = 0.507160 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:45:26.020798: step 4360/40890 (epoch 4/30), loss = 0.368053 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:45:42.444881: step 4380/40890 (epoch 4/30), loss = 0.293137 (0.836 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:45:58.091044: step 4400/40890 (epoch 4/30), loss = 0.161205 (0.884 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:46:14.144118: step 4420/40890 (epoch 4/30), loss = 0.391757 (0.739 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:46:30.431567: step 4440/40890 (epoch 4/30), loss = 0.421467 (0.838 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:46:46.911500: step 4460/40890 (epoch 4/30), loss = 0.293350 (0.835 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:47:02.771092: step 4480/40890 (epoch 4/30), loss = 0.199532 (0.785 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:47:18.683543: step 4500/40890 (epoch 4/30), loss = 0.645075 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:47:34.508229: step 4520/40890 (epoch 4/30), loss = 0.652143 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:47:50.927335: step 4540/40890 (epoch 4/30), loss = 0.512288 (0.843 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:48:07.078656: step 4560/40890 (epoch 4/30), loss = 0.711020 (0.648 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:48:23.012051: step 4580/40890 (epoch 4/30), loss = 0.276421 (0.930 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:48:38.827760: step 4600/40890 (epoch 4/30), loss = 0.504311 (0.786 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:48:54.998227: step 4620/40890 (epoch 4/30), loss = 0.807586 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:49:11.097180: step 4640/40890 (epoch 4/30), loss = 0.300543 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:49:26.899924: step 4660/40890 (epoch 4/30), loss = 0.207720 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:49:42.933052: step 4680/40890 (epoch 4/30), loss = 0.255396 (0.886 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:49:58.045642: step 4700/40890 (epoch 4/30), loss = 0.614088 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:50:13.698806: step 4720/40890 (epoch 4/30), loss = 0.304594 (0.730 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:50:29.244239: step 4740/40890 (epoch 4/30), loss = 0.313247 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:50:44.991132: step 4760/40890 (epoch 4/30), loss = 0.507653 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:51:01.221243: step 4780/40890 (epoch 4/30), loss = 0.367093 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:51:17.437881: step 4800/40890 (epoch 4/30), loss = 0.334653 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:51:33.805115: step 4820/40890 (epoch 4/30), loss = 0.617099 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:51:49.797859: step 4840/40890 (epoch 4/30), loss = 0.416411 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:52:05.578662: step 4860/40890 (epoch 4/30), loss = 0.314532 (0.942 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:52:21.870099: step 4880/40890 (epoch 4/30), loss = 0.339998 (0.936 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:52:38.766920: step 4900/40890 (epoch 4/30), loss = 0.741088 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:52:54.836947: step 4920/40890 (epoch 4/30), loss = 0.351805 (0.714 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:53:11.991584: step 4940/40890 (epoch 4/30), loss = 0.143959 (0.812 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:53:27.124121: step 4960/40890 (epoch 4/30), loss = 0.217853 (0.555 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:53:44.036684: step 4980/40890 (epoch 4/30), loss = 0.295341 (0.719 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:54:00.874660: step 5000/40890 (epoch 4/30), loss = 0.252176 (0.992 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:54:18.105668: step 5020/40890 (epoch 4/30), loss = 0.335462 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:54:35.335596: step 5040/40890 (epoch 4/30), loss = 0.374004 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:54:52.131201: step 5060/40890 (epoch 4/30), loss = 0.446439 (0.826 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:55:08.382746: step 5080/40890 (epoch 4/30), loss = 0.414594 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:55:24.980437: step 5100/40890 (epoch 4/30), loss = 0.419140 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:55:41.683282: step 5120/40890 (epoch 4/30), loss = 0.454593 (0.976 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:55:56.987359: step 5140/40890 (epoch 4/30), loss = 0.429825 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:56:12.721288: step 5160/40890 (epoch 4/30), loss = 0.405923 (0.877 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:56:28.954880: step 5180/40890 (epoch 4/30), loss = 0.162284 (0.656 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:56:45.449773: step 5200/40890 (epoch 4/30), loss = 0.392240 (0.871 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:57:01.927712: step 5220/40890 (epoch 4/30), loss = 0.553670 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:57:17.770350: step 5240/40890 (epoch 4/30), loss = 0.375464 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:57:33.087393: step 5260/40890 (epoch 4/30), loss = 0.598051 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:57:48.896121: step 5280/40890 (epoch 4/30), loss = 0.478750 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:58:05.098306: step 5300/40890 (epoch 4/30), loss = 0.369798 (0.684 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:58:21.380822: step 5320/40890 (epoch 4/30), loss = 0.471143 (0.604 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:58:37.827844: step 5340/40890 (epoch 4/30), loss = 0.374802 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:58:52.698085: step 5360/40890 (epoch 4/30), loss = 0.214692 (0.632 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:59:08.680852: step 5380/40890 (epoch 4/30), loss = 0.477404 (0.840 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:59:24.209330: step 5400/40890 (epoch 4/30), loss = 0.448528 (0.780 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:59:40.528693: step 5420/40890 (epoch 4/30), loss = 0.267585 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-11 23:59:56.286557: step 5440/40890 (epoch 4/30), loss = 0.318826 (0.825 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 65.087%\n",
      "   Recall (micro): 55.831%\n",
      "       F1 (micro): 60.105%\n",
      "epoch 4: train_loss = 0.398810, dev_loss = 0.483216, dev_f1 = 0.6010\n",
      "model saved to ./save_models/01/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 00:00:48.776860: step 5460/40890 (epoch 5/30), loss = 0.418857 (0.705 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:01:04.531732: step 5480/40890 (epoch 5/30), loss = 0.288775 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:01:20.445523: step 5500/40890 (epoch 5/30), loss = 0.411806 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:01:35.653862: step 5520/40890 (epoch 5/30), loss = 0.628193 (0.681 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:01:52.031076: step 5540/40890 (epoch 5/30), loss = 0.294569 (0.908 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:02:07.960482: step 5560/40890 (epoch 5/30), loss = 0.349841 (0.835 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:02:23.599668: step 5580/40890 (epoch 5/30), loss = 0.481392 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:02:39.967907: step 5600/40890 (epoch 5/30), loss = 0.212178 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:02:56.295248: step 5620/40890 (epoch 5/30), loss = 0.315103 (0.727 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:03:13.015539: step 5640/40890 (epoch 5/30), loss = 0.317224 (0.697 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:03:29.401723: step 5660/40890 (epoch 5/30), loss = 0.454567 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:03:45.475742: step 5680/40890 (epoch 5/30), loss = 0.537975 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:04:02.381055: step 5700/40890 (epoch 5/30), loss = 0.445455 (0.938 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:04:20.617799: step 5720/40890 (epoch 5/30), loss = 0.183146 (0.969 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:04:36.996005: step 5740/40890 (epoch 5/30), loss = 0.417532 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:04:53.251639: step 5760/40890 (epoch 5/30), loss = 0.391424 (0.817 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:05:09.423368: step 5780/40890 (epoch 5/30), loss = 0.344195 (0.688 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:05:25.622054: step 5800/40890 (epoch 5/30), loss = 0.237568 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:05:42.379654: step 5820/40890 (epoch 5/30), loss = 0.357285 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:05:58.297101: step 5840/40890 (epoch 5/30), loss = 0.246691 (0.589 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:06:14.077911: step 5860/40890 (epoch 5/30), loss = 0.386444 (0.955 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:06:30.049204: step 5880/40890 (epoch 5/30), loss = 0.404005 (0.783 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:06:47.034786: step 5900/40890 (epoch 5/30), loss = 0.426119 (0.772 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:07:03.763056: step 5920/40890 (epoch 5/30), loss = 0.353888 (0.684 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:07:20.078793: step 5940/40890 (epoch 5/30), loss = 0.518769 (0.848 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:07:35.980273: step 5960/40890 (epoch 5/30), loss = 0.346594 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:07:51.972511: step 5980/40890 (epoch 5/30), loss = 0.480016 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:08:08.533228: step 6000/40890 (epoch 5/30), loss = 0.240315 (0.643 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:08:24.411880: step 6020/40890 (epoch 5/30), loss = 0.765525 (0.537 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:08:40.303387: step 6040/40890 (epoch 5/30), loss = 0.276276 (0.714 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:08:55.543635: step 6060/40890 (epoch 5/30), loss = 0.446086 (0.614 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:09:11.358348: step 6080/40890 (epoch 5/30), loss = 0.461679 (0.685 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:09:26.872862: step 6100/40890 (epoch 5/30), loss = 0.253162 (0.727 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:09:42.686583: step 6120/40890 (epoch 5/30), loss = 0.462462 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:09:59.015919: step 6140/40890 (epoch 5/30), loss = 0.435344 (0.979 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:10:15.297383: step 6160/40890 (epoch 5/30), loss = 0.413134 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:10:31.717477: step 6180/40890 (epoch 5/30), loss = 0.347301 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:10:47.866079: step 6200/40890 (epoch 5/30), loss = 0.672310 (0.733 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:11:03.159185: step 6220/40890 (epoch 5/30), loss = 0.382054 (0.563 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:11:19.700468: step 6240/40890 (epoch 5/30), loss = 0.537120 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:11:36.180402: step 6260/40890 (epoch 5/30), loss = 0.345838 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:11:51.732824: step 6280/40890 (epoch 5/30), loss = 0.567585 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:12:08.396266: step 6300/40890 (epoch 5/30), loss = 0.591231 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:12:23.714652: step 6320/40890 (epoch 5/30), loss = 0.352950 (0.877 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:12:39.907410: step 6340/40890 (epoch 5/30), loss = 0.379642 (0.908 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:12:56.755359: step 6360/40890 (epoch 5/30), loss = 0.334634 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:13:13.035871: step 6380/40890 (epoch 5/30), loss = 0.592362 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:13:29.544727: step 6400/40890 (epoch 5/30), loss = 0.703794 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:13:46.408639: step 6420/40890 (epoch 5/30), loss = 0.404229 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:14:03.012901: step 6440/40890 (epoch 5/30), loss = 0.279953 (0.800 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:14:20.855191: step 6460/40890 (epoch 5/30), loss = 0.480351 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:14:37.756000: step 6480/40890 (epoch 5/30), loss = 0.444517 (0.894 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:14:53.145920: step 6500/40890 (epoch 5/30), loss = 0.283337 (0.840 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:15:08.878859: step 6520/40890 (epoch 5/30), loss = 0.326371 (0.694 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:15:25.622097: step 6540/40890 (epoch 5/30), loss = 0.541308 (0.866 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:15:41.693124: step 6560/40890 (epoch 5/30), loss = 0.339269 (0.740 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:15:58.363548: step 6580/40890 (epoch 5/30), loss = 0.578321 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:16:14.297284: step 6600/40890 (epoch 5/30), loss = 0.536641 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:16:29.926492: step 6620/40890 (epoch 5/30), loss = 0.268318 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:16:46.006495: step 6640/40890 (epoch 5/30), loss = 0.489877 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:17:02.710828: step 6660/40890 (epoch 5/30), loss = 0.305741 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:17:19.567754: step 6680/40890 (epoch 5/30), loss = 0.208868 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:17:35.734525: step 6700/40890 (epoch 5/30), loss = 0.292177 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:17:51.077498: step 6720/40890 (epoch 5/30), loss = 0.404527 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:18:06.810439: step 6740/40890 (epoch 5/30), loss = 0.636269 (0.923 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:18:22.145434: step 6760/40890 (epoch 5/30), loss = 0.253901 (0.745 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:18:38.309213: step 6780/40890 (epoch 5/30), loss = 0.426630 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:18:54.112955: step 6800/40890 (epoch 5/30), loss = 0.277840 (0.658 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.608%\n",
      "   Recall (micro): 56.144%\n",
      "       F1 (micro): 60.930%\n",
      "epoch 5: train_loss = 0.381972, dev_loss = 0.468309, dev_f1 = 0.6093\n",
      "model saved to ./save_models/01/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 00:19:43.097173: step 6820/40890 (epoch 6/30), loss = 0.259814 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:19:58.844126: step 6840/40890 (epoch 6/30), loss = 0.368494 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:20:14.491291: step 6860/40890 (epoch 6/30), loss = 0.223640 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:20:29.933998: step 6880/40890 (epoch 6/30), loss = 0.289037 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:20:46.226487: step 6900/40890 (epoch 6/30), loss = 0.297718 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:21:02.002303: step 6920/40890 (epoch 6/30), loss = 0.377839 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:21:17.712295: step 6940/40890 (epoch 6/30), loss = 0.478829 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:21:33.593828: step 6960/40890 (epoch 6/30), loss = 0.457449 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:21:49.482685: step 6980/40890 (epoch 6/30), loss = 0.272999 (0.657 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:22:05.739579: step 7000/40890 (epoch 6/30), loss = 0.558728 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:22:21.597177: step 7020/40890 (epoch 6/30), loss = 0.245624 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:22:37.765942: step 7040/40890 (epoch 6/30), loss = 0.399017 (0.957 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:22:54.067972: step 7060/40890 (epoch 6/30), loss = 0.430332 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:23:10.326498: step 7080/40890 (epoch 6/30), loss = 0.367391 (0.765 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:23:26.400517: step 7100/40890 (epoch 6/30), loss = 0.560456 (0.730 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:23:42.788048: step 7120/40890 (epoch 6/30), loss = 0.448574 (0.703 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:23:58.868390: step 7140/40890 (epoch 6/30), loss = 0.394462 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:24:16.035486: step 7160/40890 (epoch 6/30), loss = 0.314947 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:24:33.223527: step 7180/40890 (epoch 6/30), loss = 0.318806 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:24:49.409253: step 7200/40890 (epoch 6/30), loss = 0.106387 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:25:04.876893: step 7220/40890 (epoch 6/30), loss = 0.399481 (0.758 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:25:20.878615: step 7240/40890 (epoch 6/30), loss = 0.181411 (0.743 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:25:37.838265: step 7260/40890 (epoch 6/30), loss = 0.455352 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:25:54.718129: step 7280/40890 (epoch 6/30), loss = 0.208879 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:26:10.998602: step 7300/40890 (epoch 6/30), loss = 0.331109 (0.922 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:26:27.219229: step 7320/40890 (epoch 6/30), loss = 0.295791 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:26:43.619376: step 7340/40890 (epoch 6/30), loss = 0.443915 (0.663 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:27:00.451368: step 7360/40890 (epoch 6/30), loss = 0.404860 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:27:16.583245: step 7380/40890 (epoch 6/30), loss = 0.206828 (0.902 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:27:32.485723: step 7400/40890 (epoch 6/30), loss = 0.426176 (0.652 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:27:48.067566: step 7420/40890 (epoch 6/30), loss = 0.467622 (0.848 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:28:03.843260: step 7440/40890 (epoch 6/30), loss = 0.246782 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:28:19.232175: step 7460/40890 (epoch 6/30), loss = 0.337675 (0.725 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:28:34.901277: step 7480/40890 (epoch 6/30), loss = 0.407057 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:28:50.934740: step 7500/40890 (epoch 6/30), loss = 0.584157 (0.766 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:29:07.393729: step 7520/40890 (epoch 6/30), loss = 0.313807 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:29:23.700126: step 7540/40890 (epoch 6/30), loss = 0.493526 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:29:40.039941: step 7560/40890 (epoch 6/30), loss = 0.461759 (0.908 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:29:55.570257: step 7580/40890 (epoch 6/30), loss = 0.385070 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:30:11.453785: step 7600/40890 (epoch 6/30), loss = 0.347533 (0.616 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:30:28.045420: step 7620/40890 (epoch 6/30), loss = 0.325577 (0.769 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:30:43.797352: step 7640/40890 (epoch 6/30), loss = 0.245421 (0.786 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:31:00.195512: step 7660/40890 (epoch 6/30), loss = 0.318786 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:31:15.716065: step 7680/40890 (epoch 6/30), loss = 0.581025 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:31:31.363225: step 7700/40890 (epoch 6/30), loss = 0.417176 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:31:47.726819: step 7720/40890 (epoch 6/30), loss = 0.369341 (0.915 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:32:03.949328: step 7740/40890 (epoch 6/30), loss = 0.101382 (0.785 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:32:19.752079: step 7760/40890 (epoch 6/30), loss = 0.532642 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:32:36.662860: step 7780/40890 (epoch 6/30), loss = 0.505620 (0.899 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:32:52.956298: step 7800/40890 (epoch 6/30), loss = 0.487139 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:33:09.301592: step 7820/40890 (epoch 6/30), loss = 0.330932 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:33:25.820421: step 7840/40890 (epoch 6/30), loss = 0.272292 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:33:41.182352: step 7860/40890 (epoch 6/30), loss = 0.524822 (0.807 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:33:57.027063: step 7880/40890 (epoch 6/30), loss = 0.481835 (0.771 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:34:15.330973: step 7900/40890 (epoch 6/30), loss = 0.317646 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:34:32.274667: step 7920/40890 (epoch 6/30), loss = 0.312945 (0.835 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:34:49.125688: step 7940/40890 (epoch 6/30), loss = 0.485920 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:35:05.114941: step 7960/40890 (epoch 6/30), loss = 0.228309 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:35:20.984578: step 7980/40890 (epoch 6/30), loss = 0.495646 (0.712 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:35:37.067573: step 8000/40890 (epoch 6/30), loss = 0.622250 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:35:53.476696: step 8020/40890 (epoch 6/30), loss = 0.187788 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:36:10.251905: step 8040/40890 (epoch 6/30), loss = 0.496175 (0.942 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:36:26.806639: step 8060/40890 (epoch 6/30), loss = 0.445377 (0.839 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:36:43.008729: step 8080/40890 (epoch 6/30), loss = 0.462069 (0.879 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:36:58.743654: step 8100/40890 (epoch 6/30), loss = 0.199282 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:37:14.514484: step 8120/40890 (epoch 6/30), loss = 0.239903 (0.804 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:37:30.720151: step 8140/40890 (epoch 6/30), loss = 0.253083 (0.880 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:37:46.831071: step 8160/40890 (epoch 6/30), loss = 0.461033 (0.850 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.956%\n",
      "   Recall (micro): 58.113%\n",
      "       F1 (micro): 62.222%\n",
      "epoch 6: train_loss = 0.365855, dev_loss = 0.454300, dev_f1 = 0.6222\n",
      "model saved to ./save_models/01/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 00:38:35.864453: step 8180/40890 (epoch 7/30), loss = 0.455318 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:38:52.351368: step 8200/40890 (epoch 7/30), loss = 0.403350 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:39:07.890156: step 8220/40890 (epoch 7/30), loss = 0.524517 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:39:23.711849: step 8240/40890 (epoch 7/30), loss = 0.293119 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:39:39.670525: step 8260/40890 (epoch 7/30), loss = 0.218260 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:39:55.425436: step 8280/40890 (epoch 7/30), loss = 0.242785 (0.747 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:40:11.117532: step 8300/40890 (epoch 7/30), loss = 0.305241 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:40:26.929253: step 8320/40890 (epoch 7/30), loss = 0.376624 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:40:42.912529: step 8340/40890 (epoch 7/30), loss = 0.154085 (0.821 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:40:58.994526: step 8360/40890 (epoch 7/30), loss = 0.293564 (0.765 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:41:14.927444: step 8380/40890 (epoch 7/30), loss = 0.483948 (0.788 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:41:30.747143: step 8400/40890 (epoch 7/30), loss = 0.372303 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:41:46.530938: step 8420/40890 (epoch 7/30), loss = 0.532751 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:42:02.457351: step 8440/40890 (epoch 7/30), loss = 0.385513 (0.672 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:42:18.029055: step 8460/40890 (epoch 7/30), loss = 0.423319 (0.737 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:42:34.342434: step 8480/40890 (epoch 7/30), loss = 0.246929 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:42:50.506213: step 8500/40890 (epoch 7/30), loss = 0.364285 (0.919 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:43:06.456562: step 8520/40890 (epoch 7/30), loss = 0.236822 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:43:23.132379: step 8540/40890 (epoch 7/30), loss = 0.355149 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:43:39.215471: step 8560/40890 (epoch 7/30), loss = 0.273841 (0.740 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:43:55.144887: step 8580/40890 (epoch 7/30), loss = 0.305107 (0.803 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:44:11.979887: step 8600/40890 (epoch 7/30), loss = 0.244700 (0.831 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:44:29.566860: step 8620/40890 (epoch 7/30), loss = 0.341720 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:44:46.399849: step 8640/40890 (epoch 7/30), loss = 0.269114 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:45:02.791533: step 8660/40890 (epoch 7/30), loss = 0.112468 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:45:19.129724: step 8680/40890 (epoch 7/30), loss = 0.526380 (0.771 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:45:35.695428: step 8700/40890 (epoch 7/30), loss = 0.313420 (0.919 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:45:52.613062: step 8720/40890 (epoch 7/30), loss = 0.464806 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:46:08.740937: step 8740/40890 (epoch 7/30), loss = 0.368389 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:46:25.101191: step 8760/40890 (epoch 7/30), loss = 0.335016 (0.719 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:46:40.929929: step 8780/40890 (epoch 7/30), loss = 0.286065 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:46:56.934196: step 8800/40890 (epoch 7/30), loss = 0.268130 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:47:13.021687: step 8820/40890 (epoch 7/30), loss = 0.466492 (0.754 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:47:28.906212: step 8840/40890 (epoch 7/30), loss = 0.184497 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:47:44.835618: step 8860/40890 (epoch 7/30), loss = 0.590949 (0.701 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:48:01.185231: step 8880/40890 (epoch 7/30), loss = 0.393583 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:48:17.806791: step 8900/40890 (epoch 7/30), loss = 0.301043 (0.944 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:48:34.003482: step 8920/40890 (epoch 7/30), loss = 0.469305 (0.919 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:48:49.558887: step 8940/40890 (epoch 7/30), loss = 0.327290 (0.695 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:49:05.578058: step 8960/40890 (epoch 7/30), loss = 0.171741 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:49:22.155730: step 8980/40890 (epoch 7/30), loss = 0.385909 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:49:37.829819: step 9000/40890 (epoch 7/30), loss = 0.401028 (0.721 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:49:54.189084: step 9020/40890 (epoch 7/30), loss = 0.264062 (0.886 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:50:10.148463: step 9040/40890 (epoch 7/30), loss = 0.307965 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:50:25.572221: step 9060/40890 (epoch 7/30), loss = 0.427882 (0.663 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:50:41.652231: step 9080/40890 (epoch 7/30), loss = 0.304764 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:50:57.953642: step 9100/40890 (epoch 7/30), loss = 0.212775 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:51:13.831186: step 9120/40890 (epoch 7/30), loss = 0.274550 (0.923 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:51:30.487648: step 9140/40890 (epoch 7/30), loss = 0.295656 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:51:46.429025: step 9160/40890 (epoch 7/30), loss = 0.509729 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:52:02.209828: step 9180/40890 (epoch 7/30), loss = 0.539135 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:52:18.126268: step 9200/40890 (epoch 7/30), loss = 0.175644 (0.804 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:52:33.662724: step 9220/40890 (epoch 7/30), loss = 0.408481 (0.674 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:52:49.380695: step 9240/40890 (epoch 7/30), loss = 0.400811 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:53:05.923467: step 9260/40890 (epoch 7/30), loss = 0.133353 (0.849 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:53:21.958104: step 9280/40890 (epoch 7/30), loss = 0.212455 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:53:38.693861: step 9300/40890 (epoch 7/30), loss = 0.699133 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:53:55.119938: step 9320/40890 (epoch 7/30), loss = 0.226291 (0.904 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:54:11.792169: step 9340/40890 (epoch 7/30), loss = 0.247753 (0.997 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:54:28.707937: step 9360/40890 (epoch 7/30), loss = 0.403505 (0.910 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:54:45.113441: step 9380/40890 (epoch 7/30), loss = 0.337132 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:55:01.848782: step 9400/40890 (epoch 7/30), loss = 0.439681 (0.863 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:55:18.739628: step 9420/40890 (epoch 7/30), loss = 0.164914 (0.911 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:55:35.008127: step 9440/40890 (epoch 7/30), loss = 0.341927 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:55:50.677229: step 9460/40890 (epoch 7/30), loss = 0.277631 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:56:06.668468: step 9480/40890 (epoch 7/30), loss = 0.256558 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:56:22.876130: step 9500/40890 (epoch 7/30), loss = 0.341354 (0.907 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:56:39.525610: step 9520/40890 (epoch 7/30), loss = 0.271999 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:56:56.090823: step 9540/40890 (epoch 7/30), loss = 0.501281 (0.659 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.884%\n",
      "   Recall (micro): 57.542%\n",
      "       F1 (micro): 62.704%\n",
      "epoch 7: train_loss = 0.355265, dev_loss = 0.446472, dev_f1 = 0.6270\n",
      "model saved to ./save_models/01/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 00:57:46.203932: step 9560/40890 (epoch 8/30), loss = 0.239692 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:58:01.557876: step 9580/40890 (epoch 8/30), loss = 0.243645 (0.683 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:58:17.562082: step 9600/40890 (epoch 8/30), loss = 0.419820 (0.727 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:58:33.736831: step 9620/40890 (epoch 8/30), loss = 0.212347 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:58:49.577474: step 9640/40890 (epoch 8/30), loss = 0.231479 (0.845 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:59:05.094491: step 9660/40890 (epoch 8/30), loss = 0.258210 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:59:20.855856: step 9680/40890 (epoch 8/30), loss = 0.450976 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:59:36.773292: step 9700/40890 (epoch 8/30), loss = 0.276314 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-12 00:59:53.102636: step 9720/40890 (epoch 8/30), loss = 0.134051 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:00:09.228531: step 9740/40890 (epoch 8/30), loss = 0.476756 (0.738 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:00:25.051222: step 9760/40890 (epoch 8/30), loss = 0.429151 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:00:40.774568: step 9780/40890 (epoch 8/30), loss = 0.477086 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:00:56.679085: step 9800/40890 (epoch 8/30), loss = 0.294287 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:01:12.585552: step 9820/40890 (epoch 8/30), loss = 0.346562 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:01:28.325465: step 9840/40890 (epoch 8/30), loss = 0.386776 (0.639 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:01:43.961654: step 9860/40890 (epoch 8/30), loss = 0.422075 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:01:59.849171: step 9880/40890 (epoch 8/30), loss = 0.378456 (0.676 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:02:15.919584: step 9900/40890 (epoch 8/30), loss = 0.292377 (0.924 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:02:31.791145: step 9920/40890 (epoch 8/30), loss = 0.316531 (0.924 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:02:47.820286: step 9940/40890 (epoch 8/30), loss = 0.281410 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:03:03.990625: step 9960/40890 (epoch 8/30), loss = 0.290862 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:03:20.642137: step 9980/40890 (epoch 8/30), loss = 0.418532 (0.826 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:03:37.017350: step 10000/40890 (epoch 8/30), loss = 0.145398 (0.763 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:03:53.588410: step 10020/40890 (epoch 8/30), loss = 0.467594 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:04:10.570877: step 10040/40890 (epoch 8/30), loss = 0.195923 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:04:28.660506: step 10060/40890 (epoch 8/30), loss = 0.320311 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:04:45.503474: step 10080/40890 (epoch 8/30), loss = 0.327843 (0.716 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:05:01.837796: step 10100/40890 (epoch 8/30), loss = 0.372301 (0.885 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:05:18.323064: step 10120/40890 (epoch 8/30), loss = 0.352694 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:05:34.101872: step 10140/40890 (epoch 8/30), loss = 0.341749 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:05:49.895018: step 10160/40890 (epoch 8/30), loss = 0.373740 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:06:06.176482: step 10180/40890 (epoch 8/30), loss = 0.453754 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:06:22.032089: step 10200/40890 (epoch 8/30), loss = 0.253266 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:06:38.343473: step 10220/40890 (epoch 8/30), loss = 0.294602 (0.843 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:06:55.204389: step 10240/40890 (epoch 8/30), loss = 0.263405 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:07:11.933656: step 10260/40890 (epoch 8/30), loss = 0.294856 (0.935 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:07:28.627017: step 10280/40890 (epoch 8/30), loss = 0.523621 (0.907 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:07:44.070230: step 10300/40890 (epoch 8/30), loss = 0.186289 (0.706 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:07:59.940793: step 10320/40890 (epoch 8/30), loss = 0.457127 (0.635 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:08:16.917778: step 10340/40890 (epoch 8/30), loss = 0.304737 (0.934 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:08:32.507094: step 10360/40890 (epoch 8/30), loss = 0.506759 (0.702 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:08:48.725726: step 10380/40890 (epoch 8/30), loss = 0.417697 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:09:04.868564: step 10400/40890 (epoch 8/30), loss = 0.431522 (0.736 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:09:20.380086: step 10420/40890 (epoch 8/30), loss = 0.558927 (0.819 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:09:36.276580: step 10440/40890 (epoch 8/30), loss = 0.263409 (0.657 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:09:52.779960: step 10460/40890 (epoch 8/30), loss = 0.205322 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:10:08.560272: step 10480/40890 (epoch 8/30), loss = 0.475351 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:10:25.240673: step 10500/40890 (epoch 8/30), loss = 0.427641 (0.781 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:10:41.397982: step 10520/40890 (epoch 8/30), loss = 0.217183 (0.827 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:10:56.979318: step 10540/40890 (epoch 8/30), loss = 0.454225 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:11:12.989841: step 10560/40890 (epoch 8/30), loss = 0.129152 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:11:28.729754: step 10580/40890 (epoch 8/30), loss = 0.462423 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:11:43.907170: step 10600/40890 (epoch 8/30), loss = 0.502125 (0.720 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:11:59.734847: step 10620/40890 (epoch 8/30), loss = 0.420481 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:12:15.454367: step 10640/40890 (epoch 8/30), loss = 0.343542 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:12:31.873462: step 10660/40890 (epoch 8/30), loss = 0.210761 (0.888 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:12:48.205803: step 10680/40890 (epoch 8/30), loss = 0.192518 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:13:04.111272: step 10700/40890 (epoch 8/30), loss = 0.441236 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:13:19.926001: step 10720/40890 (epoch 8/30), loss = 0.256897 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:13:36.208462: step 10740/40890 (epoch 8/30), loss = 0.384082 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:13:52.898875: step 10760/40890 (epoch 8/30), loss = 0.236898 (0.921 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:14:10.453967: step 10780/40890 (epoch 8/30), loss = 0.284314 (0.951 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:14:27.953175: step 10800/40890 (epoch 8/30), loss = 0.446677 (0.806 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:14:43.688106: step 10820/40890 (epoch 8/30), loss = 0.268136 (0.754 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:14:59.970567: step 10840/40890 (epoch 8/30), loss = 0.293564 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:15:15.893496: step 10860/40890 (epoch 8/30), loss = 0.364574 (0.884 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:15:32.256743: step 10880/40890 (epoch 8/30), loss = 0.310967 (0.661 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:15:49.051347: step 10900/40890 (epoch 8/30), loss = 0.258100 (0.886 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.263%\n",
      "   Recall (micro): 57.965%\n",
      "       F1 (micro): 62.694%\n",
      "epoch 8: train_loss = 0.342731, dev_loss = 0.441845, dev_f1 = 0.6269\n",
      "model saved to ./save_models/01/checkpoint_epoch_8.pt\n",
      "\n",
      "2020-10-12 01:16:39.464737: step 10920/40890 (epoch 9/30), loss = 0.266192 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:16:55.226591: step 10940/40890 (epoch 9/30), loss = 0.450456 (0.651 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:17:11.623745: step 10960/40890 (epoch 9/30), loss = 0.095601 (0.722 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:17:27.704746: step 10980/40890 (epoch 9/30), loss = 0.472472 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:17:43.726963: step 11000/40890 (epoch 9/30), loss = 0.216263 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:17:59.148726: step 11020/40890 (epoch 9/30), loss = 0.338218 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:18:14.995362: step 11040/40890 (epoch 9/30), loss = 0.479629 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:18:30.936731: step 11060/40890 (epoch 9/30), loss = 0.427800 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:18:47.093528: step 11080/40890 (epoch 9/30), loss = 0.334658 (0.905 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:19:03.240352: step 11100/40890 (epoch 9/30), loss = 0.246736 (0.893 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:19:19.202669: step 11120/40890 (epoch 9/30), loss = 0.303477 (0.758 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:19:34.873766: step 11140/40890 (epoch 9/30), loss = 0.292444 (0.827 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:19:50.775756: step 11160/40890 (epoch 9/30), loss = 0.406200 (0.668 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:20:06.696518: step 11180/40890 (epoch 9/30), loss = 0.278751 (0.692 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:20:22.734146: step 11200/40890 (epoch 9/30), loss = 0.555708 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:20:37.940485: step 11220/40890 (epoch 9/30), loss = 0.350135 (0.717 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:20:53.901805: step 11240/40890 (epoch 9/30), loss = 0.318442 (0.691 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:21:09.803371: step 11260/40890 (epoch 9/30), loss = 0.501510 (0.839 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:21:25.334841: step 11280/40890 (epoch 9/30), loss = 0.139101 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:21:41.022401: step 11300/40890 (epoch 9/30), loss = 0.555553 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:21:56.535919: step 11320/40890 (epoch 9/30), loss = 0.378826 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:22:12.599325: step 11340/40890 (epoch 9/30), loss = 0.416449 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:22:28.694636: step 11360/40890 (epoch 9/30), loss = 0.243945 (0.728 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:22:45.196510: step 11380/40890 (epoch 9/30), loss = 0.203337 (0.854 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:23:01.733292: step 11400/40890 (epoch 9/30), loss = 0.210545 (0.767 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:23:18.300507: step 11420/40890 (epoch 9/30), loss = 0.262844 (0.741 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:23:35.024787: step 11440/40890 (epoch 9/30), loss = 0.331340 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:23:51.275334: step 11460/40890 (epoch 9/30), loss = 0.637041 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:24:08.186121: step 11480/40890 (epoch 9/30), loss = 0.371244 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:24:25.534732: step 11500/40890 (epoch 9/30), loss = 0.545870 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:24:41.572354: step 11520/40890 (epoch 9/30), loss = 0.320221 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:24:57.763146: step 11540/40890 (epoch 9/30), loss = 0.254836 (0.785 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:25:13.703523: step 11560/40890 (epoch 9/30), loss = 0.421842 (0.714 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:25:29.823419: step 11580/40890 (epoch 9/30), loss = 0.359586 (0.760 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:25:46.865848: step 11600/40890 (epoch 9/30), loss = 0.261507 (0.914 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:26:03.324927: step 11620/40890 (epoch 9/30), loss = 0.422978 (0.916 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:26:19.975409: step 11640/40890 (epoch 9/30), loss = 0.305364 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:26:36.213988: step 11660/40890 (epoch 9/30), loss = 0.456064 (0.663 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:26:52.636076: step 11680/40890 (epoch 9/30), loss = 0.269938 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:27:09.442208: step 11700/40890 (epoch 9/30), loss = 0.382321 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:27:26.071748: step 11720/40890 (epoch 9/30), loss = 0.499482 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:27:41.807523: step 11740/40890 (epoch 9/30), loss = 0.254723 (0.773 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:27:58.294438: step 11760/40890 (epoch 9/30), loss = 0.359485 (0.874 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:28:13.739763: step 11780/40890 (epoch 9/30), loss = 0.410168 (0.913 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:28:29.824753: step 11800/40890 (epoch 9/30), loss = 0.266491 (0.785 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:28:46.041390: step 11820/40890 (epoch 9/30), loss = 0.469182 (0.681 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:29:01.861603: step 11840/40890 (epoch 9/30), loss = 0.268112 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:29:18.388412: step 11860/40890 (epoch 9/30), loss = 0.237517 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:29:34.607043: step 11880/40890 (epoch 9/30), loss = 0.384411 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:29:50.337000: step 11900/40890 (epoch 9/30), loss = 0.309616 (0.732 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:30:06.455941: step 11920/40890 (epoch 9/30), loss = 0.249643 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:30:22.430227: step 11940/40890 (epoch 9/30), loss = 0.363075 (0.624 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:30:37.560769: step 11960/40890 (epoch 9/30), loss = 0.302850 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:30:53.171028: step 11980/40890 (epoch 9/30), loss = 0.448041 (0.875 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:31:09.176236: step 12000/40890 (epoch 9/30), loss = 0.280169 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:31:24.986959: step 12020/40890 (epoch 9/30), loss = 0.194508 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:31:41.180658: step 12040/40890 (epoch 9/30), loss = 0.203977 (0.774 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:31:56.740053: step 12060/40890 (epoch 9/30), loss = 0.478252 (0.801 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:32:11.799430: step 12080/40890 (epoch 9/30), loss = 0.359268 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:32:28.054964: step 12100/40890 (epoch 9/30), loss = 0.234202 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:32:44.483036: step 12120/40890 (epoch 9/30), loss = 0.355338 (0.762 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:33:01.392170: step 12140/40890 (epoch 9/30), loss = 0.325027 (0.695 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:33:17.964855: step 12160/40890 (epoch 9/30), loss = 0.337011 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:33:33.561151: step 12180/40890 (epoch 9/30), loss = 0.167227 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:33:49.872078: step 12200/40890 (epoch 9/30), loss = 0.393084 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:34:06.167504: step 12220/40890 (epoch 9/30), loss = 0.171474 (0.686 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:34:25.094893: step 12240/40890 (epoch 9/30), loss = 0.424334 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:34:41.765676: step 12260/40890 (epoch 9/30), loss = 0.347006 (0.904 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 69.483%\n",
      "   Recall (micro): 59.143%\n",
      "       F1 (micro): 63.897%\n",
      "epoch 9: train_loss = 0.334080, dev_loss = 0.437491, dev_f1 = 0.6390\n",
      "model saved to ./save_models/01/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 01:35:32.798177: step 12280/40890 (epoch 10/30), loss = 0.322208 (0.868 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:35:49.009835: step 12300/40890 (epoch 10/30), loss = 0.402498 (0.929 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:36:05.246419: step 12320/40890 (epoch 10/30), loss = 0.260068 (0.889 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:36:20.976366: step 12340/40890 (epoch 10/30), loss = 0.240585 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:36:37.742533: step 12360/40890 (epoch 10/30), loss = 0.206287 (0.881 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:36:53.525343: step 12380/40890 (epoch 10/30), loss = 0.248550 (0.724 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:37:09.821120: step 12400/40890 (epoch 10/30), loss = 0.486196 (0.878 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:37:25.829316: step 12420/40890 (epoch 10/30), loss = 0.540062 (0.720 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:37:41.817569: step 12440/40890 (epoch 10/30), loss = 0.095945 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:37:57.998303: step 12460/40890 (epoch 10/30), loss = 0.403996 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:38:14.138744: step 12480/40890 (epoch 10/30), loss = 0.246255 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:38:29.709110: step 12500/40890 (epoch 10/30), loss = 0.335381 (0.654 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:38:45.619565: step 12520/40890 (epoch 10/30), loss = 0.302635 (0.823 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:39:02.457619: step 12540/40890 (epoch 10/30), loss = 0.388297 (0.881 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:39:18.383048: step 12560/40890 (epoch 10/30), loss = 0.218958 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:39:33.770902: step 12580/40890 (epoch 10/30), loss = 0.561744 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:39:49.615609: step 12600/40890 (epoch 10/30), loss = 0.341939 (0.814 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:40:05.462236: step 12620/40890 (epoch 10/30), loss = 0.157529 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:40:21.202155: step 12640/40890 (epoch 10/30), loss = 0.337578 (0.644 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:40:36.765539: step 12660/40890 (epoch 10/30), loss = 0.264647 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:40:52.228707: step 12680/40890 (epoch 10/30), loss = 0.158197 (0.792 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:41:08.214961: step 12700/40890 (epoch 10/30), loss = 0.481681 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:41:24.258063: step 12720/40890 (epoch 10/30), loss = 0.195308 (0.706 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:41:40.399900: step 12740/40890 (epoch 10/30), loss = 0.245864 (0.651 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:41:56.382164: step 12760/40890 (epoch 10/30), loss = 0.260148 (0.746 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:42:12.433593: step 12780/40890 (epoch 10/30), loss = 0.486026 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:42:28.613330: step 12800/40890 (epoch 10/30), loss = 0.492076 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:42:45.178036: step 12820/40890 (epoch 10/30), loss = 0.241606 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:43:01.776652: step 12840/40890 (epoch 10/30), loss = 0.296526 (0.912 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:43:17.955391: step 12860/40890 (epoch 10/30), loss = 0.762486 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:43:33.538722: step 12880/40890 (epoch 10/30), loss = 0.402829 (0.711 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:43:49.594789: step 12900/40890 (epoch 10/30), loss = 0.212458 (0.729 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:44:05.973992: step 12920/40890 (epoch 10/30), loss = 0.290600 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:44:23.771403: step 12940/40890 (epoch 10/30), loss = 0.192571 (0.917 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:44:40.378714: step 12960/40890 (epoch 10/30), loss = 0.385934 (0.811 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:44:57.422141: step 12980/40890 (epoch 10/30), loss = 0.214625 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:45:14.368349: step 13000/40890 (epoch 10/30), loss = 0.487059 (0.864 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:45:30.766503: step 13020/40890 (epoch 10/30), loss = 0.600936 (0.825 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:45:46.806611: step 13040/40890 (epoch 10/30), loss = 0.368115 (0.778 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:46:03.581770: step 13060/40890 (epoch 10/30), loss = 0.446303 (0.921 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:46:20.132583: step 13080/40890 (epoch 10/30), loss = 0.249376 (0.764 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:46:36.330271: step 13100/40890 (epoch 10/30), loss = 0.484979 (0.739 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:46:53.102430: step 13120/40890 (epoch 10/30), loss = 0.394858 (0.733 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:47:08.491130: step 13140/40890 (epoch 10/30), loss = 0.278476 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:47:25.252311: step 13160/40890 (epoch 10/30), loss = 0.258520 (0.762 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:47:41.617063: step 13180/40890 (epoch 10/30), loss = 0.196583 (0.805 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:47:57.397866: step 13200/40890 (epoch 10/30), loss = 0.356766 (0.752 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:48:13.885778: step 13220/40890 (epoch 10/30), loss = 0.576605 (0.820 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:48:29.876022: step 13240/40890 (epoch 10/30), loss = 0.449753 (0.716 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:48:45.794456: step 13260/40890 (epoch 10/30), loss = 0.364408 (0.797 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:49:01.774725: step 13280/40890 (epoch 10/30), loss = 0.196877 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:49:17.716099: step 13300/40890 (epoch 10/30), loss = 0.284129 (0.632 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:49:32.944380: step 13320/40890 (epoch 10/30), loss = 0.135616 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:49:48.187620: step 13340/40890 (epoch 10/30), loss = 0.442699 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:50:04.225740: step 13360/40890 (epoch 10/30), loss = 0.370295 (0.882 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:50:20.041449: step 13380/40890 (epoch 10/30), loss = 0.355115 (0.735 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:50:36.227170: step 13400/40890 (epoch 10/30), loss = 0.401527 (0.672 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:50:51.840429: step 13420/40890 (epoch 10/30), loss = 0.220508 (0.769 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:51:06.958873: step 13440/40890 (epoch 10/30), loss = 0.256327 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:51:22.919196: step 13460/40890 (epoch 10/30), loss = 0.430826 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:51:39.074506: step 13480/40890 (epoch 10/30), loss = 0.256940 (0.884 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:51:55.319076: step 13500/40890 (epoch 10/30), loss = 0.306136 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:52:11.650976: step 13520/40890 (epoch 10/30), loss = 0.193546 (0.691 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:52:26.621945: step 13540/40890 (epoch 10/30), loss = 0.459683 (0.890 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:52:42.831685: step 13560/40890 (epoch 10/30), loss = 0.431318 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:52:59.168002: step 13580/40890 (epoch 10/30), loss = 0.166342 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:53:15.639964: step 13600/40890 (epoch 10/30), loss = 0.213529 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:53:31.901482: step 13620/40890 (epoch 10/30), loss = 0.387844 (0.912 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.683%\n",
      "   Recall (micro): 58.904%\n",
      "       F1 (micro): 63.418%\n",
      "epoch 10: train_loss = 0.324872, dev_loss = 0.443024, dev_f1 = 0.6342\n",
      "model saved to ./save_models/01/checkpoint_epoch_10.pt\n",
      "\n",
      "2020-10-12 01:54:24.154640: step 13640/40890 (epoch 11/30), loss = 0.180140 (0.585 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:54:40.873933: step 13660/40890 (epoch 11/30), loss = 0.412238 (0.923 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:54:56.956934: step 13680/40890 (epoch 11/30), loss = 0.282430 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:55:12.795587: step 13700/40890 (epoch 11/30), loss = 0.410595 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:55:29.770199: step 13720/40890 (epoch 11/30), loss = 0.206175 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:55:45.886104: step 13740/40890 (epoch 11/30), loss = 0.218955 (0.707 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:56:01.914246: step 13760/40890 (epoch 11/30), loss = 0.413568 (0.966 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:56:17.974809: step 13780/40890 (epoch 11/30), loss = 0.298685 (0.595 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:56:34.490647: step 13800/40890 (epoch 11/30), loss = 0.508024 (0.897 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:56:51.154598: step 13820/40890 (epoch 11/30), loss = 0.397096 (0.828 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:57:07.447032: step 13840/40890 (epoch 11/30), loss = 0.398708 (0.870 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:57:23.468547: step 13860/40890 (epoch 11/30), loss = 0.260006 (0.872 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:57:39.417408: step 13880/40890 (epoch 11/30), loss = 0.293026 (0.596 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:57:55.518359: step 13900/40890 (epoch 11/30), loss = 0.503971 (0.638 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:58:11.485663: step 13920/40890 (epoch 11/30), loss = 0.392823 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:58:26.884487: step 13940/40890 (epoch 11/30), loss = 0.333788 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:58:42.840675: step 13960/40890 (epoch 11/30), loss = 0.305950 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:58:58.536705: step 13980/40890 (epoch 11/30), loss = 0.281232 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:59:14.821218: step 14000/40890 (epoch 11/30), loss = 0.230138 (0.698 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:59:30.343712: step 14020/40890 (epoch 11/30), loss = 0.269623 (0.683 sec/batch), lr: 1.000000\n",
      "2020-10-12 01:59:45.884087: step 14040/40890 (epoch 11/30), loss = 0.232476 (0.737 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:00:01.645803: step 14060/40890 (epoch 11/30), loss = 0.185851 (0.822 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:00:18.040032: step 14080/40890 (epoch 11/30), loss = 0.277365 (0.808 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:00:34.338450: step 14100/40890 (epoch 11/30), loss = 0.191080 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:00:49.996581: step 14120/40890 (epoch 11/30), loss = 0.447754 (0.770 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:01:05.950481: step 14140/40890 (epoch 11/30), loss = 0.150006 (0.842 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:01:22.096308: step 14160/40890 (epoch 11/30), loss = 0.234575 (0.850 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:01:38.347852: step 14180/40890 (epoch 11/30), loss = 0.434330 (0.860 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:01:54.348073: step 14200/40890 (epoch 11/30), loss = 0.393323 (0.710 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:02:10.255877: step 14220/40890 (epoch 11/30), loss = 0.272220 (0.799 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:02:25.421326: step 14240/40890 (epoch 11/30), loss = 0.320031 (0.914 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:02:41.629991: step 14260/40890 (epoch 11/30), loss = 0.260049 (0.844 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:02:57.519510: step 14280/40890 (epoch 11/30), loss = 0.192646 (0.749 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:03:13.937674: step 14300/40890 (epoch 11/30), loss = 0.198976 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:03:30.425586: step 14320/40890 (epoch 11/30), loss = 0.274377 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:03:47.107978: step 14340/40890 (epoch 11/30), loss = 0.453983 (0.745 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:04:04.324846: step 14360/40890 (epoch 11/30), loss = 0.269339 (0.691 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:04:23.099643: step 14380/40890 (epoch 11/30), loss = 0.310544 (0.731 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:04:39.006110: step 14400/40890 (epoch 11/30), loss = 0.060075 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:04:55.842173: step 14420/40890 (epoch 11/30), loss = 0.239284 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:05:12.925370: step 14440/40890 (epoch 11/30), loss = 0.245646 (0.923 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:05:29.153976: step 14460/40890 (epoch 11/30), loss = 0.270528 (0.915 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:05:46.055786: step 14480/40890 (epoch 11/30), loss = 0.275572 (0.837 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:06:01.461592: step 14500/40890 (epoch 11/30), loss = 0.238284 (0.723 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:06:18.125034: step 14520/40890 (epoch 11/30), loss = 0.297296 (0.972 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:06:34.800445: step 14540/40890 (epoch 11/30), loss = 0.288359 (0.803 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:06:50.983720: step 14560/40890 (epoch 11/30), loss = 0.330976 (0.869 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:07:07.715979: step 14580/40890 (epoch 11/30), loss = 0.137617 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:07:24.280201: step 14600/40890 (epoch 11/30), loss = 0.569715 (0.750 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:07:40.243524: step 14620/40890 (epoch 11/30), loss = 0.234313 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:07:56.364417: step 14640/40890 (epoch 11/30), loss = 0.304916 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:08:12.404585: step 14660/40890 (epoch 11/30), loss = 0.165976 (0.567 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:08:27.398492: step 14680/40890 (epoch 11/30), loss = 0.289007 (0.700 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:08:42.678145: step 14700/40890 (epoch 11/30), loss = 0.325534 (0.700 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:08:58.776100: step 14720/40890 (epoch 11/30), loss = 0.187169 (0.768 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:09:14.653645: step 14740/40890 (epoch 11/30), loss = 0.312229 (0.856 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:09:30.885242: step 14760/40890 (epoch 11/30), loss = 0.379226 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:09:46.234551: step 14780/40890 (epoch 11/30), loss = 0.229223 (0.745 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:10:01.317842: step 14800/40890 (epoch 11/30), loss = 0.272369 (0.814 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:10:17.102634: step 14820/40890 (epoch 11/30), loss = 0.254527 (0.832 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:10:33.582567: step 14840/40890 (epoch 11/30), loss = 0.205718 (0.784 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:10:49.958843: step 14860/40890 (epoch 11/30), loss = 0.465034 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:11:06.219369: step 14880/40890 (epoch 11/30), loss = 0.284988 (0.857 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:11:21.366425: step 14900/40890 (epoch 11/30), loss = 0.187337 (0.649 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:11:37.046006: step 14920/40890 (epoch 11/30), loss = 0.181904 (0.666 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:11:52.605400: step 14940/40890 (epoch 11/30), loss = 0.317765 (0.744 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:12:08.848972: step 14960/40890 (epoch 11/30), loss = 0.260424 (0.883 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:12:24.534031: step 14980/40890 (epoch 11/30), loss = 0.228657 (0.678 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.052%\n",
      "   Recall (micro): 62.969%\n",
      "       F1 (micro): 65.412%\n",
      "epoch 11: train_loss = 0.317221, dev_loss = 0.435420, dev_f1 = 0.6541\n",
      "model saved to ./save_models/01/checkpoint_epoch_11.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 02:13:15.377559: step 15000/40890 (epoch 12/30), loss = 0.487865 (0.794 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:13:31.208228: step 15020/40890 (epoch 12/30), loss = 0.228701 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:13:47.591421: step 15040/40890 (epoch 12/30), loss = 0.330949 (0.887 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:14:03.840000: step 15060/40890 (epoch 12/30), loss = 0.231793 (0.873 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:14:21.743797: step 15080/40890 (epoch 12/30), loss = 0.254357 (0.779 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:14:38.160899: step 15100/40890 (epoch 12/30), loss = 0.197051 (0.867 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:14:54.160639: step 15120/40890 (epoch 12/30), loss = 0.256671 (0.789 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:15:10.568272: step 15140/40890 (epoch 12/30), loss = 0.135249 (0.840 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:15:27.022275: step 15160/40890 (epoch 12/30), loss = 0.364608 (0.815 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:15:43.651809: step 15180/40890 (epoch 12/30), loss = 0.229987 (0.833 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:15:59.982142: step 15200/40890 (epoch 12/30), loss = 0.159178 (0.905 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:16:15.948449: step 15220/40890 (epoch 12/30), loss = 0.363604 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:16:32.391481: step 15240/40890 (epoch 12/30), loss = 0.183854 (0.742 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:16:48.651871: step 15260/40890 (epoch 12/30), loss = 0.294543 (0.855 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:17:04.761302: step 15280/40890 (epoch 12/30), loss = 0.373118 (0.955 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:17:20.895165: step 15300/40890 (epoch 12/30), loss = 0.290407 (0.756 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:17:36.770715: step 15320/40890 (epoch 12/30), loss = 0.310250 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:17:52.270270: step 15340/40890 (epoch 12/30), loss = 0.238393 (0.861 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:18:08.552739: step 15360/40890 (epoch 12/30), loss = 0.328693 (0.588 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:18:24.331548: step 15380/40890 (epoch 12/30), loss = 0.224260 (0.881 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:18:39.802963: step 15400/40890 (epoch 12/30), loss = 0.274994 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:18:55.488022: step 15420/40890 (epoch 12/30), loss = 0.243180 (0.858 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:19:11.996878: step 15440/40890 (epoch 12/30), loss = 0.211108 (0.867 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:19:28.216507: step 15460/40890 (epoch 12/30), loss = 0.416687 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:19:43.948448: step 15480/40890 (epoch 12/30), loss = 0.271428 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:19:59.883838: step 15500/40890 (epoch 12/30), loss = 0.186661 (0.810 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:20:16.024678: step 15520/40890 (epoch 12/30), loss = 0.274975 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:20:32.536526: step 15540/40890 (epoch 12/30), loss = 0.247246 (0.816 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:20:48.382163: step 15560/40890 (epoch 12/30), loss = 0.550330 (0.862 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:21:04.113433: step 15580/40890 (epoch 12/30), loss = 0.120822 (0.876 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:21:19.493308: step 15600/40890 (epoch 12/30), loss = 0.562404 (0.741 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:21:35.167397: step 15620/40890 (epoch 12/30), loss = 0.293644 (0.751 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:21:50.766685: step 15640/40890 (epoch 12/30), loss = 0.226990 (0.733 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:22:06.482661: step 15660/40890 (epoch 12/30), loss = 0.344084 (0.813 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:22:22.580616: step 15680/40890 (epoch 12/30), loss = 0.261537 (0.796 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:22:39.215136: step 15700/40890 (epoch 12/30), loss = 0.216444 (0.853 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:22:56.208696: step 15720/40890 (epoch 12/30), loss = 0.452281 (0.928 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:23:12.848080: step 15740/40890 (epoch 12/30), loss = 0.409723 (0.673 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:23:28.655810: step 15760/40890 (epoch 12/30), loss = 0.215482 (0.670 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:23:45.298323: step 15780/40890 (epoch 12/30), loss = 0.325783 (0.932 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:24:02.613024: step 15800/40890 (epoch 12/30), loss = 0.195367 (0.944 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:24:19.636504: step 15820/40890 (epoch 12/30), loss = 0.250991 (0.892 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:24:36.986112: step 15840/40890 (epoch 12/30), loss = 0.295065 (0.865 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:24:52.736631: step 15860/40890 (epoch 12/30), loss = 0.383529 (0.901 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:25:08.921393: step 15880/40890 (epoch 12/30), loss = 0.305577 (0.859 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:25:25.800767: step 15900/40890 (epoch 12/30), loss = 0.318690 (0.943 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:25:42.050316: step 15920/40890 (epoch 12/30), loss = 0.388050 (0.588 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:25:58.527258: step 15940/40890 (epoch 12/30), loss = 0.543980 (0.830 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:26:15.308386: step 15960/40890 (epoch 12/30), loss = 0.352571 (0.755 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:26:31.587856: step 15980/40890 (epoch 12/30), loss = 0.467862 (0.793 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:26:47.885277: step 16000/40890 (epoch 12/30), loss = 0.448041 (0.829 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:27:04.467936: step 16020/40890 (epoch 12/30), loss = 0.357172 (0.798 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:27:19.815896: step 16040/40890 (epoch 12/30), loss = 0.367545 (0.818 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:27:35.350358: step 16060/40890 (epoch 12/30), loss = 0.290093 (0.782 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:27:51.268302: step 16080/40890 (epoch 12/30), loss = 0.324523 (0.847 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:28:07.132287: step 16100/40890 (epoch 12/30), loss = 0.434181 (0.917 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:28:23.226306: step 16120/40890 (epoch 12/30), loss = 0.254816 (0.646 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:28:38.896914: step 16140/40890 (epoch 12/30), loss = 0.605089 (0.834 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:28:54.119783: step 16160/40890 (epoch 12/30), loss = 0.279964 (0.753 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:29:09.652249: step 16180/40890 (epoch 12/30), loss = 0.273367 (0.542 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:29:25.952663: step 16200/40890 (epoch 12/30), loss = 0.311416 (0.841 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:29:42.156841: step 16220/40890 (epoch 12/30), loss = 0.193357 (0.768 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:29:58.253307: step 16240/40890 (epoch 12/30), loss = 0.420386 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:30:13.729924: step 16260/40890 (epoch 12/30), loss = 0.218170 (0.597 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:30:29.193574: step 16280/40890 (epoch 12/30), loss = 0.261538 (0.635 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:30:44.627648: step 16300/40890 (epoch 12/30), loss = 0.138159 (0.627 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:31:00.686707: step 16320/40890 (epoch 12/30), loss = 0.440249 (0.824 sec/batch), lr: 1.000000\n",
      "2020-10-12 02:31:16.617110: step 16340/40890 (epoch 12/30), loss = 0.294192 (0.763 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.514%\n",
      "   Recall (micro): 61.405%\n",
      "       F1 (micro): 64.765%\n",
      "epoch 12: train_loss = 0.309815, dev_loss = 0.432671, dev_f1 = 0.6477\n",
      "model saved to ./save_models/01/checkpoint_epoch_12.pt\n",
      "\n",
      "2020-10-12 02:32:05.044411: step 16360/40890 (epoch 13/30), loss = 0.250293 (0.665 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:32:20.747421: step 16380/40890 (epoch 13/30), loss = 0.513847 (0.765 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:32:36.554155: step 16400/40890 (epoch 13/30), loss = 0.368130 (0.729 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:32:52.492536: step 16420/40890 (epoch 13/30), loss = 0.293653 (0.654 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:33:09.209345: step 16440/40890 (epoch 13/30), loss = 0.366300 (0.829 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:33:25.157700: step 16460/40890 (epoch 13/30), loss = 0.280646 (0.744 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:33:41.147453: step 16480/40890 (epoch 13/30), loss = 0.287910 (0.715 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:33:57.412961: step 16500/40890 (epoch 13/30), loss = 0.822126 (0.723 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:34:15.607310: step 16520/40890 (epoch 13/30), loss = 0.195074 (1.026 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:34:32.922011: step 16540/40890 (epoch 13/30), loss = 0.396755 (0.878 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:34:49.285272: step 16560/40890 (epoch 13/30), loss = 0.158241 (0.589 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:35:05.444087: step 16580/40890 (epoch 13/30), loss = 0.536524 (0.627 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:35:22.073627: step 16600/40890 (epoch 13/30), loss = 0.292818 (0.872 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:35:38.135678: step 16620/40890 (epoch 13/30), loss = 0.125658 (0.807 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:35:54.242618: step 16640/40890 (epoch 13/30), loss = 0.215256 (0.687 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:36:10.655239: step 16660/40890 (epoch 13/30), loss = 0.555498 (0.751 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:36:26.522810: step 16680/40890 (epoch 13/30), loss = 0.284958 (0.657 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:36:42.423299: step 16700/40890 (epoch 13/30), loss = 0.219442 (0.801 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:36:59.085334: step 16720/40890 (epoch 13/30), loss = 0.219896 (0.936 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:37:15.076588: step 16740/40890 (epoch 13/30), loss = 0.354645 (0.859 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:37:30.566170: step 16760/40890 (epoch 13/30), loss = 0.196098 (0.551 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:37:46.104136: step 16780/40890 (epoch 13/30), loss = 0.088922 (0.679 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:38:02.529217: step 16800/40890 (epoch 13/30), loss = 0.516642 (0.664 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:38:18.815750: step 16820/40890 (epoch 13/30), loss = 0.346087 (0.791 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:38:34.671352: step 16840/40890 (epoch 13/30), loss = 0.351296 (0.720 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:38:50.503019: step 16860/40890 (epoch 13/30), loss = 0.243732 (0.768 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:39:06.538142: step 16880/40890 (epoch 13/30), loss = 0.230083 (0.778 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:39:23.763084: step 16900/40890 (epoch 13/30), loss = 0.443452 (0.634 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:39:39.424207: step 16920/40890 (epoch 13/30), loss = 0.135369 (0.626 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:39:55.447868: step 16940/40890 (epoch 13/30), loss = 0.293372 (0.777 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:40:10.926003: step 16960/40890 (epoch 13/30), loss = 0.366911 (0.793 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:40:26.430544: step 16980/40890 (epoch 13/30), loss = 0.196640 (0.762 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:40:41.907216: step 17000/40890 (epoch 13/30), loss = 0.485594 (0.712 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:40:57.482568: step 17020/40890 (epoch 13/30), loss = 0.457654 (0.634 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:41:13.605509: step 17040/40890 (epoch 13/30), loss = 0.234146 (0.772 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:41:29.995683: step 17060/40890 (epoch 13/30), loss = 0.247547 (0.770 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:41:46.367904: step 17080/40890 (epoch 13/30), loss = 0.227714 (0.680 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:42:02.585559: step 17100/40890 (epoch 13/30), loss = 0.121989 (0.772 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:42:18.111050: step 17120/40890 (epoch 13/30), loss = 0.113716 (0.899 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:42:34.202024: step 17140/40890 (epoch 13/30), loss = 0.375325 (0.876 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:42:51.153792: step 17160/40890 (epoch 13/30), loss = 0.273125 (0.816 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:43:07.390437: step 17180/40890 (epoch 13/30), loss = 0.320503 (0.829 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:43:24.040915: step 17200/40890 (epoch 13/30), loss = 0.283395 (0.860 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:43:40.075046: step 17220/40890 (epoch 13/30), loss = 0.204945 (0.707 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:43:56.111166: step 17240/40890 (epoch 13/30), loss = 0.404297 (0.866 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:44:13.602395: step 17260/40890 (epoch 13/30), loss = 0.206266 (0.869 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:44:31.166429: step 17280/40890 (epoch 13/30), loss = 0.268591 (0.874 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:44:47.397504: step 17300/40890 (epoch 13/30), loss = 0.418064 (0.737 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:45:04.261410: step 17320/40890 (epoch 13/30), loss = 0.280438 (0.797 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:45:20.951846: step 17340/40890 (epoch 13/30), loss = 0.233617 (0.762 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:45:36.979499: step 17360/40890 (epoch 13/30), loss = 0.290995 (0.764 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:45:53.384633: step 17380/40890 (epoch 13/30), loss = 0.229204 (0.852 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:46:08.829841: step 17400/40890 (epoch 13/30), loss = 0.357811 (0.857 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:46:24.648543: step 17420/40890 (epoch 13/30), loss = 0.212903 (0.785 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:46:41.067507: step 17440/40890 (epoch 13/30), loss = 0.423302 (0.855 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:46:57.005889: step 17460/40890 (epoch 13/30), loss = 0.210870 (0.909 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:47:13.815999: step 17480/40890 (epoch 13/30), loss = 0.390416 (0.935 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:47:29.687560: step 17500/40890 (epoch 13/30), loss = 0.388895 (0.665 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:47:44.959723: step 17520/40890 (epoch 13/30), loss = 0.392713 (0.582 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:48:00.479224: step 17540/40890 (epoch 13/30), loss = 0.298292 (0.797 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:48:16.563080: step 17560/40890 (epoch 13/30), loss = 0.460874 (0.771 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:48:32.648071: step 17580/40890 (epoch 13/30), loss = 0.370909 (0.802 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:48:48.839775: step 17600/40890 (epoch 13/30), loss = 0.574812 (0.899 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:49:04.575741: step 17620/40890 (epoch 13/30), loss = 0.248499 (0.682 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:49:19.915777: step 17640/40890 (epoch 13/30), loss = 0.215695 (0.870 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:49:35.259748: step 17660/40890 (epoch 13/30), loss = 0.167088 (0.754 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:49:51.122202: step 17680/40890 (epoch 13/30), loss = 0.521533 (0.870 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:50:07.226250: step 17700/40890 (epoch 13/30), loss = 0.351135 (0.801 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 69.009%\n",
      "   Recall (micro): 62.141%\n",
      "       F1 (micro): 65.395%\n",
      "epoch 13: train_loss = 0.296551, dev_loss = 0.438863, dev_f1 = 0.6540\n",
      "model saved to ./save_models/01/checkpoint_epoch_13.pt\n",
      "\n",
      "2020-10-12 02:50:55.580968: step 17720/40890 (epoch 14/30), loss = 0.448748 (0.597 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:51:11.297136: step 17740/40890 (epoch 14/30), loss = 0.261802 (0.640 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:51:26.745827: step 17760/40890 (epoch 14/30), loss = 0.246461 (0.837 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:51:42.504201: step 17780/40890 (epoch 14/30), loss = 0.245828 (0.841 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:51:58.434605: step 17800/40890 (epoch 14/30), loss = 0.102095 (0.818 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:52:14.232961: step 17820/40890 (epoch 14/30), loss = 0.406413 (0.642 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:52:29.803327: step 17840/40890 (epoch 14/30), loss = 0.321871 (0.671 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:52:46.025948: step 17860/40890 (epoch 14/30), loss = 0.251758 (0.869 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:53:02.360278: step 17880/40890 (epoch 14/30), loss = 0.155944 (0.890 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:53:18.970377: step 17900/40890 (epoch 14/30), loss = 0.311180 (0.871 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:53:35.094262: step 17920/40890 (epoch 14/30), loss = 0.271846 (0.706 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:53:51.201311: step 17940/40890 (epoch 14/30), loss = 0.344649 (0.674 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:54:07.999439: step 17960/40890 (epoch 14/30), loss = 0.137815 (0.955 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:54:25.743983: step 17980/40890 (epoch 14/30), loss = 0.658958 (0.929 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:54:41.834948: step 18000/40890 (epoch 14/30), loss = 0.290869 (0.817 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:54:58.104502: step 18020/40890 (epoch 14/30), loss = 0.239202 (0.768 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:55:14.409901: step 18040/40890 (epoch 14/30), loss = 0.160961 (0.856 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:55:30.416102: step 18060/40890 (epoch 14/30), loss = 0.285119 (0.676 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:55:46.998767: step 18080/40890 (epoch 14/30), loss = 0.529188 (0.800 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:56:03.050844: step 18100/40890 (epoch 14/30), loss = 0.208327 (0.817 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:56:18.967290: step 18120/40890 (epoch 14/30), loss = 0.262033 (0.683 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:56:34.897692: step 18140/40890 (epoch 14/30), loss = 0.268349 (0.707 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:56:51.398079: step 18160/40890 (epoch 14/30), loss = 0.216064 (0.659 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:57:08.248030: step 18180/40890 (epoch 14/30), loss = 0.362878 (0.953 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:57:24.658157: step 18200/40890 (epoch 14/30), loss = 0.202031 (0.679 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:57:40.505799: step 18220/40890 (epoch 14/30), loss = 0.186475 (0.839 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:57:56.587797: step 18240/40890 (epoch 14/30), loss = 0.228904 (0.576 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:58:13.216332: step 18260/40890 (epoch 14/30), loss = 0.220695 (0.853 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:58:28.934303: step 18280/40890 (epoch 14/30), loss = 0.367298 (0.713 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:58:44.983465: step 18300/40890 (epoch 14/30), loss = 0.445882 (0.789 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:59:00.302509: step 18320/40890 (epoch 14/30), loss = 0.221919 (0.733 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:59:15.984576: step 18340/40890 (epoch 14/30), loss = 0.375950 (0.821 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:59:31.720498: step 18360/40890 (epoch 14/30), loss = 0.250036 (0.809 sec/batch), lr: 0.900000\n",
      "2020-10-12 02:59:47.241995: step 18380/40890 (epoch 14/30), loss = 0.427449 (0.871 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:00:03.289091: step 18400/40890 (epoch 14/30), loss = 0.256370 (0.862 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:00:19.821898: step 18420/40890 (epoch 14/30), loss = 0.258887 (0.822 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:00:35.999640: step 18440/40890 (epoch 14/30), loss = 0.100220 (0.787 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:00:52.294069: step 18460/40890 (epoch 14/30), loss = 0.221841 (0.853 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:01:08.067276: step 18480/40890 (epoch 14/30), loss = 0.248873 (0.836 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:01:23.975794: step 18500/40890 (epoch 14/30), loss = 0.465036 (0.841 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:01:40.741968: step 18520/40890 (epoch 14/30), loss = 0.373015 (0.857 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:01:56.495843: step 18540/40890 (epoch 14/30), loss = 0.305456 (0.736 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:02:12.739923: step 18560/40890 (epoch 14/30), loss = 0.396776 (0.852 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:02:28.693264: step 18580/40890 (epoch 14/30), loss = 0.416235 (0.804 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:02:44.644615: step 18600/40890 (epoch 14/30), loss = 0.297113 (0.678 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:03:01.173781: step 18620/40890 (epoch 14/30), loss = 0.301951 (0.816 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:03:18.164353: step 18640/40890 (epoch 14/30), loss = 0.375793 (0.913 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:03:34.241363: step 18660/40890 (epoch 14/30), loss = 0.190782 (0.625 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:03:51.434773: step 18680/40890 (epoch 14/30), loss = 0.202777 (0.880 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:04:08.636659: step 18700/40890 (epoch 14/30), loss = 0.258926 (1.013 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:04:26.675424: step 18720/40890 (epoch 14/30), loss = 0.306665 (0.807 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:04:43.155451: step 18740/40890 (epoch 14/30), loss = 0.266355 (0.856 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:04:58.935763: step 18760/40890 (epoch 14/30), loss = 0.207845 (0.768 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:05:14.516115: step 18780/40890 (epoch 14/30), loss = 0.369298 (0.797 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:05:30.926235: step 18800/40890 (epoch 14/30), loss = 0.193743 (0.913 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:05:46.918472: step 18820/40890 (epoch 14/30), loss = 0.335132 (0.801 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:06:03.447782: step 18840/40890 (epoch 14/30), loss = 0.426606 (0.762 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:06:19.926234: step 18860/40890 (epoch 14/30), loss = 0.181382 (0.902 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:06:35.556439: step 18880/40890 (epoch 14/30), loss = 0.287388 (0.807 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:06:51.319331: step 18900/40890 (epoch 14/30), loss = 0.155605 (0.876 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:07:07.506048: step 18920/40890 (epoch 14/30), loss = 0.132282 (0.795 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:07:24.187444: step 18940/40890 (epoch 14/30), loss = 0.443529 (0.874 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:07:40.535732: step 18960/40890 (epoch 14/30), loss = 0.298647 (0.798 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:07:56.422254: step 18980/40890 (epoch 14/30), loss = 0.696062 (0.614 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:08:11.691423: step 19000/40890 (epoch 14/30), loss = 0.297464 (0.725 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:08:27.226882: step 19020/40890 (epoch 14/30), loss = 0.357248 (0.705 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:08:42.823178: step 19040/40890 (epoch 14/30), loss = 0.276672 (0.878 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:08:58.907178: step 19060/40890 (epoch 14/30), loss = 0.233839 (0.866 sec/batch), lr: 0.900000\n",
      "2020-10-12 03:09:15.211639: step 19080/40890 (epoch 14/30), loss = 0.349298 (0.762 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.368%\n",
      "   Recall (micro): 62.399%\n",
      "       F1 (micro): 64.788%\n",
      "epoch 14: train_loss = 0.292657, dev_loss = 0.446219, dev_f1 = 0.6479\n",
      "model saved to ./save_models/01/checkpoint_epoch_14.pt\n",
      "\n",
      "2020-10-12 03:10:03.251140: step 19100/40890 (epoch 15/30), loss = 0.283346 (0.862 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:10:18.653959: step 19120/40890 (epoch 15/30), loss = 0.367574 (0.759 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:10:34.488618: step 19140/40890 (epoch 15/30), loss = 0.252319 (0.770 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:10:50.343102: step 19160/40890 (epoch 15/30), loss = 0.215440 (0.803 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:11:06.163798: step 19180/40890 (epoch 15/30), loss = 0.254961 (0.737 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:11:21.731523: step 19200/40890 (epoch 15/30), loss = 0.383770 (0.819 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:11:37.473431: step 19220/40890 (epoch 15/30), loss = 0.391466 (0.745 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:11:53.265709: step 19240/40890 (epoch 15/30), loss = 0.345387 (0.694 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:12:09.613528: step 19260/40890 (epoch 15/30), loss = 0.268013 (0.807 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:12:25.770325: step 19280/40890 (epoch 15/30), loss = 0.319510 (0.857 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:12:42.006910: step 19300/40890 (epoch 15/30), loss = 0.247335 (0.784 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:12:58.068967: step 19320/40890 (epoch 15/30), loss = 0.244661 (0.789 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:13:14.280678: step 19340/40890 (epoch 15/30), loss = 0.386546 (0.876 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:13:30.481354: step 19360/40890 (epoch 15/30), loss = 0.212324 (0.823 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:13:46.761829: step 19380/40890 (epoch 15/30), loss = 0.230663 (0.877 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:14:03.030328: step 19400/40890 (epoch 15/30), loss = 0.226798 (1.021 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:14:20.903047: step 19420/40890 (epoch 15/30), loss = 0.544990 (0.982 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:14:37.035908: step 19440/40890 (epoch 15/30), loss = 0.267112 (0.773 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:14:53.139847: step 19460/40890 (epoch 15/30), loss = 0.319259 (0.827 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:15:09.225354: step 19480/40890 (epoch 15/30), loss = 0.366974 (0.744 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:15:25.443483: step 19500/40890 (epoch 15/30), loss = 0.151161 (0.670 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:15:42.032484: step 19520/40890 (epoch 15/30), loss = 0.284815 (0.833 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:15:58.435623: step 19540/40890 (epoch 15/30), loss = 0.202682 (0.859 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:16:14.903108: step 19560/40890 (epoch 15/30), loss = 0.144179 (0.879 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:16:31.066887: step 19580/40890 (epoch 15/30), loss = 0.446103 (0.863 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:16:47.463044: step 19600/40890 (epoch 15/30), loss = 0.446211 (0.732 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:17:04.291047: step 19620/40890 (epoch 15/30), loss = 0.217063 (0.914 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:17:20.513668: step 19640/40890 (epoch 15/30), loss = 0.199340 (0.851 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:17:36.685426: step 19660/40890 (epoch 15/30), loss = 0.152251 (0.834 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:17:52.122476: step 19680/40890 (epoch 15/30), loss = 0.256180 (0.842 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:18:07.465450: step 19700/40890 (epoch 15/30), loss = 0.255102 (0.712 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:18:23.370925: step 19720/40890 (epoch 15/30), loss = 0.288383 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:18:38.808651: step 19740/40890 (epoch 15/30), loss = 0.090406 (0.842 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:18:54.660265: step 19760/40890 (epoch 15/30), loss = 0.148909 (0.831 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:19:11.038983: step 19780/40890 (epoch 15/30), loss = 0.317176 (0.703 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:19:27.131809: step 19800/40890 (epoch 15/30), loss = 0.176515 (0.748 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:19:43.390355: step 19820/40890 (epoch 15/30), loss = 0.360694 (0.788 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:19:58.994630: step 19840/40890 (epoch 15/30), loss = 0.341385 (0.786 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:20:14.883650: step 19860/40890 (epoch 15/30), loss = 0.288119 (0.846 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:20:31.303744: step 19880/40890 (epoch 15/30), loss = 0.252604 (0.833 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:20:47.111475: step 19900/40890 (epoch 15/30), loss = 0.347439 (0.609 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:21:03.217753: step 19920/40890 (epoch 15/30), loss = 0.268072 (0.905 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:21:19.427409: step 19940/40890 (epoch 15/30), loss = 0.391140 (0.645 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:21:34.866127: step 19960/40890 (epoch 15/30), loss = 0.394729 (0.843 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:21:50.923093: step 19980/40890 (epoch 15/30), loss = 0.392566 (0.806 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:22:07.127763: step 20000/40890 (epoch 15/30), loss = 0.237608 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:22:22.876651: step 20020/40890 (epoch 15/30), loss = 0.213956 (0.842 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:22:39.846275: step 20040/40890 (epoch 15/30), loss = 0.174169 (0.937 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:22:56.332193: step 20060/40890 (epoch 15/30), loss = 0.335863 (0.919 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:23:12.535868: step 20080/40890 (epoch 15/30), loss = 0.289779 (0.725 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:23:28.871188: step 20100/40890 (epoch 15/30), loss = 0.227292 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:23:44.958748: step 20120/40890 (epoch 15/30), loss = 0.145042 (0.723 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:24:00.512167: step 20140/40890 (epoch 15/30), loss = 0.292987 (0.848 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:24:17.804434: step 20160/40890 (epoch 15/30), loss = 0.482038 (0.828 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:24:34.161696: step 20180/40890 (epoch 15/30), loss = 0.229457 (0.627 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:24:50.930856: step 20200/40890 (epoch 15/30), loss = 0.229627 (0.922 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:25:07.316105: step 20220/40890 (epoch 15/30), loss = 0.329932 (0.875 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:25:23.364194: step 20240/40890 (epoch 15/30), loss = 0.339063 (0.756 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:25:39.050257: step 20260/40890 (epoch 15/30), loss = 0.256761 (0.885 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:25:55.425471: step 20280/40890 (epoch 15/30), loss = 0.298637 (0.752 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:26:12.048023: step 20300/40890 (epoch 15/30), loss = 0.187990 (0.949 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:26:28.687530: step 20320/40890 (epoch 15/30), loss = 0.208351 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:26:45.151506: step 20340/40890 (epoch 15/30), loss = 0.256550 (0.844 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:27:00.835568: step 20360/40890 (epoch 15/30), loss = 0.199899 (0.826 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:27:17.004848: step 20380/40890 (epoch 15/30), loss = 0.400621 (0.750 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:27:32.616104: step 20400/40890 (epoch 15/30), loss = 0.301918 (0.765 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:27:48.749131: step 20420/40890 (epoch 15/30), loss = 0.142168 (0.869 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:28:05.010648: step 20440/40890 (epoch 15/30), loss = 0.322483 (0.912 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.351%\n",
      "   Recall (micro): 62.730%\n",
      "       F1 (micro): 64.959%\n",
      "epoch 15: train_loss = 0.282483, dev_loss = 0.446030, dev_f1 = 0.6496\n",
      "model saved to ./save_models/01/checkpoint_epoch_15.pt\n",
      "\n",
      "2020-10-12 03:28:53.584019: step 20460/40890 (epoch 16/30), loss = 0.202399 (0.768 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:29:09.095551: step 20480/40890 (epoch 16/30), loss = 0.280747 (0.692 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:29:25.067842: step 20500/40890 (epoch 16/30), loss = 0.220861 (0.867 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:29:40.679116: step 20520/40890 (epoch 16/30), loss = 0.278808 (0.916 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:29:56.974118: step 20540/40890 (epoch 16/30), loss = 0.193322 (0.765 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:30:12.336041: step 20560/40890 (epoch 16/30), loss = 0.179725 (0.672 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:30:27.941314: step 20580/40890 (epoch 16/30), loss = 0.282231 (0.751 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:30:43.886677: step 20600/40890 (epoch 16/30), loss = 0.271593 (0.876 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:30:59.912828: step 20620/40890 (epoch 16/30), loss = 0.278036 (0.854 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:31:16.068138: step 20640/40890 (epoch 16/30), loss = 0.301006 (0.901 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:31:32.044267: step 20660/40890 (epoch 16/30), loss = 0.304694 (0.681 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:31:47.614143: step 20680/40890 (epoch 16/30), loss = 0.412636 (0.771 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:32:03.557512: step 20700/40890 (epoch 16/30), loss = 0.241721 (0.792 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:32:19.498396: step 20720/40890 (epoch 16/30), loss = 0.226230 (0.682 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:32:35.604329: step 20740/40890 (epoch 16/30), loss = 0.404449 (0.660 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:32:51.257813: step 20760/40890 (epoch 16/30), loss = 0.251833 (0.674 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:33:07.978694: step 20780/40890 (epoch 16/30), loss = 0.264351 (0.892 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:33:24.098590: step 20800/40890 (epoch 16/30), loss = 0.167196 (0.945 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:33:40.090828: step 20820/40890 (epoch 16/30), loss = 0.118153 (0.707 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:33:56.131952: step 20840/40890 (epoch 16/30), loss = 0.149087 (0.739 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:34:13.784749: step 20860/40890 (epoch 16/30), loss = 0.202910 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:34:31.202176: step 20880/40890 (epoch 16/30), loss = 0.248992 (0.769 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:34:48.036163: step 20900/40890 (epoch 16/30), loss = 0.188194 (0.874 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:35:04.346553: step 20920/40890 (epoch 16/30), loss = 0.360215 (0.763 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:35:20.929212: step 20940/40890 (epoch 16/30), loss = 0.283108 (0.848 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:35:37.303936: step 20960/40890 (epoch 16/30), loss = 0.162631 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:35:53.861661: step 20980/40890 (epoch 16/30), loss = 0.310785 (0.910 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:36:10.193989: step 21000/40890 (epoch 16/30), loss = 0.223226 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:36:26.752712: step 21020/40890 (epoch 16/30), loss = 0.229830 (0.774 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:36:42.765894: step 21040/40890 (epoch 16/30), loss = 0.469752 (0.646 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:36:58.603053: step 21060/40890 (epoch 16/30), loss = 0.345946 (0.785 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:37:14.721952: step 21080/40890 (epoch 16/30), loss = 0.121406 (0.864 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:37:30.498766: step 21100/40890 (epoch 16/30), loss = 0.174253 (0.817 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:37:46.246657: step 21120/40890 (epoch 16/30), loss = 0.233823 (0.818 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:38:02.665261: step 21140/40890 (epoch 16/30), loss = 0.241173 (0.890 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:38:18.688923: step 21160/40890 (epoch 16/30), loss = 0.279117 (0.847 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:38:35.016264: step 21180/40890 (epoch 16/30), loss = 0.137465 (0.898 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:38:50.765153: step 21200/40890 (epoch 16/30), loss = 0.157860 (0.711 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:39:06.518030: step 21220/40890 (epoch 16/30), loss = 0.682700 (0.799 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:39:22.924668: step 21240/40890 (epoch 16/30), loss = 0.283111 (0.895 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:39:39.918792: step 21260/40890 (epoch 16/30), loss = 0.225802 (0.807 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:39:55.678223: step 21280/40890 (epoch 16/30), loss = 0.320630 (0.808 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:40:12.123607: step 21300/40890 (epoch 16/30), loss = 0.318224 (0.831 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:40:27.288057: step 21320/40890 (epoch 16/30), loss = 0.455660 (0.910 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:40:43.477767: step 21340/40890 (epoch 16/30), loss = 0.262814 (0.850 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:40:59.824058: step 21360/40890 (epoch 16/30), loss = 0.261392 (0.915 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:41:15.256801: step 21380/40890 (epoch 16/30), loss = 0.259397 (0.684 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:41:31.859406: step 21400/40890 (epoch 16/30), loss = 0.240667 (0.784 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:41:47.909489: step 21420/40890 (epoch 16/30), loss = 0.218525 (0.835 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:42:03.635510: step 21440/40890 (epoch 16/30), loss = 0.298572 (0.629 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:42:19.620767: step 21460/40890 (epoch 16/30), loss = 0.260890 (0.783 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:42:35.873308: step 21480/40890 (epoch 16/30), loss = 0.309211 (0.905 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:42:51.143482: step 21500/40890 (epoch 16/30), loss = 0.149210 (0.768 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:43:07.120763: step 21520/40890 (epoch 16/30), loss = 0.326988 (0.805 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:43:23.531880: step 21540/40890 (epoch 16/30), loss = 0.156780 (0.868 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:43:39.754502: step 21560/40890 (epoch 16/30), loss = 0.154875 (0.773 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:43:56.286802: step 21580/40890 (epoch 16/30), loss = 0.144882 (0.778 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:44:13.024054: step 21600/40890 (epoch 16/30), loss = 0.371151 (0.831 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:44:29.043219: step 21620/40890 (epoch 16/30), loss = 0.266290 (0.797 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:44:45.555575: step 21640/40890 (epoch 16/30), loss = 0.250951 (0.877 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:45:01.853999: step 21660/40890 (epoch 16/30), loss = 0.128656 (0.810 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:45:18.681905: step 21680/40890 (epoch 16/30), loss = 0.240344 (0.799 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:45:35.146879: step 21700/40890 (epoch 16/30), loss = 0.369521 (0.736 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:45:50.426023: step 21720/40890 (epoch 16/30), loss = 0.206015 (0.618 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:46:06.819188: step 21740/40890 (epoch 16/30), loss = 0.641960 (0.876 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:46:22.782900: step 21760/40890 (epoch 16/30), loss = 0.306919 (0.865 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:46:39.148148: step 21780/40890 (epoch 16/30), loss = 0.352945 (0.856 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:46:55.421727: step 21800/40890 (epoch 16/30), loss = 0.224982 (0.835 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.811%\n",
      "   Recall (micro): 63.208%\n",
      "       F1 (micro): 65.429%\n",
      "epoch 16: train_loss = 0.275425, dev_loss = 0.445317, dev_f1 = 0.6543\n",
      "model saved to ./save_models/01/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 03:47:45.130920: step 21820/40890 (epoch 17/30), loss = 0.150169 (0.809 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:48:00.921063: step 21840/40890 (epoch 17/30), loss = 0.177210 (0.758 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:48:16.832524: step 21860/40890 (epoch 17/30), loss = 0.420612 (0.757 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:48:32.183476: step 21880/40890 (epoch 17/30), loss = 0.215748 (0.868 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:48:48.423053: step 21900/40890 (epoch 17/30), loss = 0.342669 (0.657 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:49:03.964415: step 21920/40890 (epoch 17/30), loss = 0.223994 (0.749 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:49:19.557232: step 21940/40890 (epoch 17/30), loss = 0.223267 (0.753 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:49:35.299139: step 21960/40890 (epoch 17/30), loss = 0.168533 (0.714 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:49:51.208108: step 21980/40890 (epoch 17/30), loss = 0.356185 (0.725 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:50:07.590302: step 22000/40890 (epoch 17/30), loss = 0.227143 (0.742 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:50:23.404017: step 22020/40890 (epoch 17/30), loss = 0.157631 (0.754 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:50:39.139946: step 22040/40890 (epoch 17/30), loss = 0.278924 (0.874 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:50:54.821016: step 22060/40890 (epoch 17/30), loss = 0.415462 (0.737 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:51:10.892042: step 22080/40890 (epoch 17/30), loss = 0.285042 (0.841 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:51:26.689800: step 22100/40890 (epoch 17/30), loss = 0.266747 (0.707 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:51:42.112614: step 22120/40890 (epoch 17/30), loss = 0.446526 (0.845 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:51:57.886436: step 22140/40890 (epoch 17/30), loss = 0.306694 (0.883 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:52:13.641359: step 22160/40890 (epoch 17/30), loss = 0.241546 (0.815 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:52:29.515912: step 22180/40890 (epoch 17/30), loss = 0.333800 (0.594 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:52:45.286857: step 22200/40890 (epoch 17/30), loss = 0.060441 (0.758 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:53:01.206299: step 22220/40890 (epoch 17/30), loss = 0.113462 (0.782 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:53:17.535158: step 22240/40890 (epoch 17/30), loss = 0.301879 (0.893 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:53:34.143747: step 22260/40890 (epoch 17/30), loss = 0.142389 (0.824 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:53:50.816166: step 22280/40890 (epoch 17/30), loss = 0.597654 (0.852 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:54:07.411790: step 22300/40890 (epoch 17/30), loss = 0.181894 (0.871 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:54:25.061595: step 22320/40890 (epoch 17/30), loss = 0.211104 (0.904 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:54:41.933481: step 22340/40890 (epoch 17/30), loss = 0.236911 (0.874 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:54:58.426385: step 22360/40890 (epoch 17/30), loss = 0.134122 (0.873 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:55:14.885374: step 22380/40890 (epoch 17/30), loss = 0.299117 (0.899 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:55:31.047158: step 22400/40890 (epoch 17/30), loss = 0.295552 (0.680 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:55:46.791060: step 22420/40890 (epoch 17/30), loss = 0.259299 (0.805 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:56:02.812727: step 22440/40890 (epoch 17/30), loss = 0.323103 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:56:18.725178: step 22460/40890 (epoch 17/30), loss = 0.149770 (0.757 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:56:34.990685: step 22480/40890 (epoch 17/30), loss = 0.179153 (0.843 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:56:51.702005: step 22500/40890 (epoch 17/30), loss = 0.249572 (0.856 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:57:08.184940: step 22520/40890 (epoch 17/30), loss = 0.253545 (0.768 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:57:25.015935: step 22540/40890 (epoch 17/30), loss = 0.352092 (0.879 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:57:41.146802: step 22560/40890 (epoch 17/30), loss = 0.207972 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:57:57.007392: step 22580/40890 (epoch 17/30), loss = 0.331483 (0.821 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:58:13.567121: step 22600/40890 (epoch 17/30), loss = 0.258807 (0.756 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:58:29.967268: step 22620/40890 (epoch 17/30), loss = 0.198302 (0.748 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:58:45.700262: step 22640/40890 (epoch 17/30), loss = 0.372539 (0.820 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:59:02.055528: step 22660/40890 (epoch 17/30), loss = 0.234789 (0.653 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:59:17.048438: step 22680/40890 (epoch 17/30), loss = 0.123738 (0.768 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:59:33.388745: step 22700/40890 (epoch 17/30), loss = 0.350179 (0.785 sec/batch), lr: 0.810000\n",
      "2020-10-12 03:59:49.597404: step 22720/40890 (epoch 17/30), loss = 0.141214 (0.688 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:00:05.423086: step 22740/40890 (epoch 17/30), loss = 0.235345 (0.836 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:00:21.869620: step 22760/40890 (epoch 17/30), loss = 0.176779 (0.896 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:00:37.988519: step 22780/40890 (epoch 17/30), loss = 0.197475 (0.824 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:00:53.864743: step 22800/40890 (epoch 17/30), loss = 0.171644 (0.797 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:01:09.780240: step 22820/40890 (epoch 17/30), loss = 0.281648 (0.757 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:01:25.881187: step 22840/40890 (epoch 17/30), loss = 0.246411 (0.702 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:01:40.861698: step 22860/40890 (epoch 17/30), loss = 0.148927 (0.736 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:01:56.182730: step 22880/40890 (epoch 17/30), loss = 0.268604 (0.817 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:02:12.176033: step 22900/40890 (epoch 17/30), loss = 0.363334 (0.855 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:02:28.064549: step 22920/40890 (epoch 17/30), loss = 0.235081 (0.724 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:02:44.660172: step 22940/40890 (epoch 17/30), loss = 0.314429 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:03:00.520762: step 22960/40890 (epoch 17/30), loss = 0.169332 (0.850 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:03:15.915605: step 22980/40890 (epoch 17/30), loss = 0.280450 (0.643 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:03:32.191086: step 23000/40890 (epoch 17/30), loss = 0.170214 (0.885 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:03:48.895424: step 23020/40890 (epoch 17/30), loss = 0.202606 (0.886 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:04:05.884995: step 23040/40890 (epoch 17/30), loss = 0.262346 (0.868 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:04:24.682730: step 23060/40890 (epoch 17/30), loss = 0.166447 (0.793 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:04:39.809657: step 23080/40890 (epoch 17/30), loss = 0.427907 (0.814 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:04:56.156953: step 23100/40890 (epoch 17/30), loss = 0.258061 (0.834 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:05:12.071473: step 23120/40890 (epoch 17/30), loss = 0.312725 (0.910 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:05:28.538442: step 23140/40890 (epoch 17/30), loss = 0.282730 (0.656 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:05:44.839858: step 23160/40890 (epoch 17/30), loss = 0.135919 (0.834 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.025%\n",
      "   Recall (micro): 63.558%\n",
      "       F1 (micro): 65.716%\n",
      "epoch 17: train_loss = 0.267105, dev_loss = 0.444937, dev_f1 = 0.6572\n",
      "model saved to ./save_models/01/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "2020-10-12 04:06:34.638441: step 23180/40890 (epoch 18/30), loss = 0.255611 (0.709 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:06:50.825161: step 23200/40890 (epoch 18/30), loss = 0.146613 (0.852 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:07:07.020853: step 23220/40890 (epoch 18/30), loss = 0.502259 (0.809 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:07:22.560309: step 23240/40890 (epoch 18/30), loss = 0.272061 (0.826 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:07:39.119032: step 23260/40890 (epoch 18/30), loss = 0.169227 (0.923 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:07:55.068383: step 23280/40890 (epoch 18/30), loss = 0.443010 (0.977 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:08:10.351518: step 23300/40890 (epoch 18/30), loss = 0.382356 (0.709 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:08:26.321814: step 23320/40890 (epoch 18/30), loss = 0.352880 (0.852 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:08:42.082670: step 23340/40890 (epoch 18/30), loss = 0.240371 (0.815 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:08:58.309280: step 23360/40890 (epoch 18/30), loss = 0.174006 (0.845 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:09:14.112034: step 23380/40890 (epoch 18/30), loss = 0.198990 (0.633 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:09:29.716308: step 23400/40890 (epoch 18/30), loss = 0.176058 (0.705 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:09:45.824237: step 23420/40890 (epoch 18/30), loss = 0.234067 (0.690 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:10:01.848896: step 23440/40890 (epoch 18/30), loss = 0.174977 (0.829 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:10:17.399316: step 23460/40890 (epoch 18/30), loss = 0.369798 (0.816 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:10:32.685441: step 23480/40890 (epoch 18/30), loss = 0.423951 (0.544 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:10:48.477546: step 23500/40890 (epoch 18/30), loss = 0.194771 (0.793 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:11:04.099773: step 23520/40890 (epoch 18/30), loss = 0.315378 (0.869 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:11:20.353413: step 23540/40890 (epoch 18/30), loss = 0.258432 (0.852 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:11:35.736279: step 23560/40890 (epoch 18/30), loss = 0.370975 (0.718 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:11:51.176992: step 23580/40890 (epoch 18/30), loss = 0.220749 (0.895 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:12:06.703475: step 23600/40890 (epoch 18/30), loss = 0.180529 (0.788 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:12:23.058150: step 23620/40890 (epoch 18/30), loss = 0.211798 (0.819 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:12:39.449466: step 23640/40890 (epoch 18/30), loss = 0.442841 (0.850 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:12:55.588311: step 23660/40890 (epoch 18/30), loss = 0.280224 (0.670 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:13:11.976001: step 23680/40890 (epoch 18/30), loss = 0.362295 (0.759 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:13:28.398088: step 23700/40890 (epoch 18/30), loss = 0.293767 (0.836 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:13:44.932875: step 23720/40890 (epoch 18/30), loss = 0.307933 (0.628 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:14:01.599826: step 23740/40890 (epoch 18/30), loss = 0.266202 (1.060 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:14:19.297529: step 23760/40890 (epoch 18/30), loss = 0.310606 (0.902 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:14:34.972615: step 23780/40890 (epoch 18/30), loss = 0.252119 (0.671 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:14:51.259597: step 23800/40890 (epoch 18/30), loss = 0.230290 (0.739 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:15:07.162986: step 23820/40890 (epoch 18/30), loss = 0.310994 (0.813 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:15:23.470891: step 23840/40890 (epoch 18/30), loss = 0.137917 (0.843 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:15:40.052549: step 23860/40890 (epoch 18/30), loss = 0.198145 (0.750 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:15:56.673107: step 23880/40890 (epoch 18/30), loss = 0.324103 (0.724 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:16:13.331579: step 23900/40890 (epoch 18/30), loss = 0.221564 (0.766 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:16:29.911246: step 23920/40890 (epoch 18/30), loss = 0.179159 (0.855 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:16:45.635269: step 23940/40890 (epoch 18/30), loss = 0.167322 (0.801 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:17:02.247848: step 23960/40890 (epoch 18/30), loss = 0.290184 (0.684 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:17:19.165617: step 23980/40890 (epoch 18/30), loss = 0.261446 (0.641 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:17:35.197749: step 24000/40890 (epoch 18/30), loss = 0.301603 (0.909 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:17:51.757472: step 24020/40890 (epoch 18/30), loss = 0.409448 (0.812 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:18:06.950845: step 24040/40890 (epoch 18/30), loss = 0.235911 (0.684 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:18:22.962033: step 24060/40890 (epoch 18/30), loss = 0.734162 (0.838 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:18:39.261955: step 24080/40890 (epoch 18/30), loss = 0.445164 (0.888 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:18:55.028801: step 24100/40890 (epoch 18/30), loss = 0.215870 (0.773 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:19:11.245447: step 24120/40890 (epoch 18/30), loss = 0.241908 (0.887 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:19:27.568799: step 24140/40890 (epoch 18/30), loss = 0.263530 (0.719 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:19:43.324669: step 24160/40890 (epoch 18/30), loss = 0.296296 (0.806 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:19:59.463117: step 24180/40890 (epoch 18/30), loss = 0.270229 (0.864 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:20:15.666304: step 24200/40890 (epoch 18/30), loss = 0.400082 (0.787 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:20:30.557486: step 24220/40890 (epoch 18/30), loss = 0.163452 (0.768 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:20:45.871603: step 24240/40890 (epoch 18/30), loss = 0.128465 (0.866 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:21:01.873814: step 24260/40890 (epoch 18/30), loss = 0.195717 (0.727 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:21:17.549950: step 24280/40890 (epoch 18/30), loss = 0.452331 (0.744 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:21:33.838395: step 24300/40890 (epoch 18/30), loss = 0.181640 (0.814 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:21:49.336957: step 24320/40890 (epoch 18/30), loss = 0.467085 (0.850 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:22:04.304934: step 24340/40890 (epoch 18/30), loss = 0.105570 (0.631 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:22:20.065790: step 24360/40890 (epoch 18/30), loss = 0.431227 (0.718 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:22:36.495857: step 24380/40890 (epoch 18/30), loss = 0.411344 (0.842 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:22:53.231468: step 24400/40890 (epoch 18/30), loss = 0.117778 (0.788 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:23:09.846125: step 24420/40890 (epoch 18/30), loss = 0.246274 (0.875 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:23:25.284843: step 24440/40890 (epoch 18/30), loss = 0.072993 (0.687 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:23:41.322958: step 24460/40890 (epoch 18/30), loss = 0.209622 (0.795 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:23:57.115729: step 24480/40890 (epoch 18/30), loss = 0.166789 (0.879 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:24:14.563096: step 24500/40890 (epoch 18/30), loss = 0.257673 (0.730 sec/batch), lr: 0.810000\n",
      "2020-10-12 04:24:31.456923: step 24520/40890 (epoch 18/30), loss = 0.159911 (0.780 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.934%\n",
      "   Recall (micro): 61.461%\n",
      "       F1 (micro): 64.535%\n",
      "epoch 18: train_loss = 0.264154, dev_loss = 0.456669, dev_f1 = 0.6454\n",
      "model saved to ./save_models/01/checkpoint_epoch_18.pt\n",
      "\n",
      "2020-10-12 04:25:21.769905: step 24540/40890 (epoch 19/30), loss = 0.216795 (0.911 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:25:37.438009: step 24560/40890 (epoch 19/30), loss = 0.227528 (0.685 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:25:53.695537: step 24580/40890 (epoch 19/30), loss = 0.331816 (0.799 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:26:09.433820: step 24600/40890 (epoch 19/30), loss = 0.180502 (0.800 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:26:26.024458: step 24620/40890 (epoch 19/30), loss = 0.226963 (0.823 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:26:42.145351: step 24640/40890 (epoch 19/30), loss = 0.244233 (0.897 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:26:57.945103: step 24660/40890 (epoch 19/30), loss = 0.317285 (0.746 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:27:14.144786: step 24680/40890 (epoch 19/30), loss = 0.335098 (0.736 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:27:30.409296: step 24700/40890 (epoch 19/30), loss = 0.232411 (0.804 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:27:46.562105: step 24720/40890 (epoch 19/30), loss = 0.425425 (0.714 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:28:02.428768: step 24740/40890 (epoch 19/30), loss = 0.173555 (0.666 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:28:18.130782: step 24760/40890 (epoch 19/30), loss = 0.208066 (0.717 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:28:34.081131: step 24780/40890 (epoch 19/30), loss = 0.072002 (0.861 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:28:49.713332: step 24800/40890 (epoch 19/30), loss = 0.337502 (0.823 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:29:05.769398: step 24820/40890 (epoch 19/30), loss = 0.267765 (0.836 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:29:21.758033: step 24840/40890 (epoch 19/30), loss = 0.270141 (0.812 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:29:37.294489: step 24860/40890 (epoch 19/30), loss = 0.346333 (0.821 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:29:52.941649: step 24880/40890 (epoch 19/30), loss = 0.162652 (0.845 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:30:09.247049: step 24900/40890 (epoch 19/30), loss = 0.282112 (0.876 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:30:24.639889: step 24920/40890 (epoch 19/30), loss = 0.122626 (0.640 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:30:39.628810: step 24940/40890 (epoch 19/30), loss = 0.121392 (0.705 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:30:55.207216: step 24960/40890 (epoch 19/30), loss = 0.437794 (0.836 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:31:11.664717: step 24980/40890 (epoch 19/30), loss = 0.379467 (0.753 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:31:27.994053: step 25000/40890 (epoch 19/30), loss = 0.246309 (0.832 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:31:43.656180: step 25020/40890 (epoch 19/30), loss = 0.278536 (0.600 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:31:59.516769: step 25040/40890 (epoch 19/30), loss = 0.530993 (0.606 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:32:15.526474: step 25060/40890 (epoch 19/30), loss = 0.214157 (0.776 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:32:32.057271: step 25080/40890 (epoch 19/30), loss = 0.279766 (0.833 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:32:48.236626: step 25100/40890 (epoch 19/30), loss = 0.120241 (0.931 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:33:04.315672: step 25120/40890 (epoch 19/30), loss = 0.351917 (0.783 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:33:20.959168: step 25140/40890 (epoch 19/30), loss = 0.496230 (0.722 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:33:37.867955: step 25160/40890 (epoch 19/30), loss = 0.094619 (0.779 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:33:53.950042: step 25180/40890 (epoch 19/30), loss = 0.223304 (0.900 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:34:11.312867: step 25200/40890 (epoch 19/30), loss = 0.358003 (0.985 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:34:29.357616: step 25220/40890 (epoch 19/30), loss = 0.179595 (0.806 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:34:46.314275: step 25240/40890 (epoch 19/30), loss = 0.239847 (0.750 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:35:03.106374: step 25260/40890 (epoch 19/30), loss = 0.154737 (0.837 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:35:20.048072: step 25280/40890 (epoch 19/30), loss = 0.261302 (0.872 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:35:35.939579: step 25300/40890 (epoch 19/30), loss = 0.345445 (0.908 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:35:52.270910: step 25320/40890 (epoch 19/30), loss = 0.190758 (0.945 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:36:09.307861: step 25340/40890 (epoch 19/30), loss = 0.280373 (0.934 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:36:25.060739: step 25360/40890 (epoch 19/30), loss = 0.469629 (0.714 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:36:42.900543: step 25380/40890 (epoch 19/30), loss = 0.253454 (1.022 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:36:58.465922: step 25400/40890 (epoch 19/30), loss = 0.183714 (0.705 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:37:14.736785: step 25420/40890 (epoch 19/30), loss = 0.094149 (0.850 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:37:31.259604: step 25440/40890 (epoch 19/30), loss = 0.441681 (0.890 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:37:47.574326: step 25460/40890 (epoch 19/30), loss = 0.227542 (0.725 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:38:03.508726: step 25480/40890 (epoch 19/30), loss = 0.368324 (0.767 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:38:20.070441: step 25500/40890 (epoch 19/30), loss = 0.351112 (0.831 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:38:35.925046: step 25520/40890 (epoch 19/30), loss = 0.497962 (0.709 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:38:52.028587: step 25540/40890 (epoch 19/30), loss = 0.356123 (0.863 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:39:08.111638: step 25560/40890 (epoch 19/30), loss = 0.173585 (0.862 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:39:23.064654: step 25580/40890 (epoch 19/30), loss = 0.327331 (0.815 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:39:38.524316: step 25600/40890 (epoch 19/30), loss = 0.293219 (0.879 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:39:55.192255: step 25620/40890 (epoch 19/30), loss = 0.175830 (0.708 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:40:10.975179: step 25640/40890 (epoch 19/30), loss = 0.128008 (0.799 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:40:28.360690: step 25660/40890 (epoch 19/30), loss = 0.309312 (1.014 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:40:44.574841: step 25680/40890 (epoch 19/30), loss = 0.242178 (0.637 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:40:59.999596: step 25700/40890 (epoch 19/30), loss = 0.134020 (0.836 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:41:15.735046: step 25720/40890 (epoch 19/30), loss = 0.253227 (0.818 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:41:31.715315: step 25740/40890 (epoch 19/30), loss = 0.293393 (0.897 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:41:48.056620: step 25760/40890 (epoch 19/30), loss = 0.188822 (0.880 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:42:04.063818: step 25780/40890 (epoch 19/30), loss = 0.170319 (0.862 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:42:19.691032: step 25800/40890 (epoch 19/30), loss = 0.163435 (0.704 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:42:35.119775: step 25820/40890 (epoch 19/30), loss = 0.285172 (0.858 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:42:50.949015: step 25840/40890 (epoch 19/30), loss = 0.280989 (0.940 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:43:07.260464: step 25860/40890 (epoch 19/30), loss = 0.296682 (0.906 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:43:23.622596: step 25880/40890 (epoch 19/30), loss = 0.329217 (0.767 sec/batch), lr: 0.729000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.027%\n",
      "   Recall (micro): 61.957%\n",
      "       F1 (micro): 64.850%\n",
      "epoch 19: train_loss = 0.256365, dev_loss = 0.464478, dev_f1 = 0.6485\n",
      "model saved to ./save_models/01/checkpoint_epoch_19.pt\n",
      "\n",
      "2020-10-12 04:44:16.033961: step 25900/40890 (epoch 20/30), loss = 0.252088 (0.829 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:44:32.394215: step 25920/40890 (epoch 20/30), loss = 0.294996 (0.749 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:44:48.421360: step 25940/40890 (epoch 20/30), loss = 0.472107 (0.852 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:45:04.489501: step 25960/40890 (epoch 20/30), loss = 0.283478 (0.675 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:45:21.130005: step 25980/40890 (epoch 20/30), loss = 0.303740 (0.863 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:45:37.225965: step 26000/40890 (epoch 20/30), loss = 0.138378 (0.757 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:45:53.258096: step 26020/40890 (epoch 20/30), loss = 0.157969 (0.831 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:46:09.551528: step 26040/40890 (epoch 20/30), loss = 0.354428 (0.834 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:46:25.842966: step 26060/40890 (epoch 20/30), loss = 0.314923 (0.784 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:46:42.325891: step 26080/40890 (epoch 20/30), loss = 0.174554 (0.743 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:46:58.724043: step 26100/40890 (epoch 20/30), loss = 0.321594 (0.872 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:47:14.839956: step 26120/40890 (epoch 20/30), loss = 0.306551 (0.780 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:47:31.079532: step 26140/40890 (epoch 20/30), loss = 0.270709 (0.886 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:47:46.938128: step 26160/40890 (epoch 20/30), loss = 0.058334 (0.701 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:48:02.856563: step 26180/40890 (epoch 20/30), loss = 0.214271 (0.895 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:48:18.759922: step 26200/40890 (epoch 20/30), loss = 0.447100 (0.804 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:48:34.298373: step 26220/40890 (epoch 20/30), loss = 0.329476 (0.643 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:48:49.818872: step 26240/40890 (epoch 20/30), loss = 0.153145 (0.719 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:49:06.001606: step 26260/40890 (epoch 20/30), loss = 0.241757 (0.748 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:49:21.682676: step 26280/40890 (epoch 20/30), loss = 0.240279 (0.756 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:49:37.117409: step 26300/40890 (epoch 20/30), loss = 0.121056 (0.750 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:49:52.607324: step 26320/40890 (epoch 20/30), loss = 0.431295 (0.698 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:50:09.065330: step 26340/40890 (epoch 20/30), loss = 0.170459 (0.890 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:50:25.196197: step 26360/40890 (epoch 20/30), loss = 0.376809 (0.781 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:50:41.099672: step 26380/40890 (epoch 20/30), loss = 0.186410 (0.866 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:50:56.847570: step 26400/40890 (epoch 20/30), loss = 0.164440 (0.805 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:51:12.945543: step 26420/40890 (epoch 20/30), loss = 0.286350 (0.831 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:51:29.297818: step 26440/40890 (epoch 20/30), loss = 0.265737 (0.774 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:51:44.914061: step 26460/40890 (epoch 20/30), loss = 0.206744 (0.795 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:52:00.723794: step 26480/40890 (epoch 20/30), loss = 0.161697 (0.619 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:52:16.187449: step 26500/40890 (epoch 20/30), loss = 0.237377 (0.815 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:52:31.716924: step 26520/40890 (epoch 20/30), loss = 0.285838 (0.774 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:52:47.678244: step 26540/40890 (epoch 20/30), loss = 0.231212 (0.683 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:53:04.608037: step 26560/40890 (epoch 20/30), loss = 0.356372 (0.760 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:53:20.955325: step 26580/40890 (epoch 20/30), loss = 0.151219 (0.847 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:53:38.046631: step 26600/40890 (epoch 20/30), loss = 0.182843 (0.959 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:53:55.125477: step 26620/40890 (epoch 20/30), loss = 0.101967 (0.882 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:54:12.614811: step 26640/40890 (epoch 20/30), loss = 0.227258 (0.912 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:54:29.247336: step 26660/40890 (epoch 20/30), loss = 0.219147 (0.846 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:54:46.015499: step 26680/40890 (epoch 20/30), loss = 0.223419 (0.897 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:55:03.023394: step 26700/40890 (epoch 20/30), loss = 0.240055 (0.942 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:55:19.131322: step 26720/40890 (epoch 20/30), loss = 0.321668 (0.893 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:55:35.926413: step 26740/40890 (epoch 20/30), loss = 0.500370 (0.865 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:55:52.032361: step 26760/40890 (epoch 20/30), loss = 0.268148 (0.771 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:56:07.821153: step 26780/40890 (epoch 20/30), loss = 0.194359 (0.748 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:56:24.337992: step 26800/40890 (epoch 20/30), loss = 0.141875 (0.847 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:56:41.149040: step 26820/40890 (epoch 20/30), loss = 0.079223 (0.942 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:56:57.353710: step 26840/40890 (epoch 20/30), loss = 0.414060 (0.910 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:57:14.241552: step 26860/40890 (epoch 20/30), loss = 0.170358 (0.598 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:57:30.822216: step 26880/40890 (epoch 20/30), loss = 0.262845 (0.860 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:57:46.501863: step 26900/40890 (epoch 20/30), loss = 0.225167 (0.625 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:58:02.582863: step 26920/40890 (epoch 20/30), loss = 0.185785 (0.793 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:58:17.652567: step 26940/40890 (epoch 20/30), loss = 0.186428 (0.662 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:58:33.144144: step 26960/40890 (epoch 20/30), loss = 0.347784 (0.857 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:58:49.176274: step 26980/40890 (epoch 20/30), loss = 0.247756 (0.903 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:59:04.597040: step 27000/40890 (epoch 20/30), loss = 0.315651 (0.698 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:59:20.931363: step 27020/40890 (epoch 20/30), loss = 0.124077 (0.794 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:59:36.746075: step 27040/40890 (epoch 20/30), loss = 0.282740 (0.650 sec/batch), lr: 0.729000\n",
      "2020-10-12 04:59:52.140750: step 27060/40890 (epoch 20/30), loss = 0.070780 (0.768 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:00:07.441177: step 27080/40890 (epoch 20/30), loss = 0.377567 (0.554 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:00:23.499238: step 27100/40890 (epoch 20/30), loss = 0.223353 (0.820 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:00:39.686463: step 27120/40890 (epoch 20/30), loss = 0.304990 (0.751 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:00:55.838282: step 27140/40890 (epoch 20/30), loss = 0.371773 (0.733 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:01:11.733778: step 27160/40890 (epoch 20/30), loss = 0.249764 (0.748 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:01:26.870304: step 27180/40890 (epoch 20/30), loss = 0.241126 (0.763 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:01:42.287088: step 27200/40890 (epoch 20/30), loss = 0.349247 (0.688 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:01:58.087838: step 27220/40890 (epoch 20/30), loss = 0.173051 (0.800 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:02:14.226683: step 27240/40890 (epoch 20/30), loss = 0.171474 (0.865 sec/batch), lr: 0.729000\n",
      "2020-10-12 05:02:29.973577: step 27260/40890 (epoch 20/30), loss = 0.016979 (0.470 sec/batch), lr: 0.729000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.832%\n",
      "   Recall (micro): 59.842%\n",
      "       F1 (micro): 64.023%\n",
      "epoch 20: train_loss = 0.249243, dev_loss = 0.464919, dev_f1 = 0.6402\n",
      "model saved to ./save_models/01/checkpoint_epoch_20.pt\n",
      "\n",
      "2020-10-12 05:03:21.092052: step 27280/40890 (epoch 21/30), loss = 0.195015 (0.813 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:03:36.772125: step 27300/40890 (epoch 21/30), loss = 0.111283 (0.821 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:03:52.902016: step 27320/40890 (epoch 21/30), loss = 0.179453 (0.788 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:04:10.093048: step 27340/40890 (epoch 21/30), loss = 0.120155 (0.923 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:04:28.015126: step 27360/40890 (epoch 21/30), loss = 0.091503 (0.871 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:04:44.075182: step 27380/40890 (epoch 21/30), loss = 0.281590 (0.905 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:05:00.044481: step 27400/40890 (epoch 21/30), loss = 0.335904 (0.886 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:05:16.388777: step 27420/40890 (epoch 21/30), loss = 0.117367 (0.906 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:05:33.175300: step 27440/40890 (epoch 21/30), loss = 0.090561 (0.873 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:05:49.607362: step 27460/40890 (epoch 21/30), loss = 0.263421 (0.846 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:06:05.675470: step 27480/40890 (epoch 21/30), loss = 0.246772 (0.665 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:06:21.650752: step 27500/40890 (epoch 21/30), loss = 0.186712 (0.638 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:06:37.926232: step 27520/40890 (epoch 21/30), loss = 0.202199 (0.758 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:06:53.967339: step 27540/40890 (epoch 21/30), loss = 0.149798 (0.675 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:07:10.196961: step 27560/40890 (epoch 21/30), loss = 0.270690 (0.864 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:07:25.992724: step 27580/40890 (epoch 21/30), loss = 0.135840 (0.839 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:07:41.792476: step 27600/40890 (epoch 21/30), loss = 0.246969 (0.670 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:07:57.782719: step 27620/40890 (epoch 21/30), loss = 0.654139 (0.711 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:08:13.439980: step 27640/40890 (epoch 21/30), loss = 0.169358 (0.798 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:08:29.065199: step 27660/40890 (epoch 21/30), loss = 0.356450 (0.894 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:08:44.506253: step 27680/40890 (epoch 21/30), loss = 0.072199 (0.810 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:09:00.510459: step 27700/40890 (epoch 21/30), loss = 0.388841 (0.746 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:09:16.631423: step 27720/40890 (epoch 21/30), loss = 0.352296 (0.879 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:09:32.681505: step 27740/40890 (epoch 21/30), loss = 0.199687 (0.868 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:09:48.206992: step 27760/40890 (epoch 21/30), loss = 0.350557 (0.808 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:10:04.302952: step 27780/40890 (epoch 21/30), loss = 0.361930 (0.862 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:10:20.531912: step 27800/40890 (epoch 21/30), loss = 0.267319 (0.804 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:10:36.304737: step 27820/40890 (epoch 21/30), loss = 0.432939 (0.742 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:10:52.329886: step 27840/40890 (epoch 21/30), loss = 0.211281 (0.708 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:11:07.586092: step 27860/40890 (epoch 21/30), loss = 0.323367 (0.638 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:11:23.075674: step 27880/40890 (epoch 21/30), loss = 0.274278 (0.797 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:11:38.782673: step 27900/40890 (epoch 21/30), loss = 0.288359 (0.636 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:11:54.077780: step 27920/40890 (epoch 21/30), loss = 0.091423 (0.729 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:12:10.059047: step 27940/40890 (epoch 21/30), loss = 0.309694 (0.814 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:12:26.342506: step 27960/40890 (epoch 21/30), loss = 0.344188 (0.813 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:12:42.814460: step 27980/40890 (epoch 21/30), loss = 0.228008 (0.796 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:12:59.219594: step 28000/40890 (epoch 21/30), loss = 0.158964 (0.632 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:13:15.283640: step 28020/40890 (epoch 21/30), loss = 0.369510 (0.928 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:13:31.441434: step 28040/40890 (epoch 21/30), loss = 0.199279 (0.936 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:13:48.480872: step 28060/40890 (epoch 21/30), loss = 0.474995 (0.711 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:14:05.259008: step 28080/40890 (epoch 21/30), loss = 0.274900 (0.987 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:14:23.103800: step 28100/40890 (epoch 21/30), loss = 0.318838 (0.840 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:14:39.525397: step 28120/40890 (epoch 21/30), loss = 0.280892 (0.822 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:14:55.600419: step 28140/40890 (epoch 21/30), loss = 0.142856 (0.867 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:15:11.948704: step 28160/40890 (epoch 21/30), loss = 0.195494 (0.847 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:15:28.682958: step 28180/40890 (epoch 21/30), loss = 0.288222 (0.734 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:15:44.952531: step 28200/40890 (epoch 21/30), loss = 0.100626 (0.885 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:16:01.820427: step 28220/40890 (epoch 21/30), loss = 0.334781 (0.719 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:16:18.253992: step 28240/40890 (epoch 21/30), loss = 0.281671 (0.774 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:16:34.445696: step 28260/40890 (epoch 21/30), loss = 0.305143 (0.926 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:16:50.832878: step 28280/40890 (epoch 21/30), loss = 0.182609 (0.850 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:17:06.593734: step 28300/40890 (epoch 21/30), loss = 0.320832 (0.644 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:17:22.022478: step 28320/40890 (epoch 21/30), loss = 0.351221 (0.635 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:17:38.300951: step 28340/40890 (epoch 21/30), loss = 0.304519 (0.798 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:17:54.122153: step 28360/40890 (epoch 21/30), loss = 0.291731 (0.870 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:18:10.393063: step 28380/40890 (epoch 21/30), loss = 0.372323 (0.755 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:18:26.251658: step 28400/40890 (epoch 21/30), loss = 0.142104 (0.748 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:18:41.598130: step 28420/40890 (epoch 21/30), loss = 0.167543 (0.723 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:18:57.081728: step 28440/40890 (epoch 21/30), loss = 0.192078 (0.874 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:19:12.990241: step 28460/40890 (epoch 21/30), loss = 0.167142 (0.810 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:19:29.137065: step 28480/40890 (epoch 21/30), loss = 0.302674 (0.784 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:19:45.269926: step 28500/40890 (epoch 21/30), loss = 0.180867 (0.832 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:20:01.294589: step 28520/40890 (epoch 21/30), loss = 0.502263 (0.924 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:20:16.424134: step 28540/40890 (epoch 21/30), loss = 0.048222 (0.798 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:20:32.108195: step 28560/40890 (epoch 21/30), loss = 0.274612 (0.625 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:20:47.492628: step 28580/40890 (epoch 21/30), loss = 0.106999 (0.613 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:21:03.642444: step 28600/40890 (epoch 21/30), loss = 0.191293 (0.705 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:21:20.121384: step 28620/40890 (epoch 21/30), loss = 0.102613 (0.836 sec/batch), lr: 0.656100\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.793%\n",
      "   Recall (micro): 64.606%\n",
      "       F1 (micro): 65.682%\n",
      "epoch 21: train_loss = 0.240092, dev_loss = 0.468049, dev_f1 = 0.6568\n",
      "model saved to ./save_models/01/checkpoint_epoch_21.pt\n",
      "\n",
      "2020-10-12 05:22:10.363804: step 28640/40890 (epoch 22/30), loss = 0.170320 (0.874 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:22:26.585428: step 28660/40890 (epoch 22/30), loss = 0.131644 (0.901 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:22:43.096476: step 28680/40890 (epoch 22/30), loss = 0.313993 (0.744 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:22:59.541502: step 28700/40890 (epoch 22/30), loss = 0.249303 (0.840 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:23:15.962699: step 28720/40890 (epoch 22/30), loss = 0.320927 (0.789 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:23:31.854277: step 28740/40890 (epoch 22/30), loss = 0.098627 (0.807 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:23:48.152710: step 28760/40890 (epoch 22/30), loss = 0.394923 (0.812 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:24:04.812943: step 28780/40890 (epoch 22/30), loss = 0.354871 (0.780 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:24:22.969456: step 28800/40890 (epoch 22/30), loss = 0.305396 (0.919 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:24:39.456263: step 28820/40890 (epoch 22/30), loss = 0.309695 (0.834 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:24:56.017983: step 28840/40890 (epoch 22/30), loss = 0.105161 (0.923 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:25:12.089010: step 28860/40890 (epoch 22/30), loss = 0.276809 (0.913 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:25:28.467446: step 28880/40890 (epoch 22/30), loss = 0.210112 (0.833 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:25:44.886541: step 28900/40890 (epoch 22/30), loss = 0.230886 (0.816 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:26:01.208166: step 28920/40890 (epoch 22/30), loss = 0.402956 (0.667 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:26:17.122613: step 28940/40890 (epoch 22/30), loss = 0.169100 (0.821 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:26:33.648769: step 28960/40890 (epoch 22/30), loss = 0.103315 (0.834 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:26:50.055414: step 28980/40890 (epoch 22/30), loss = 0.170281 (0.878 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:27:05.999789: step 29000/40890 (epoch 22/30), loss = 0.107523 (0.830 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:27:22.037420: step 29020/40890 (epoch 22/30), loss = 0.201168 (0.827 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:27:38.161306: step 29040/40890 (epoch 22/30), loss = 0.174703 (0.862 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:27:54.240247: step 29060/40890 (epoch 22/30), loss = 0.305852 (0.892 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:28:10.414003: step 29080/40890 (epoch 22/30), loss = 0.197738 (0.927 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:28:26.534914: step 29100/40890 (epoch 22/30), loss = 0.542711 (0.843 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:28:42.353946: step 29120/40890 (epoch 22/30), loss = 0.278442 (0.714 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:28:58.667440: step 29140/40890 (epoch 22/30), loss = 0.354816 (0.763 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:29:15.071576: step 29160/40890 (epoch 22/30), loss = 0.244020 (0.883 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:29:30.980043: step 29180/40890 (epoch 22/30), loss = 0.366960 (0.825 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:29:47.011176: step 29200/40890 (epoch 22/30), loss = 0.201400 (0.890 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:30:02.499761: step 29220/40890 (epoch 22/30), loss = 0.289595 (0.738 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:30:18.091582: step 29240/40890 (epoch 22/30), loss = 0.288046 (0.685 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:30:33.960655: step 29260/40890 (epoch 22/30), loss = 0.156505 (0.834 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:30:49.388422: step 29280/40890 (epoch 22/30), loss = 0.211284 (0.751 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:31:05.235610: step 29300/40890 (epoch 22/30), loss = 0.340617 (0.817 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:31:21.931479: step 29320/40890 (epoch 22/30), loss = 0.499245 (0.837 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:31:38.108171: step 29340/40890 (epoch 22/30), loss = 0.253628 (0.859 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:31:54.404103: step 29360/40890 (epoch 22/30), loss = 0.244673 (0.833 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:32:10.143608: step 29380/40890 (epoch 22/30), loss = 0.404857 (0.830 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:32:26.022150: step 29400/40890 (epoch 22/30), loss = 0.131622 (0.758 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:32:42.823229: step 29420/40890 (epoch 22/30), loss = 0.198491 (0.865 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:32:59.366081: step 29440/40890 (epoch 22/30), loss = 0.170287 (0.698 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:33:15.613214: step 29460/40890 (epoch 22/30), loss = 0.128304 (0.785 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:33:32.514534: step 29480/40890 (epoch 22/30), loss = 0.205202 (0.757 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:33:48.207148: step 29500/40890 (epoch 22/30), loss = 0.202895 (0.759 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:34:05.297257: step 29520/40890 (epoch 22/30), loss = 0.154967 (0.867 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:34:24.013212: step 29540/40890 (epoch 22/30), loss = 0.175336 (0.830 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:34:40.327654: step 29560/40890 (epoch 22/30), loss = 0.136590 (0.902 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:34:57.412969: step 29580/40890 (epoch 22/30), loss = 0.162739 (0.930 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:35:14.171778: step 29600/40890 (epoch 22/30), loss = 0.332072 (0.880 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:35:30.387430: step 29620/40890 (epoch 22/30), loss = 0.201422 (0.912 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:35:46.744324: step 29640/40890 (epoch 22/30), loss = 0.047265 (0.702 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:36:03.142496: step 29660/40890 (epoch 22/30), loss = 0.203290 (0.738 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:36:18.642051: step 29680/40890 (epoch 22/30), loss = 0.141197 (0.680 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:36:34.778902: step 29700/40890 (epoch 22/30), loss = 0.139746 (0.833 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:36:51.030800: step 29720/40890 (epoch 22/30), loss = 0.169630 (0.722 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:37:07.435940: step 29740/40890 (epoch 22/30), loss = 0.135560 (0.850 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:37:23.887017: step 29760/40890 (epoch 22/30), loss = 0.280950 (0.697 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:37:39.585099: step 29780/40890 (epoch 22/30), loss = 0.344435 (0.659 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:37:54.855773: step 29800/40890 (epoch 22/30), loss = 0.116340 (0.935 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:38:11.036518: step 29820/40890 (epoch 22/30), loss = 0.227167 (0.763 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:38:27.037369: step 29840/40890 (epoch 22/30), loss = 0.190495 (0.754 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:38:43.396630: step 29860/40890 (epoch 22/30), loss = 0.240833 (0.705 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:38:59.643193: step 29880/40890 (epoch 22/30), loss = 0.252103 (0.675 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:39:15.069945: step 29900/40890 (epoch 22/30), loss = 0.205230 (0.919 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:39:31.057315: step 29920/40890 (epoch 22/30), loss = 0.103612 (0.863 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:39:46.334465: step 29940/40890 (epoch 22/30), loss = 0.362660 (0.682 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:40:03.505334: step 29960/40890 (epoch 22/30), loss = 0.422476 (0.720 sec/batch), lr: 0.656100\n",
      "2020-10-12 05:40:19.887080: step 29980/40890 (epoch 22/30), loss = 0.197158 (0.858 sec/batch), lr: 0.656100\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.305%\n",
      "   Recall (micro): 63.153%\n",
      "       F1 (micro): 65.628%\n",
      "epoch 22: train_loss = 0.237902, dev_loss = 0.465713, dev_f1 = 0.6563\n",
      "model saved to ./save_models/01/checkpoint_epoch_22.pt\n",
      "\n",
      "2020-10-12 05:41:08.675651: step 30000/40890 (epoch 23/30), loss = 0.215102 (0.773 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:41:24.245019: step 30020/40890 (epoch 23/30), loss = 0.217244 (0.657 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:41:40.044771: step 30040/40890 (epoch 23/30), loss = 0.347412 (0.601 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:41:55.535466: step 30060/40890 (epoch 23/30), loss = 0.253259 (0.750 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:42:12.074407: step 30080/40890 (epoch 23/30), loss = 0.201843 (0.836 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:42:27.595906: step 30100/40890 (epoch 23/30), loss = 0.324214 (0.832 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:42:43.431132: step 30120/40890 (epoch 23/30), loss = 0.128157 (0.719 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:42:59.757476: step 30140/40890 (epoch 23/30), loss = 0.297177 (0.856 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:43:16.202147: step 30160/40890 (epoch 23/30), loss = 0.220567 (0.907 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:43:32.674108: step 30180/40890 (epoch 23/30), loss = 0.410240 (0.703 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:43:49.446908: step 30200/40890 (epoch 23/30), loss = 0.124197 (0.918 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:44:05.716986: step 30220/40890 (epoch 23/30), loss = 0.416839 (0.755 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:44:23.603044: step 30240/40890 (epoch 23/30), loss = 0.205398 (0.904 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:44:40.117402: step 30260/40890 (epoch 23/30), loss = 0.285459 (0.851 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:44:56.547053: step 30280/40890 (epoch 23/30), loss = 0.163515 (0.836 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:45:12.278614: step 30300/40890 (epoch 23/30), loss = 0.136373 (0.932 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:45:28.682750: step 30320/40890 (epoch 23/30), loss = 0.269946 (0.851 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:45:44.717383: step 30340/40890 (epoch 23/30), loss = 0.182196 (0.659 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:46:00.990422: step 30360/40890 (epoch 23/30), loss = 0.071821 (0.813 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:46:17.038575: step 30380/40890 (epoch 23/30), loss = 0.234896 (0.958 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:46:33.045841: step 30400/40890 (epoch 23/30), loss = 0.131518 (0.896 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:46:49.310352: step 30420/40890 (epoch 23/30), loss = 0.416040 (0.829 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:47:05.958834: step 30440/40890 (epoch 23/30), loss = 0.445282 (0.900 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:47:22.417308: step 30460/40890 (epoch 23/30), loss = 0.322855 (0.811 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:47:38.701652: step 30480/40890 (epoch 23/30), loss = 0.284609 (0.752 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:47:54.923283: step 30500/40890 (epoch 23/30), loss = 0.167295 (0.867 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:48:11.193795: step 30520/40890 (epoch 23/30), loss = 0.109742 (0.918 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:48:27.168641: step 30540/40890 (epoch 23/30), loss = 0.219315 (0.669 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:48:43.428244: step 30560/40890 (epoch 23/30), loss = 0.129017 (0.918 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:48:59.143315: step 30580/40890 (epoch 23/30), loss = 0.109474 (0.875 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:49:14.441760: step 30600/40890 (epoch 23/30), loss = 0.349372 (0.899 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:49:30.257329: step 30620/40890 (epoch 23/30), loss = 0.069611 (0.926 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:49:45.921454: step 30640/40890 (epoch 23/30), loss = 0.236156 (0.891 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:50:01.596388: step 30660/40890 (epoch 23/30), loss = 0.366055 (0.750 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:50:17.984621: step 30680/40890 (epoch 23/30), loss = 0.191849 (0.890 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:50:34.135368: step 30700/40890 (epoch 23/30), loss = 0.334122 (0.832 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:50:50.664237: step 30720/40890 (epoch 23/30), loss = 0.225032 (0.792 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:51:06.663456: step 30740/40890 (epoch 23/30), loss = 0.281921 (0.851 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:51:22.348519: step 30760/40890 (epoch 23/30), loss = 0.168721 (0.838 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:51:38.849967: step 30780/40890 (epoch 23/30), loss = 0.143821 (0.750 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:51:55.105500: step 30800/40890 (epoch 23/30), loss = 0.283580 (0.771 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:52:10.989544: step 30820/40890 (epoch 23/30), loss = 0.300556 (0.792 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:52:27.335344: step 30840/40890 (epoch 23/30), loss = 0.287280 (0.746 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:52:42.908574: step 30860/40890 (epoch 23/30), loss = 0.338646 (0.904 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:52:59.563630: step 30880/40890 (epoch 23/30), loss = 0.182206 (0.801 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:53:16.322399: step 30900/40890 (epoch 23/30), loss = 0.175785 (0.772 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:53:32.445361: step 30920/40890 (epoch 23/30), loss = 0.282755 (0.717 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:53:49.449409: step 30940/40890 (epoch 23/30), loss = 0.216251 (0.752 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:54:06.382286: step 30960/40890 (epoch 23/30), loss = 0.144939 (0.747 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:54:24.195842: step 30980/40890 (epoch 23/30), loss = 0.324489 (0.794 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:54:40.981308: step 31000/40890 (epoch 23/30), loss = 0.197170 (0.733 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:54:57.495664: step 31020/40890 (epoch 23/30), loss = 0.054659 (0.828 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:55:13.208652: step 31040/40890 (epoch 23/30), loss = 0.085999 (0.746 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:55:29.078218: step 31060/40890 (epoch 23/30), loss = 0.210017 (0.894 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:55:45.375255: step 31080/40890 (epoch 23/30), loss = 0.249026 (0.692 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:56:02.071609: step 31100/40890 (epoch 23/30), loss = 0.138174 (0.879 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:56:18.741036: step 31120/40890 (epoch 23/30), loss = 0.322951 (0.850 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:56:34.638933: step 31140/40890 (epoch 23/30), loss = 0.355997 (0.856 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:56:50.143490: step 31160/40890 (epoch 23/30), loss = 0.248121 (0.729 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:57:06.560675: step 31180/40890 (epoch 23/30), loss = 0.414501 (0.704 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:57:23.177638: step 31200/40890 (epoch 23/30), loss = 0.320635 (0.696 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:57:39.924881: step 31220/40890 (epoch 23/30), loss = 0.224453 (0.749 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:57:56.283145: step 31240/40890 (epoch 23/30), loss = 0.176954 (0.808 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:58:11.446991: step 31260/40890 (epoch 23/30), loss = 0.098377 (0.872 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:58:27.214847: step 31280/40890 (epoch 23/30), loss = 0.140077 (0.824 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:58:42.912887: step 31300/40890 (epoch 23/30), loss = 0.324220 (0.790 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:58:59.017203: step 31320/40890 (epoch 23/30), loss = 0.042598 (0.841 sec/batch), lr: 0.590490\n",
      "2020-10-12 05:59:14.978038: step 31340/40890 (epoch 23/30), loss = 0.146758 (0.836 sec/batch), lr: 0.590490\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 65.927%\n",
      "   Recall (micro): 65.066%\n",
      "       F1 (micro): 65.494%\n",
      "epoch 23: train_loss = 0.227884, dev_loss = 0.477185, dev_f1 = 0.6549\n",
      "model saved to ./save_models/01/checkpoint_epoch_23.pt\n",
      "\n",
      "2020-10-12 06:00:03.991001: step 31360/40890 (epoch 24/30), loss = 0.137982 (0.946 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:00:19.780651: step 31380/40890 (epoch 24/30), loss = 0.194886 (0.664 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:00:35.637423: step 31400/40890 (epoch 24/30), loss = 0.125755 (0.795 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:00:50.990715: step 31420/40890 (epoch 24/30), loss = 0.144904 (0.838 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:01:07.449033: step 31440/40890 (epoch 24/30), loss = 0.158161 (0.843 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:01:22.957072: step 31460/40890 (epoch 24/30), loss = 0.268983 (0.652 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:01:38.660599: step 31480/40890 (epoch 24/30), loss = 0.400749 (0.660 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:01:54.589023: step 31500/40890 (epoch 24/30), loss = 0.221343 (0.915 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:02:10.463606: step 31520/40890 (epoch 24/30), loss = 0.313694 (0.704 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:02:26.889683: step 31540/40890 (epoch 24/30), loss = 0.161226 (0.830 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:02:43.083945: step 31560/40890 (epoch 24/30), loss = 0.164699 (0.885 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:02:59.275033: step 31580/40890 (epoch 24/30), loss = 0.324664 (0.920 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:03:15.599741: step 31600/40890 (epoch 24/30), loss = 0.155178 (0.908 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:03:32.176077: step 31620/40890 (epoch 24/30), loss = 0.235464 (0.855 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:03:48.510468: step 31640/40890 (epoch 24/30), loss = 0.191149 (0.843 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:04:04.626974: step 31660/40890 (epoch 24/30), loss = 0.126270 (0.848 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:04:23.000136: step 31680/40890 (epoch 24/30), loss = 0.157289 (0.858 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:04:39.282620: step 31700/40890 (epoch 24/30), loss = 0.312491 (0.882 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:04:56.010978: step 31720/40890 (epoch 24/30), loss = 0.225721 (0.852 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:05:11.821706: step 31740/40890 (epoch 24/30), loss = 0.158378 (0.783 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:05:27.738211: step 31760/40890 (epoch 24/30), loss = 0.232734 (0.692 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:05:43.964332: step 31780/40890 (epoch 24/30), loss = 0.193487 (0.904 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:06:00.686207: step 31800/40890 (epoch 24/30), loss = 0.344132 (0.793 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:06:17.222006: step 31820/40890 (epoch 24/30), loss = 0.437858 (0.671 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:06:33.596728: step 31840/40890 (epoch 24/30), loss = 0.231884 (0.927 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:06:49.884664: step 31860/40890 (epoch 24/30), loss = 0.246099 (0.801 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:07:06.387556: step 31880/40890 (epoch 24/30), loss = 0.366800 (0.893 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:07:23.035058: step 31900/40890 (epoch 24/30), loss = 0.136451 (0.862 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:07:39.218885: step 31920/40890 (epoch 24/30), loss = 0.087175 (0.771 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:07:55.343211: step 31940/40890 (epoch 24/30), loss = 0.242843 (0.961 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:08:10.482311: step 31960/40890 (epoch 24/30), loss = 0.321887 (0.839 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:08:26.103541: step 31980/40890 (epoch 24/30), loss = 0.090534 (0.747 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:08:41.726770: step 32000/40890 (epoch 24/30), loss = 0.155937 (0.792 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:08:57.479226: step 32020/40890 (epoch 24/30), loss = 0.361146 (0.658 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:09:13.750616: step 32040/40890 (epoch 24/30), loss = 0.174383 (0.774 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:09:30.011137: step 32060/40890 (epoch 24/30), loss = 0.147404 (0.859 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:09:46.440278: step 32080/40890 (epoch 24/30), loss = 0.328175 (0.805 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:10:02.517533: step 32100/40890 (epoch 24/30), loss = 0.198928 (0.640 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:10:18.248985: step 32120/40890 (epoch 24/30), loss = 0.178202 (0.950 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:10:34.607244: step 32140/40890 (epoch 24/30), loss = 0.221122 (0.881 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:10:51.070582: step 32160/40890 (epoch 24/30), loss = 0.380161 (0.843 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:11:06.730717: step 32180/40890 (epoch 24/30), loss = 0.162333 (0.589 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:11:23.348283: step 32200/40890 (epoch 24/30), loss = 0.073346 (0.777 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:11:38.250492: step 32220/40890 (epoch 24/30), loss = 0.149487 (0.565 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:11:54.564283: step 32240/40890 (epoch 24/30), loss = 0.148531 (0.634 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:12:10.992009: step 32260/40890 (epoch 24/30), loss = 0.161762 (0.856 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:12:26.666120: step 32280/40890 (epoch 24/30), loss = 0.232950 (0.667 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:12:43.373813: step 32300/40890 (epoch 24/30), loss = 0.326400 (0.835 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:13:00.190359: step 32320/40890 (epoch 24/30), loss = 0.248888 (0.855 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:13:16.578620: step 32340/40890 (epoch 24/30), loss = 0.149654 (0.894 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:13:32.986821: step 32360/40890 (epoch 24/30), loss = 0.184786 (0.794 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:13:49.587441: step 32380/40890 (epoch 24/30), loss = 0.166806 (0.935 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:14:05.439071: step 32400/40890 (epoch 24/30), loss = 0.239904 (0.935 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:14:22.596194: step 32420/40890 (epoch 24/30), loss = 0.198145 (0.948 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:14:38.925530: step 32440/40890 (epoch 24/30), loss = 0.078159 (0.687 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:14:55.584560: step 32460/40890 (epoch 24/30), loss = 0.218674 (0.876 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:15:12.191162: step 32480/40890 (epoch 24/30), loss = 0.244636 (0.751 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:15:28.103613: step 32500/40890 (epoch 24/30), loss = 0.279812 (0.681 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:15:43.830654: step 32520/40890 (epoch 24/30), loss = 0.333656 (0.849 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:16:00.055271: step 32540/40890 (epoch 24/30), loss = 0.304933 (0.914 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:16:16.643422: step 32560/40890 (epoch 24/30), loss = 0.188800 (0.680 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:16:33.262886: step 32580/40890 (epoch 24/30), loss = 0.280015 (0.660 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:16:50.071945: step 32600/40890 (epoch 24/30), loss = 0.140887 (0.846 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:17:05.301421: step 32620/40890 (epoch 24/30), loss = 0.243922 (0.616 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:17:21.602832: step 32640/40890 (epoch 24/30), loss = 0.291605 (0.810 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:17:37.422540: step 32660/40890 (epoch 24/30), loss = 0.176459 (0.764 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:17:53.554912: step 32680/40890 (epoch 24/30), loss = 0.407526 (0.816 sec/batch), lr: 0.531441\n",
      "2020-10-12 06:18:09.462376: step 32700/40890 (epoch 24/30), loss = 0.113589 (0.822 sec/batch), lr: 0.531441\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.928%\n",
      "   Recall (micro): 62.730%\n",
      "       F1 (micro): 64.761%\n",
      "epoch 24: train_loss = 0.221151, dev_loss = 0.486185, dev_f1 = 0.6476\n",
      "model saved to ./save_models/01/checkpoint_epoch_24.pt\n",
      "\n",
      "2020-10-12 06:18:58.832072: step 32720/40890 (epoch 25/30), loss = 0.274166 (0.650 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:19:14.489206: step 32740/40890 (epoch 25/30), loss = 0.162377 (0.860 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:19:30.343369: step 32760/40890 (epoch 25/30), loss = 0.271488 (0.889 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:19:45.447010: step 32780/40890 (epoch 25/30), loss = 0.444166 (0.634 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:20:01.915028: step 32800/40890 (epoch 25/30), loss = 0.217553 (0.916 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:20:17.744215: step 32820/40890 (epoch 25/30), loss = 0.143770 (0.778 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:20:33.253325: step 32840/40890 (epoch 25/30), loss = 0.293497 (0.859 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:20:49.346294: step 32860/40890 (epoch 25/30), loss = 0.163734 (0.805 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:21:05.176990: step 32880/40890 (epoch 25/30), loss = 0.204790 (0.729 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:21:21.376805: step 32900/40890 (epoch 25/30), loss = 0.142098 (0.644 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:21:37.383005: step 32920/40890 (epoch 25/30), loss = 0.217205 (0.852 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:21:53.035512: step 32940/40890 (epoch 25/30), loss = 0.302346 (0.849 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:22:09.215306: step 32960/40890 (epoch 25/30), loss = 0.297814 (0.860 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:22:25.148601: step 32980/40890 (epoch 25/30), loss = 0.059708 (0.887 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:22:41.048087: step 33000/40890 (epoch 25/30), loss = 0.266092 (0.889 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:22:57.197903: step 33020/40890 (epoch 25/30), loss = 0.280601 (0.867 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:23:13.223573: step 33040/40890 (epoch 25/30), loss = 0.124936 (0.738 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:23:29.229410: step 33060/40890 (epoch 25/30), loss = 0.110521 (0.729 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:23:46.004561: step 33080/40890 (epoch 25/30), loss = 0.182075 (0.925 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:24:02.138433: step 33100/40890 (epoch 25/30), loss = 0.092433 (0.721 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:24:19.084121: step 33120/40890 (epoch 25/30), loss = 0.128285 (1.021 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:24:35.265361: step 33140/40890 (epoch 25/30), loss = 0.102974 (0.797 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:24:52.292830: step 33160/40890 (epoch 25/30), loss = 0.147791 (0.785 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:25:08.927219: step 33180/40890 (epoch 25/30), loss = 0.195135 (0.663 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:25:25.332370: step 33200/40890 (epoch 25/30), loss = 0.342800 (0.842 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:25:41.536043: step 33220/40890 (epoch 25/30), loss = 0.223322 (0.806 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:25:58.000535: step 33240/40890 (epoch 25/30), loss = 0.231806 (0.763 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:26:14.856977: step 33260/40890 (epoch 25/30), loss = 0.134007 (0.652 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:26:31.104547: step 33280/40890 (epoch 25/30), loss = 0.664896 (0.609 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:26:47.358155: step 33300/40890 (epoch 25/30), loss = 0.152939 (0.762 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:27:03.031769: step 33320/40890 (epoch 25/30), loss = 0.300597 (0.669 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:27:19.174472: step 33340/40890 (epoch 25/30), loss = 0.262071 (0.763 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:27:35.085932: step 33360/40890 (epoch 25/30), loss = 0.182410 (0.717 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:27:50.981436: step 33380/40890 (epoch 25/30), loss = 0.310566 (0.754 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:28:07.351663: step 33400/40890 (epoch 25/30), loss = 0.286926 (0.980 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:28:23.646092: step 33420/40890 (epoch 25/30), loss = 0.211962 (0.829 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:28:40.079995: step 33440/40890 (epoch 25/30), loss = 0.180213 (0.811 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:28:56.288696: step 33460/40890 (epoch 25/30), loss = 0.375092 (0.748 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:29:11.692568: step 33480/40890 (epoch 25/30), loss = 0.244769 (0.558 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:29:28.251804: step 33500/40890 (epoch 25/30), loss = 0.345953 (0.857 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:29:44.748692: step 33520/40890 (epoch 25/30), loss = 0.246853 (0.698 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:30:00.313081: step 33540/40890 (epoch 25/30), loss = 0.327574 (0.845 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:30:17.055534: step 33560/40890 (epoch 25/30), loss = 0.248955 (0.818 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:30:32.341171: step 33580/40890 (epoch 25/30), loss = 0.170214 (0.842 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:30:48.195300: step 33600/40890 (epoch 25/30), loss = 0.287571 (0.870 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:31:04.485251: step 33620/40890 (epoch 25/30), loss = 0.156315 (0.823 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:31:20.381593: step 33640/40890 (epoch 25/30), loss = 0.241456 (0.821 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:31:36.518349: step 33660/40890 (epoch 25/30), loss = 0.527280 (0.854 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:31:52.995309: step 33680/40890 (epoch 25/30), loss = 0.155523 (0.873 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:32:08.713280: step 33700/40890 (epoch 25/30), loss = 0.162299 (0.594 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:32:24.726556: step 33720/40890 (epoch 25/30), loss = 0.374289 (0.679 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:32:41.092799: step 33740/40890 (epoch 25/30), loss = 0.240391 (0.882 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:32:56.455229: step 33760/40890 (epoch 25/30), loss = 0.216514 (0.843 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:33:12.168213: step 33780/40890 (epoch 25/30), loss = 0.206229 (0.673 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:33:28.737972: step 33800/40890 (epoch 25/30), loss = 0.277873 (0.816 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:33:44.787415: step 33820/40890 (epoch 25/30), loss = 0.149012 (0.683 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:34:01.677274: step 33840/40890 (epoch 25/30), loss = 0.338832 (1.037 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:34:19.665182: step 33860/40890 (epoch 25/30), loss = 0.293614 (0.865 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:34:35.765082: step 33880/40890 (epoch 25/30), loss = 0.183460 (0.668 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:34:51.984221: step 33900/40890 (epoch 25/30), loss = 0.229456 (0.859 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:35:08.709498: step 33920/40890 (epoch 25/30), loss = 0.212400 (0.668 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:35:25.683913: step 33940/40890 (epoch 25/30), loss = 0.051771 (0.849 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:35:42.136442: step 33960/40890 (epoch 25/30), loss = 0.163588 (0.877 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:35:57.877855: step 33980/40890 (epoch 25/30), loss = 0.294694 (0.775 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:36:13.928935: step 34000/40890 (epoch 25/30), loss = 0.339958 (0.922 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:36:29.689364: step 34020/40890 (epoch 25/30), loss = 0.109361 (0.723 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:36:46.214245: step 34040/40890 (epoch 25/30), loss = 0.148628 (0.864 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:37:02.436009: step 34060/40890 (epoch 25/30), loss = 0.200387 (0.649 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 68.042%\n",
      "   Recall (micro): 62.785%\n",
      "       F1 (micro): 65.308%\n",
      "epoch 25: train_loss = 0.216068, dev_loss = 0.477942, dev_f1 = 0.6531\n",
      "model saved to ./save_models/01/checkpoint_epoch_25.pt\n",
      "\n",
      "2020-10-12 06:37:52.560455: step 34080/40890 (epoch 26/30), loss = 0.168796 (0.844 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:38:08.432855: step 34100/40890 (epoch 26/30), loss = 0.247131 (0.781 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:38:24.068402: step 34120/40890 (epoch 26/30), loss = 0.121904 (0.878 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:38:39.414368: step 34140/40890 (epoch 26/30), loss = 0.168713 (0.712 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:38:55.693837: step 34160/40890 (epoch 26/30), loss = 0.188026 (0.702 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:39:11.412805: step 34180/40890 (epoch 26/30), loss = 0.261785 (0.850 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:39:27.077434: step 34200/40890 (epoch 26/30), loss = 0.202597 (0.858 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:39:42.922578: step 34220/40890 (epoch 26/30), loss = 0.286489 (0.832 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:39:58.935659: step 34240/40890 (epoch 26/30), loss = 0.155486 (0.710 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:40:15.806056: step 34260/40890 (epoch 26/30), loss = 0.409266 (0.821 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:40:32.024022: step 34280/40890 (epoch 26/30), loss = 0.175136 (0.922 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:40:47.794906: step 34300/40890 (epoch 26/30), loss = 0.273956 (0.880 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:41:03.714844: step 34320/40890 (epoch 26/30), loss = 0.160649 (0.680 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:41:19.475148: step 34340/40890 (epoch 26/30), loss = 0.090211 (0.764 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:41:35.161205: step 34360/40890 (epoch 26/30), loss = 0.402599 (0.749 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:41:51.159990: step 34380/40890 (epoch 26/30), loss = 0.151311 (0.678 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:42:06.766377: step 34400/40890 (epoch 26/30), loss = 0.235048 (0.710 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:42:22.362087: step 34420/40890 (epoch 26/30), loss = 0.165701 (0.700 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:42:38.797140: step 34440/40890 (epoch 26/30), loss = 0.137431 (0.756 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:42:54.879146: step 34460/40890 (epoch 26/30), loss = 0.046492 (0.910 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:43:10.329831: step 34480/40890 (epoch 26/30), loss = 0.197764 (0.762 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:43:26.322016: step 34500/40890 (epoch 26/30), loss = 0.044026 (0.723 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:43:43.270695: step 34520/40890 (epoch 26/30), loss = 0.464826 (0.848 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:44:00.012935: step 34540/40890 (epoch 26/30), loss = 0.109960 (0.863 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:44:17.570989: step 34560/40890 (epoch 26/30), loss = 0.253962 (0.979 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:44:34.264409: step 34580/40890 (epoch 26/30), loss = 0.134248 (0.891 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:44:50.640632: step 34600/40890 (epoch 26/30), loss = 0.411949 (0.637 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:45:07.653183: step 34620/40890 (epoch 26/30), loss = 0.191421 (0.919 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:45:23.896973: step 34640/40890 (epoch 26/30), loss = 0.097235 (0.919 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:45:40.120827: step 34660/40890 (epoch 26/30), loss = 0.371699 (0.731 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:45:56.057724: step 34680/40890 (epoch 26/30), loss = 0.333727 (0.855 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:46:12.156677: step 34700/40890 (epoch 26/30), loss = 0.193253 (0.869 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:46:28.003315: step 34720/40890 (epoch 26/30), loss = 0.206009 (0.795 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:46:44.078517: step 34740/40890 (epoch 26/30), loss = 0.181798 (0.819 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:47:00.423214: step 34760/40890 (epoch 26/30), loss = 0.458258 (0.736 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:47:17.353848: step 34780/40890 (epoch 26/30), loss = 0.204216 (0.891 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:47:34.164426: step 34800/40890 (epoch 26/30), loss = 0.324413 (0.794 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:47:50.586518: step 34820/40890 (epoch 26/30), loss = 0.277042 (0.832 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:48:06.127962: step 34840/40890 (epoch 26/30), loss = 0.255542 (0.796 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:48:22.068337: step 34860/40890 (epoch 26/30), loss = 0.235848 (0.669 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:48:38.788629: step 34880/40890 (epoch 26/30), loss = 0.199836 (0.720 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:48:54.397893: step 34900/40890 (epoch 26/30), loss = 0.172772 (0.754 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:49:10.861245: step 34920/40890 (epoch 26/30), loss = 0.275923 (0.862 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:49:26.191118: step 34940/40890 (epoch 26/30), loss = 0.325910 (0.718 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:49:42.030166: step 34960/40890 (epoch 26/30), loss = 0.340190 (0.839 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:49:58.206746: step 34980/40890 (epoch 26/30), loss = 0.180212 (0.908 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:50:14.498035: step 35000/40890 (epoch 26/30), loss = 0.059007 (0.731 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:50:30.214259: step 35020/40890 (epoch 26/30), loss = 0.325699 (0.833 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:50:46.751887: step 35040/40890 (epoch 26/30), loss = 0.284046 (0.840 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:51:02.711607: step 35060/40890 (epoch 26/30), loss = 0.316513 (0.690 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:51:18.473968: step 35080/40890 (epoch 26/30), loss = 0.333246 (0.807 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:51:34.652573: step 35100/40890 (epoch 26/30), loss = 0.120506 (0.799 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:51:49.709873: step 35120/40890 (epoch 26/30), loss = 0.235074 (0.791 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:52:05.151140: step 35140/40890 (epoch 26/30), loss = 0.457534 (0.760 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:52:21.220828: step 35160/40890 (epoch 26/30), loss = 0.282793 (0.726 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:52:36.947993: step 35180/40890 (epoch 26/30), loss = 0.283210 (0.862 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:52:53.673273: step 35200/40890 (epoch 26/30), loss = 0.229764 (0.821 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:53:09.806009: step 35220/40890 (epoch 26/30), loss = 0.148465 (0.745 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:53:25.480098: step 35240/40890 (epoch 26/30), loss = 0.375575 (0.707 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:53:41.493279: step 35260/40890 (epoch 26/30), loss = 0.385923 (0.792 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:53:57.925239: step 35280/40890 (epoch 26/30), loss = 0.130058 (0.773 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:54:15.921156: step 35300/40890 (epoch 26/30), loss = 0.302321 (1.071 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:54:33.282835: step 35320/40890 (epoch 26/30), loss = 0.288773 (0.848 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:54:49.610040: step 35340/40890 (epoch 26/30), loss = 0.250859 (0.847 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:55:05.385441: step 35360/40890 (epoch 26/30), loss = 0.237777 (0.863 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:55:21.387656: step 35380/40890 (epoch 26/30), loss = 0.139286 (0.835 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:55:37.948714: step 35400/40890 (epoch 26/30), loss = 0.194396 (0.879 sec/batch), lr: 0.478297\n",
      "2020-10-12 06:55:54.430659: step 35420/40890 (epoch 26/30), loss = 0.190072 (0.847 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.423%\n",
      "   Recall (micro): 63.576%\n",
      "       F1 (micro): 64.969%\n",
      "epoch 26: train_loss = 0.211499, dev_loss = 0.493388, dev_f1 = 0.6497\n",
      "model saved to ./save_models/01/checkpoint_epoch_26.pt\n",
      "\n",
      "2020-10-12 06:56:45.572777: step 35440/40890 (epoch 27/30), loss = 0.120967 (0.823 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:57:01.596443: step 35460/40890 (epoch 27/30), loss = 0.195876 (0.716 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:57:17.571726: step 35480/40890 (epoch 27/30), loss = 0.221800 (0.736 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:57:33.878128: step 35500/40890 (epoch 27/30), loss = 0.233841 (0.710 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:57:49.967035: step 35520/40890 (epoch 27/30), loss = 0.115667 (0.730 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:58:05.935905: step 35540/40890 (epoch 27/30), loss = 0.253853 (0.802 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:58:21.564626: step 35560/40890 (epoch 27/30), loss = 0.165856 (0.837 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:58:37.449658: step 35580/40890 (epoch 27/30), loss = 0.229318 (0.834 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:58:53.441903: step 35600/40890 (epoch 27/30), loss = 0.122348 (0.777 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:59:09.585301: step 35620/40890 (epoch 27/30), loss = 0.160499 (0.713 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:59:25.497913: step 35640/40890 (epoch 27/30), loss = 0.182462 (0.771 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:59:41.371896: step 35660/40890 (epoch 27/30), loss = 0.138664 (0.863 sec/batch), lr: 0.430467\n",
      "2020-10-12 06:59:57.235558: step 35680/40890 (epoch 27/30), loss = 0.179153 (0.847 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:00:13.170948: step 35700/40890 (epoch 27/30), loss = 0.137812 (0.731 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:00:28.810140: step 35720/40890 (epoch 27/30), loss = 0.254641 (0.700 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:00:44.861727: step 35740/40890 (epoch 27/30), loss = 0.133652 (0.790 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:01:00.577563: step 35760/40890 (epoch 27/30), loss = 0.222640 (0.897 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:01:16.160404: step 35780/40890 (epoch 27/30), loss = 0.138647 (0.816 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:01:32.288278: step 35800/40890 (epoch 27/30), loss = 0.224085 (0.869 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:01:47.913559: step 35820/40890 (epoch 27/30), loss = 0.184600 (0.747 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:02:03.462988: step 35840/40890 (epoch 27/30), loss = 0.145874 (0.796 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:02:18.939251: step 35860/40890 (epoch 27/30), loss = 0.244160 (0.754 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:02:35.202720: step 35880/40890 (epoch 27/30), loss = 0.132234 (0.904 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:02:51.977932: step 35900/40890 (epoch 27/30), loss = 0.228915 (0.899 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:03:08.222907: step 35920/40890 (epoch 27/30), loss = 0.139935 (0.760 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:03:24.429571: step 35940/40890 (epoch 27/30), loss = 0.345957 (0.779 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:03:41.034235: step 35960/40890 (epoch 27/30), loss = 0.111459 (0.921 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:03:57.962681: step 35980/40890 (epoch 27/30), loss = 0.286556 (0.872 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:04:15.725929: step 36000/40890 (epoch 27/30), loss = 0.201172 (0.988 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:04:32.931351: step 36020/40890 (epoch 27/30), loss = 0.136849 (0.752 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:04:48.753045: step 36040/40890 (epoch 27/30), loss = 0.184550 (0.702 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:05:04.784207: step 36060/40890 (epoch 27/30), loss = 0.103967 (0.755 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:05:21.024289: step 36080/40890 (epoch 27/30), loss = 0.313510 (0.778 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:05:37.031992: step 36100/40890 (epoch 27/30), loss = 0.047033 (0.854 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:05:53.327498: step 36120/40890 (epoch 27/30), loss = 0.481784 (0.695 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:06:10.153584: step 36140/40890 (epoch 27/30), loss = 0.307516 (0.754 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:06:27.083315: step 36160/40890 (epoch 27/30), loss = 0.290470 (0.967 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:06:43.674950: step 36180/40890 (epoch 27/30), loss = 0.178836 (0.893 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:06:59.551437: step 36200/40890 (epoch 27/30), loss = 0.225917 (0.684 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:07:16.075253: step 36220/40890 (epoch 27/30), loss = 0.105410 (0.925 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:07:33.024930: step 36240/40890 (epoch 27/30), loss = 0.248395 (0.820 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:07:48.811723: step 36260/40890 (epoch 27/30), loss = 0.344007 (0.657 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:08:05.179955: step 36280/40890 (epoch 27/30), loss = 0.116978 (0.907 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:08:21.022570: step 36300/40890 (epoch 27/30), loss = 0.195229 (0.766 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:08:36.479240: step 36320/40890 (epoch 27/30), loss = 0.240794 (0.664 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:08:52.582218: step 36340/40890 (epoch 27/30), loss = 0.221357 (0.814 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:09:09.008352: step 36360/40890 (epoch 27/30), loss = 0.081405 (0.737 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:09:24.700957: step 36380/40890 (epoch 27/30), loss = 0.056756 (0.871 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:09:41.433555: step 36400/40890 (epoch 27/30), loss = 0.176145 (0.922 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:09:57.417204: step 36420/40890 (epoch 27/30), loss = 0.308203 (0.841 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:10:13.231923: step 36440/40890 (epoch 27/30), loss = 0.217493 (0.884 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:10:29.139387: step 36460/40890 (epoch 27/30), loss = 0.050450 (0.806 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:10:44.341253: step 36480/40890 (epoch 27/30), loss = 0.156523 (0.619 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:10:59.667785: step 36500/40890 (epoch 27/30), loss = 0.451480 (0.754 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:11:15.716924: step 36520/40890 (epoch 27/30), loss = 0.097718 (0.769 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:11:31.315265: step 36540/40890 (epoch 27/30), loss = 0.124969 (0.770 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:11:47.625796: step 36560/40890 (epoch 27/30), loss = 0.264136 (0.832 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:12:03.662480: step 36580/40890 (epoch 27/30), loss = 0.297685 (0.841 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:12:19.028413: step 36600/40890 (epoch 27/30), loss = 0.088414 (0.915 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:12:34.386350: step 36620/40890 (epoch 27/30), loss = 0.250146 (0.874 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:12:50.528301: step 36640/40890 (epoch 27/30), loss = 0.189470 (0.855 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:13:07.309508: step 36660/40890 (epoch 27/30), loss = 0.169119 (0.871 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:13:24.055729: step 36680/40890 (epoch 27/30), loss = 0.076379 (0.902 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:13:40.296388: step 36700/40890 (epoch 27/30), loss = 0.280664 (0.880 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:13:55.962000: step 36720/40890 (epoch 27/30), loss = 0.214083 (0.823 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:14:13.093704: step 36740/40890 (epoch 27/30), loss = 0.069316 (0.778 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:14:30.052433: step 36760/40890 (epoch 27/30), loss = 0.134196 (0.896 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:14:46.717866: step 36780/40890 (epoch 27/30), loss = 0.187860 (0.873 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:15:03.270610: step 36800/40890 (epoch 27/30), loss = 0.253487 (0.698 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.623%\n",
      "   Recall (micro): 62.896%\n",
      "       F1 (micro): 65.173%\n",
      "epoch 27: train_loss = 0.208247, dev_loss = 0.488223, dev_f1 = 0.6517\n",
      "model saved to ./save_models/01/checkpoint_epoch_27.pt\n",
      "\n",
      "2020-10-12 07:15:54.208937: step 36820/40890 (epoch 28/30), loss = 0.195112 (0.831 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:16:09.993865: step 36840/40890 (epoch 28/30), loss = 0.166858 (0.735 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:16:26.219994: step 36860/40890 (epoch 28/30), loss = 0.265493 (0.800 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:16:42.631736: step 36880/40890 (epoch 28/30), loss = 0.209605 (0.821 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:16:58.979023: step 36900/40890 (epoch 28/30), loss = 0.106452 (0.885 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:17:14.866570: step 36920/40890 (epoch 28/30), loss = 0.288447 (0.777 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:17:30.965041: step 36940/40890 (epoch 28/30), loss = 0.302572 (0.729 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:17:47.134314: step 36960/40890 (epoch 28/30), loss = 0.166922 (0.838 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:18:03.642847: step 36980/40890 (epoch 28/30), loss = 0.082671 (0.830 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:18:19.710882: step 37000/40890 (epoch 28/30), loss = 0.321667 (0.690 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:18:35.615192: step 37020/40890 (epoch 28/30), loss = 0.318645 (0.760 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:18:51.362988: step 37040/40890 (epoch 28/30), loss = 0.157298 (0.871 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:19:07.241549: step 37060/40890 (epoch 28/30), loss = 0.198433 (0.875 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:19:23.083697: step 37080/40890 (epoch 28/30), loss = 0.165462 (0.833 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:19:38.763168: step 37100/40890 (epoch 28/30), loss = 0.294711 (0.622 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:19:54.394371: step 37120/40890 (epoch 28/30), loss = 0.318837 (0.893 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:20:10.295362: step 37140/40890 (epoch 28/30), loss = 0.182530 (0.660 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:20:26.331489: step 37160/40890 (epoch 28/30), loss = 0.164499 (0.935 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:20:42.028411: step 37180/40890 (epoch 28/30), loss = 0.315747 (0.894 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:20:57.538453: step 37200/40890 (epoch 28/30), loss = 0.134621 (0.826 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:21:13.138246: step 37220/40890 (epoch 28/30), loss = 0.127382 (0.880 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:21:29.250273: step 37240/40890 (epoch 28/30), loss = 0.196333 (0.825 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:21:45.262479: step 37260/40890 (epoch 28/30), loss = 0.143921 (0.731 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:22:01.498585: step 37280/40890 (epoch 28/30), loss = 0.238272 (0.858 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:22:17.199665: step 37300/40890 (epoch 28/30), loss = 0.100274 (0.788 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:22:33.363783: step 37320/40890 (epoch 28/30), loss = 0.160167 (0.842 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:22:49.992729: step 37340/40890 (epoch 28/30), loss = 0.277152 (0.776 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:23:06.257238: step 37360/40890 (epoch 28/30), loss = 0.275948 (0.890 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:23:22.786060: step 37380/40890 (epoch 28/30), loss = 0.245875 (0.852 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:23:38.479093: step 37400/40890 (epoch 28/30), loss = 0.106647 (0.770 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:23:54.231971: step 37420/40890 (epoch 28/30), loss = 0.232774 (0.874 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:24:11.352702: step 37440/40890 (epoch 28/30), loss = 0.273487 (0.915 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:24:28.184201: step 37460/40890 (epoch 28/30), loss = 0.191182 (0.798 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:24:44.647714: step 37480/40890 (epoch 28/30), loss = 0.203839 (0.852 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:25:01.454846: step 37500/40890 (epoch 28/30), loss = 0.177649 (0.821 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:25:18.328173: step 37520/40890 (epoch 28/30), loss = 0.234996 (0.908 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:25:35.106309: step 37540/40890 (epoch 28/30), loss = 0.131560 (0.950 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:25:50.960915: step 37560/40890 (epoch 28/30), loss = 0.079124 (0.735 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:26:07.147554: step 37580/40890 (epoch 28/30), loss = 0.137377 (0.681 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:26:24.396432: step 37600/40890 (epoch 28/30), loss = 0.198643 (0.956 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:26:40.481428: step 37620/40890 (epoch 28/30), loss = 0.231139 (0.753 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:26:57.071588: step 37640/40890 (epoch 28/30), loss = 0.229906 (0.888 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:27:13.577059: step 37660/40890 (epoch 28/30), loss = 0.388602 (0.693 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:27:29.558406: step 37680/40890 (epoch 28/30), loss = 0.237050 (0.837 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:27:45.821275: step 37700/40890 (epoch 28/30), loss = 0.146772 (0.664 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:28:02.195491: step 37720/40890 (epoch 28/30), loss = 0.125776 (0.797 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:28:18.106978: step 37740/40890 (epoch 28/30), loss = 0.269142 (0.881 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:28:34.751490: step 37760/40890 (epoch 28/30), loss = 0.178123 (0.764 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:28:50.925752: step 37780/40890 (epoch 28/30), loss = 0.106953 (0.847 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:29:06.509083: step 37800/40890 (epoch 28/30), loss = 0.233162 (0.787 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:29:22.536232: step 37820/40890 (epoch 28/30), loss = 0.031229 (0.760 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:29:38.222521: step 37840/40890 (epoch 28/30), loss = 0.096459 (0.731 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:29:53.390031: step 37860/40890 (epoch 28/30), loss = 0.304658 (0.727 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:30:09.180379: step 37880/40890 (epoch 28/30), loss = 0.229501 (0.851 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:30:24.881401: step 37900/40890 (epoch 28/30), loss = 0.220129 (0.816 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:30:41.149900: step 37920/40890 (epoch 28/30), loss = 0.165683 (0.858 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:30:57.091274: step 37940/40890 (epoch 28/30), loss = 0.103333 (0.898 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:31:12.332846: step 37960/40890 (epoch 28/30), loss = 0.243134 (0.790 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:31:27.644903: step 37980/40890 (epoch 28/30), loss = 0.124042 (0.813 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:31:43.564336: step 38000/40890 (epoch 28/30), loss = 0.164948 (0.754 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:31:59.782877: step 38020/40890 (epoch 28/30), loss = 0.196784 (0.850 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:32:16.015098: step 38040/40890 (epoch 28/30), loss = 0.207252 (0.887 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:32:32.092109: step 38060/40890 (epoch 28/30), loss = 0.164732 (0.792 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:32:47.549157: step 38080/40890 (epoch 28/30), loss = 0.210869 (0.715 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:33:04.025100: step 38100/40890 (epoch 28/30), loss = 0.095473 (0.882 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:33:19.804906: step 38120/40890 (epoch 28/30), loss = 0.326100 (0.833 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:33:36.656924: step 38140/40890 (epoch 28/30), loss = 0.127991 (0.650 sec/batch), lr: 0.430467\n",
      "2020-10-12 07:33:53.475086: step 38160/40890 (epoch 28/30), loss = 0.258118 (0.843 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.336%\n",
      "   Recall (micro): 64.018%\n",
      "       F1 (micro): 65.156%\n",
      "epoch 28: train_loss = 0.204725, dev_loss = 0.494777, dev_f1 = 0.6516\n",
      "model saved to ./save_models/01/checkpoint_epoch_28.pt\n",
      "\n",
      "2020-10-12 07:34:47.485809: step 38180/40890 (epoch 29/30), loss = 0.161669 (0.744 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:35:03.525435: step 38200/40890 (epoch 29/30), loss = 0.263939 (0.655 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:35:20.113080: step 38220/40890 (epoch 29/30), loss = 0.060085 (0.767 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:35:36.271928: step 38240/40890 (epoch 29/30), loss = 0.350721 (0.893 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:35:52.803241: step 38260/40890 (epoch 29/30), loss = 0.078607 (0.663 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:36:08.745188: step 38280/40890 (epoch 29/30), loss = 0.203397 (0.710 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:36:25.096473: step 38300/40890 (epoch 29/30), loss = 0.185780 (0.873 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:36:41.454731: step 38320/40890 (epoch 29/30), loss = 0.236911 (0.806 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:36:58.112778: step 38340/40890 (epoch 29/30), loss = 0.221282 (0.930 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:37:14.654544: step 38360/40890 (epoch 29/30), loss = 0.084289 (0.918 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:37:30.610397: step 38380/40890 (epoch 29/30), loss = 0.142017 (0.746 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:37:46.295967: step 38400/40890 (epoch 29/30), loss = 0.351090 (0.824 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:38:04.808730: step 38420/40890 (epoch 29/30), loss = 0.183235 (1.053 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:38:21.278757: step 38440/40890 (epoch 29/30), loss = 0.174313 (0.683 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:38:37.397912: step 38460/40890 (epoch 29/30), loss = 0.282734 (0.804 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:38:52.597798: step 38480/40890 (epoch 29/30), loss = 0.169892 (0.678 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:39:08.645172: step 38500/40890 (epoch 29/30), loss = 0.118170 (0.631 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:39:24.494818: step 38520/40890 (epoch 29/30), loss = 0.232779 (0.810 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:39:40.149047: step 38540/40890 (epoch 29/30), loss = 0.147913 (0.851 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:39:55.781307: step 38560/40890 (epoch 29/30), loss = 0.229425 (0.789 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:40:11.270319: step 38580/40890 (epoch 29/30), loss = 0.090599 (0.820 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:40:27.777607: step 38600/40890 (epoch 29/30), loss = 0.162364 (0.977 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:40:44.210677: step 38620/40890 (epoch 29/30), loss = 0.092532 (0.694 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:41:00.242323: step 38640/40890 (epoch 29/30), loss = 0.178089 (0.861 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:41:16.148790: step 38660/40890 (epoch 29/30), loss = 0.110534 (0.744 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:41:32.126575: step 38680/40890 (epoch 29/30), loss = 0.118843 (0.763 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:41:48.359774: step 38700/40890 (epoch 29/30), loss = 0.283780 (0.851 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:42:04.275139: step 38720/40890 (epoch 29/30), loss = 0.356396 (0.860 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:42:20.249480: step 38740/40890 (epoch 29/30), loss = 0.261883 (0.699 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:42:35.801954: step 38760/40890 (epoch 29/30), loss = 0.275135 (0.697 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:42:51.278570: step 38780/40890 (epoch 29/30), loss = 0.131386 (0.796 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:43:07.241829: step 38800/40890 (epoch 29/30), loss = 0.158656 (0.780 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:43:22.685661: step 38820/40890 (epoch 29/30), loss = 0.372533 (0.630 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:43:38.559574: step 38840/40890 (epoch 29/30), loss = 0.174579 (0.785 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:43:56.142021: step 38860/40890 (epoch 29/30), loss = 0.243781 (0.921 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:44:14.514894: step 38880/40890 (epoch 29/30), loss = 0.299924 (1.045 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:44:32.400660: step 38900/40890 (epoch 29/30), loss = 0.175262 (0.711 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:44:48.290351: step 38920/40890 (epoch 29/30), loss = 0.411997 (0.681 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:45:04.243693: step 38940/40890 (epoch 29/30), loss = 0.135343 (0.809 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:45:20.636859: step 38960/40890 (epoch 29/30), loss = 0.205645 (0.777 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:45:36.990245: step 38980/40890 (epoch 29/30), loss = 0.221184 (0.876 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:45:52.594968: step 39000/40890 (epoch 29/30), loss = 0.215162 (0.693 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:46:09.170668: step 39020/40890 (epoch 29/30), loss = 0.155951 (0.893 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:46:24.390028: step 39040/40890 (epoch 29/30), loss = 0.242744 (0.912 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:46:40.423511: step 39060/40890 (epoch 29/30), loss = 0.129035 (0.753 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:46:56.659119: step 39080/40890 (epoch 29/30), loss = 0.229132 (0.630 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:47:12.352156: step 39100/40890 (epoch 29/30), loss = 0.103979 (0.859 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:47:28.777237: step 39120/40890 (epoch 29/30), loss = 0.109151 (0.757 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:47:44.835814: step 39140/40890 (epoch 29/30), loss = 0.144215 (0.828 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:48:00.521382: step 39160/40890 (epoch 29/30), loss = 0.264031 (0.729 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:48:16.543545: step 39180/40890 (epoch 29/30), loss = 0.080660 (0.788 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:48:32.454437: step 39200/40890 (epoch 29/30), loss = 0.155449 (0.618 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:48:47.497731: step 39220/40890 (epoch 29/30), loss = 0.174153 (0.720 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:49:03.189771: step 39240/40890 (epoch 29/30), loss = 0.198573 (0.891 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:49:19.146612: step 39260/40890 (epoch 29/30), loss = 0.154077 (0.850 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:49:35.087001: step 39280/40890 (epoch 29/30), loss = 0.120374 (0.862 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:49:51.203909: step 39300/40890 (epoch 29/30), loss = 0.149044 (0.807 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:50:06.613066: step 39320/40890 (epoch 29/30), loss = 0.158921 (0.758 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:50:21.613472: step 39340/40890 (epoch 29/30), loss = 0.288554 (0.683 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:50:37.888803: step 39360/40890 (epoch 29/30), loss = 0.108437 (0.797 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:50:53.832185: step 39380/40890 (epoch 29/30), loss = 0.152572 (0.747 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:51:10.148071: step 39400/40890 (epoch 29/30), loss = 0.241860 (0.733 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:51:26.149285: step 39420/40890 (epoch 29/30), loss = 0.145157 (0.712 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:51:41.304229: step 39440/40890 (epoch 29/30), loss = 0.064505 (0.875 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:51:57.178888: step 39460/40890 (epoch 29/30), loss = 0.324797 (0.856 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:52:12.692405: step 39480/40890 (epoch 29/30), loss = 0.075295 (0.625 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:52:28.819796: step 39500/40890 (epoch 29/30), loss = 0.240347 (0.773 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:52:44.778125: step 39520/40890 (epoch 29/30), loss = 0.153934 (0.812 sec/batch), lr: 0.387420\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 67.155%\n",
      "   Recall (micro): 64.054%\n",
      "       F1 (micro): 65.568%\n",
      "epoch 29: train_loss = 0.197965, dev_loss = 0.503728, dev_f1 = 0.6557\n",
      "model saved to ./save_models/01/checkpoint_epoch_29.pt\n",
      "\n",
      "2020-10-12 07:53:33.904842: step 39540/40890 (epoch 30/30), loss = 0.234421 (0.815 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:53:49.577933: step 39560/40890 (epoch 30/30), loss = 0.234584 (0.886 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:54:05.646067: step 39580/40890 (epoch 30/30), loss = 0.105171 (0.868 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:54:22.175875: step 39600/40890 (epoch 30/30), loss = 0.182043 (0.820 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:54:38.563452: step 39620/40890 (epoch 30/30), loss = 0.180041 (0.827 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:54:54.098422: step 39640/40890 (epoch 30/30), loss = 0.149794 (0.771 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:55:09.883277: step 39660/40890 (epoch 30/30), loss = 0.235959 (0.805 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:55:25.551387: step 39680/40890 (epoch 30/30), loss = 0.346403 (0.640 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:55:41.594996: step 39700/40890 (epoch 30/30), loss = 0.077707 (0.786 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:55:57.783761: step 39720/40890 (epoch 30/30), loss = 0.307426 (0.661 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:56:14.054767: step 39740/40890 (epoch 30/30), loss = 0.049424 (0.894 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:56:29.557318: step 39760/40890 (epoch 30/30), loss = 0.073535 (0.604 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:56:45.411923: step 39780/40890 (epoch 30/30), loss = 0.065887 (0.859 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:57:01.486939: step 39800/40890 (epoch 30/30), loss = 0.131298 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:57:17.391376: step 39820/40890 (epoch 30/30), loss = 0.184131 (0.878 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:57:32.629139: step 39840/40890 (epoch 30/30), loss = 0.351793 (0.681 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:57:48.424902: step 39860/40890 (epoch 30/30), loss = 0.309818 (0.810 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:58:04.489879: step 39880/40890 (epoch 30/30), loss = 0.102567 (0.869 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:58:20.194451: step 39900/40890 (epoch 30/30), loss = 0.218126 (0.668 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:58:35.751855: step 39920/40890 (epoch 30/30), loss = 0.195153 (0.672 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:58:51.425452: step 39940/40890 (epoch 30/30), loss = 0.155668 (0.801 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:59:07.397799: step 39960/40890 (epoch 30/30), loss = 0.197912 (0.875 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:59:23.385050: step 39980/40890 (epoch 30/30), loss = 0.109068 (0.729 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:59:39.481010: step 40000/40890 (epoch 30/30), loss = 0.167653 (0.665 sec/batch), lr: 0.387420\n",
      "2020-10-12 07:59:55.442330: step 40020/40890 (epoch 30/30), loss = 0.376503 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:00:11.526679: step 40040/40890 (epoch 30/30), loss = 0.292653 (0.902 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:00:27.540864: step 40060/40890 (epoch 30/30), loss = 0.272642 (0.751 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:00:43.757072: step 40080/40890 (epoch 30/30), loss = 0.206517 (0.696 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:00:59.695454: step 40100/40890 (epoch 30/30), loss = 0.217263 (0.840 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:01:15.761555: step 40120/40890 (epoch 30/30), loss = 0.439112 (0.878 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:01:31.734843: step 40140/40890 (epoch 30/30), loss = 0.375285 (0.694 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:01:47.469373: step 40160/40890 (epoch 30/30), loss = 0.175022 (0.708 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:02:03.157476: step 40180/40890 (epoch 30/30), loss = 0.195922 (0.890 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:02:19.207564: step 40200/40890 (epoch 30/30), loss = 0.138585 (0.892 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:02:35.564879: step 40220/40890 (epoch 30/30), loss = 0.191896 (0.740 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:02:51.783911: step 40240/40890 (epoch 30/30), loss = 0.157033 (0.796 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:03:08.372554: step 40260/40890 (epoch 30/30), loss = 0.251481 (0.895 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:03:24.300540: step 40280/40890 (epoch 30/30), loss = 0.324670 (0.800 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:03:40.065386: step 40300/40890 (epoch 30/30), loss = 0.218760 (0.768 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:03:56.490648: step 40320/40890 (epoch 30/30), loss = 0.231144 (0.930 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:04:13.899099: step 40340/40890 (epoch 30/30), loss = 0.142021 (0.799 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:04:30.177080: step 40360/40890 (epoch 30/30), loss = 0.289879 (0.762 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:04:46.557301: step 40380/40890 (epoch 30/30), loss = 0.328868 (0.694 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:05:01.770131: step 40400/40890 (epoch 30/30), loss = 0.174571 (0.796 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:05:18.057089: step 40420/40890 (epoch 30/30), loss = 0.149796 (0.784 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:05:34.428375: step 40440/40890 (epoch 30/30), loss = 0.100003 (0.811 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:05:50.221146: step 40460/40890 (epoch 30/30), loss = 0.192080 (0.787 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:06:06.663181: step 40480/40890 (epoch 30/30), loss = 0.241744 (0.749 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:06:22.770135: step 40500/40890 (epoch 30/30), loss = 0.222045 (0.664 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:06:38.765803: step 40520/40890 (epoch 30/30), loss = 0.179539 (0.716 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:06:54.813796: step 40540/40890 (epoch 30/30), loss = 0.063818 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:07:10.698978: step 40560/40890 (epoch 30/30), loss = 0.118470 (0.578 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:07:25.921274: step 40580/40890 (epoch 30/30), loss = 0.036610 (0.838 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:07:41.159114: step 40600/40890 (epoch 30/30), loss = 0.184808 (0.781 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:07:57.290549: step 40620/40890 (epoch 30/30), loss = 0.171014 (0.909 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:08:13.056898: step 40640/40890 (epoch 30/30), loss = 0.205006 (0.739 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:08:29.330254: step 40660/40890 (epoch 30/30), loss = 0.317865 (0.726 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:08:44.860734: step 40680/40890 (epoch 30/30), loss = 0.145877 (0.711 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:08:59.993278: step 40700/40890 (epoch 30/30), loss = 0.222067 (0.852 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:09:16.018794: step 40720/40890 (epoch 30/30), loss = 0.279867 (0.845 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:09:32.210498: step 40740/40890 (epoch 30/30), loss = 0.180584 (0.862 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:09:48.482877: step 40760/40890 (epoch 30/30), loss = 0.100215 (0.902 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:10:04.698517: step 40780/40890 (epoch 30/30), loss = 0.165326 (0.659 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:10:19.647884: step 40800/40890 (epoch 30/30), loss = 0.181349 (0.844 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:10:35.430754: step 40820/40890 (epoch 30/30), loss = 0.317851 (0.684 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:10:51.112820: step 40840/40890 (epoch 30/30), loss = 0.107242 (0.854 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:11:07.063178: step 40860/40890 (epoch 30/30), loss = 0.098303 (0.870 sec/batch), lr: 0.387420\n",
      "2020-10-12 08:11:22.970582: step 40880/40890 (epoch 30/30), loss = 0.293793 (0.876 sec/batch), lr: 0.387420\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 66.245%\n",
      "   Recall (micro): 63.650%\n",
      "       F1 (micro): 64.922%\n",
      "epoch 30: train_loss = 0.193845, dev_loss = 0.503002, dev_f1 = 0.6492\n",
      "model saved to ./save_models/01/checkpoint_epoch_30.pt\n",
      "\n",
      "Training ended with 30 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Train model using Position Aware\n",
    "train.train_model(vocab_params, training_params, train_batch, dev_batch, model_id='00')\n",
    "\n",
    "# Train model using LSTM\n",
    "train.train_model(vocab_params, training_params, train_batch, dev_batch, model_id='01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from C:\\prabhu\\edu\\code\\w266\f",
      "inal_project\\save_modelsest.model.pt\n",
      "[ Fail: model loading failed. ]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dump' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c39d876ff584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\prabhu\\edu\\code\\w266\\final_project\\save_models\\00\\best.model.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading model from {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# dump = torch.load(model_file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\prabhu\\edu\\code\\w266\\final_project\\utils\\torch_utils.py\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[ Fail: model loading failed. ]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'dump' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Evaluating the Model\n",
    "# eval.evaluate_model(eval_params)\n",
    "from utils import torch_utils\n",
    "import torch\n",
    "\n",
    "# model_file = eval_params.model_dir + '\\' + eval_params.model\n",
    "model_file = 'C:\\prabhu\\edu\\code\\w266\\final_project\\save_models\\00\\best.model.pt'\n",
    "print(\"Loading model from {}\".format(model_file))\n",
    "opt = torch_utils.load_config(model_file)\n",
    "# dump = torch.load(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "embedded null character",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d468a1d67f09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\prabhu\\edu\\code\\w266\\final_project\\save_models\\00\\best.model.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: embedded null character"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dump = torch.load(\"C:\\prabhu\\edu\\code\\w266\\final_project\\save_models\\00\\best.model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
