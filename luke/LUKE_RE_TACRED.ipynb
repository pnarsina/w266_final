{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import WEIGHTS_NAME\n",
    "\n",
    "from utils.entity_vocab import MASK_TOKEN\n",
    "\n",
    "from exp_utils import set_seed\n",
    "from exp_utils.trainer import Trainer, trainer_args\n",
    "from re_model import LukeForRelationClassification\n",
    "from re_utils import HEAD_TOKEN, TAIL_TOKEN, convert_examples_to_features, DatasetProcessor\n",
    "from transformers.tokenization_roberta import RobertaTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForPreTraining,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from luke.model import LukeConfig\n",
    "from luke.optimization import LukeAdamW\n",
    "from luke.pretraining.batch_generator import LukePretrainingBatchGenerator, MultilingualBatchGenerator\n",
    "from luke.pretraining.dataset import WikipediaPretrainingDataset\n",
    "from luke.pretraining.model import LukePretrainingModel\n",
    "from luke.utils.model_utils import ENTITY_VOCAB_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "metadata_folder = \"luke_model/\"\n",
    "\n",
    "class obj(object):\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "with open(os.path.join(metadata_folder, \"metadata.json\")) as f:\n",
    "    model_config = obj(json.load(f)[\"model_config\"])\n",
    "\n",
    "print(model_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params:\n",
    "    def __init__(self, model_config):\n",
    "        self.data_dir = \"data/tacred/json\"\n",
    "        self.do_train = \"--no-train\"\n",
    "        self.train_batch_size = 4\n",
    "        self.num_train_epochs = 5.0\n",
    "        self.do_val = \"--no-eval\"\n",
    "        self.eval_batch_size = 128\n",
    "        self.seed = 42\n",
    "        self.bert_model_name = \"roberta-large\"\n",
    "        self.max_mention_length = 30\n",
    "        self.local_rank = -1\n",
    "        self.tokenizer =  RobertaTokenizer.from_pretrained(self.bert_model_name)\n",
    "        self.model_config = model_config\n",
    "        self.model_weights = {\"embeddings.word_embeddings.weight\":0.25, \"entity_embeddings.entity_embeddings.weight\":0.25}\n",
    "        self.adam_b1 =  0.9\n",
    "        self.adam_b2 =  0.999\n",
    "        self.adam_eps =  1e-06\n",
    "        self.batch_size =  2048\n",
    "        self.dataset_dir =  \"enwiki_20181220_dataset_500k_roberta_cand30_unk\"\n",
    "        self.entity_emb_size =  256\n",
    "#         self.fix_bert_weights =  False\n",
    "        self.fp16 =  False\n",
    "        self.fp16_master_weights =  True\n",
    "        self.fp16_max_loss_scale =  4\n",
    "        self.fp16_min_loss_scale =  1\n",
    "        self.fp16_opt_level =  \"O2\"\n",
    "#         self.global_step =  96454\n",
    "#         self.grad_avg_on_cpu =  False\n",
    "        self.gradient_accumulation_steps =  64\n",
    "        self.learning_rate =  1e-05\n",
    "#         self.local_rank =  0\n",
    "#         self.log_dir =  \"log_mon/roberta_large_luke7_500k_nonoise_mlm0.15_ment0.15_emb256_b2048_lrate1e-5_warm2500_epoch20_pre20\"\n",
    "        self.lr_schedule =  \"warmup_linear\"\n",
    "        self.masked_entity_prob =  0.15\n",
    "        self.masked_lm_prob =  0.15\n",
    "        self.max_grad_norm =  0.0\n",
    "#         self.model_file =  \"out_mon/roberta_large_luke7_500k_nonoise_mlm0.15_ment0.15_emb256_b2048_lrate1e-5_warm2500_epoch20_pre20/model_step0096454.bin\"\n",
    "#         self.num_epochs =  20\n",
    "        self.optimizer_file =  \"out_mon/roberta_large_luke7_500k_nonoise_mlm0.15_ment0.15_emb256_b2048_lrate1e-5_warm2500_epoch20_pre20/optimizer_step0096454.bin\"\n",
    "        self.output_dir =  \"out_mon/roberta_large_luke7_500k_nonoise_mlm0.15_ment0.15_emb256_b2048_lrate1e-5_warm2500_epoch20_pre20\"\n",
    "#         self.parallel =  True\n",
    "#         self.save_interval_sec =  1600\n",
    "#         self.scheduler_file =  \"out_mon/roberta_large_luke7_500k_nonoise_mlm0.15_ment0.15_emb256_b2048_lrate1e-5_warm2500_epoch20_pre20/scheduler_step0096454.bin\"\n",
    "        self.warmup_steps =  2500\n",
    "        self.weight_decay =  0.01\n",
    "#         self.whole_word_masking =  True\n",
    "# BASED ON GUSS from Here\n",
    "        self.adam_correct_bias = 0  #based on guess\n",
    "        self.warmup_proportion = 0.025 #based on guess\n",
    "        self.device = \"cpu\" #cuda - didn't work\n",
    "#         self.tokenizer = {\"max_len\": 512, \"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"sep_token\": \"</s>\", \"pad_token\": \"<pad>\", \"cls_token\": \"<s>\", \"mask_token\": \"<mask>\", \"init_inputs\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params(model_config)\n",
    "args.tokenizer.pad_token_id\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, fold=\"train\"):\n",
    "\n",
    "    processor = DatasetProcessor()\n",
    "    if fold == \"train\":\n",
    "        examples = processor.get_train_examples(args.data_dir)\n",
    "    elif fold == \"dev\":\n",
    "        examples = processor.get_dev_examples(args.data_dir)\n",
    "    else:\n",
    "        examples = processor.get_test_examples(args.data_dir)\n",
    "\n",
    "    label_list = processor.get_label_list(args.data_dir)\n",
    "\n",
    "    bert_model_name = args.bert_model_name\n",
    "\n",
    "    cache_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_\" + \"_\".join((args.bert_model_name.split(\"-\")[0], str(args.max_mention_length), fold)) + \".pkl\",\n",
    "    )\n",
    "    print('cache_file', cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cache_file)\n",
    "        tokens = [] # This is is only used for testing and is available when we are calculating first time. It is not available when we read from the cache.\n",
    "        features = torch.load(cache_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file\")\n",
    "        features, tokens = convert_examples_to_features(examples, label_list, args.tokenizer, args.max_mention_length)\n",
    "\n",
    "        if args.local_rank in (-1, 0):\n",
    "            torch.save(features, cache_file)\n",
    "\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        def create_padded_sequence(attr_name, padding_value):\n",
    "            tensors = [torch.tensor(getattr(o, attr_name), dtype=torch.long) for o in batch]\n",
    "            return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "        return dict(\n",
    "            word_ids=create_padded_sequence(\"word_ids\", args.tokenizer.pad_token_id),\n",
    "            word_attention_mask=create_padded_sequence(\"word_attention_mask\", 0),\n",
    "            word_segment_ids=create_padded_sequence(\"word_segment_ids\", 0),\n",
    "            entity_ids=create_padded_sequence(\"entity_ids\", 0),\n",
    "            entity_attention_mask=create_padded_sequence(\"entity_attention_mask\", 0),\n",
    "            entity_position_ids=create_padded_sequence(\"entity_position_ids\", -1),\n",
    "            entity_segment_ids=create_padded_sequence(\"entity_segment_ids\", 0),\n",
    "            label=torch.tensor([o.label for o in batch], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    if fold in (\"dev\", \"test\"):\n",
    "        dataloader = DataLoader(features, batch_size=args.eval_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    else:\n",
    "        if args.local_rank == -1:\n",
    "            sampler = RandomSampler(features)\n",
    "        else:\n",
    "            sampler = DistributedSampler(features)\n",
    "        dataloader = DataLoader(features, sampler=sampler, batch_size=args.train_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader, examples, features, tokens, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.model_config.vocab_size += 2\n",
    "# word_emb = args.model_weights[\"embeddings.word_embeddings.weight\"]\n",
    "# head_emb = word_emb[args.tokenizer.convert_tokens_to_ids([\"@\"])[0]].unsqueeze(0)\n",
    "# tail_emb = word_emb[args.tokenizer.convert_tokens_to_ids([\"#\"])[0]].unsqueeze(0)\n",
    "# args.model_weights[\"embeddings.word_embeddings.weight\"] = torch.cat([word_emb, head_emb, tail_emb])\n",
    "# args.tokenizer.add_special_tokens(dict(additional_special_tokens=[HEAD_TOKEN, TAIL_TOKEN]))\n",
    "\n",
    "# entity_emb = args.model_weights[\"entity_embeddings.entity_embeddings.weight\"]\n",
    "# mask_emb = entity_emb[args.entity_vocab[MASK_TOKEN]].unsqueeze(0).expand(2, -1)\n",
    "# args.model_config.entity_vocab_size = 3\n",
    "# args.model_weights[\"entity_embeddings.entity_embeddings.weight\"] = torch.cat([entity_emb[:1], mask_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 75/68124 [00:00<01:31, 744.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_file data/tacred/json\\cached_roberta_30_train.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 68124/68124 [00:27<00:00, 2459.50it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader, examples, features, tokens, label_list = load_and_cache_examples(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader, list, list, list)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader), type(examples), type(features), type(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68124,), (68124,), (42,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(examples).shape, np.array(features).shape, np.array(label_list).shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_relation', 'org:alternate_names', 'org:city_of_headquarters', 'org:country_of_headquarters', 'org:dissolved', 'org:founded', 'org:founded_by', 'org:member_of', 'org:members', 'org:number_of_employees/members', 'org:parents', 'org:political/religious_affiliation', 'org:shareholders', 'org:stateorprovince_of_headquarters', 'org:subsidiaries', 'org:top_members/employees', 'org:website', 'per:age', 'per:alternate_names', 'per:cause_of_death']\n"
     ]
    }
   ],
   "source": [
    "print(label_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 77] [0, 12] ORGANIZATION PERSON org:founded_by\n",
      "[35, 44] [95, 103] PERSON PERSON no_relation\n",
      "[137, 141] [36, 49] ORGANIZATION ORGANIZATION no_relation\n",
      "[306, 313] [155, 167] ORGANIZATION NUMBER no_relation\n",
      "[128, 159] [72, 78] ORGANIZATION DATE no_relation\n"
     ]
    }
   ],
   "source": [
    "from re_utils import InputExample\n",
    "for a in examples[0:5]:\n",
    "    print ( a.span_a, a.span_b, a.type_a, a.type_b, a.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠRes\n",
      "istant\n",
      "Ġ:\n",
      "Ġ100\n",
      "m\n",
      "/\n",
      "330\n",
      "ft\n",
      "ĠCrystal\n",
      "Ġ:\n",
      "ĠSc\n",
      "ratch\n",
      "ĠRes\n",
      "istant\n",
      "ĠSapphire\n",
      "ĠOur\n",
      "ĠPrice\n",
      "Ġ:\n",
      "Ġhttp\n",
      "://\n",
      "www\n",
      ".\n",
      "wh\n",
      "oles\n",
      "ale\n",
      "-\n",
      "w\n",
      "atches\n",
      ".\n",
      "org\n",
      "/\n",
      "w\n",
      "rist\n",
      "watch\n",
      "-\n",
      "251\n",
      ".\n",
      "html\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "for b in tokens[100:150]:\n",
    "    print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 1560, 2032, 873, 1728, 1437, 3, 6490, 11, 779, 94, 76, 7, 1026, 5, 1437, 3, 404, 7093, 6157, 139, 9127, 1437, 3, 111, 574, 26770, 12, 3943, 111, 25733, 387, 12, 2156, 6724, 5, 1929, 19, 601, 453, 9, 3589, 2156, 3735, 6100, 20303, 1745, 40702, 324, 6395, 7, 30887, 3589, 8, 486, 5, 6788, 729, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[17, 18, 19, 20, 21, 22, 23, 24, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 6\n",
      "[0, 96, 13668, 2156, 10, 76, 71, 5, 2669, 2156, 1437, 3, 18095, 2865, 1437, 3, 829, 5, 98, 12, 4155, 45518, 16333, 2354, 12801, 31, 5, 1437, 3, 610, 211, 4, 1437, 3, 8, 10530, 255, 4, 31357, 2475, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [28, 29, 30, 31, 32, 33, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 152, 21, 566, 10, 14398, 9, 42979, 1437, 3, 9238, 623, 1437, 3, 128, 29, 37885, 14, 38, 21, 576, 25, 10, 7970, 13, 2600, 8, 18240, 15, 10, 37048, 13, 1437, 3, 384, 9673, 1437, 3, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[33, 34, 35, 36, 37, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [9, 10, 11, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 20, 665, 803, 21, 8672, 71, 5, 2124, 837, 11, 3010, 303, 211, 3376, 8, 63, 3787, 2156, 2488, 4150, 27374, 2156, 2181, 9, 2183, 211, 3376, 128, 29, 111, 574, 26770, 12, 10353, 673, 111, 25733, 387, 12, 1437, 3, 13442, 153, 1437, 3, 111, 574, 26770, 12, 172, 68, 8325, 153, 111, 25733, 387, 12, 1968, 11, 274, 219, 3145, 293, 71, 4150, 27374, 480, 67, 10, 274, 219, 3145, 293, 736, 23, 5, 86, 480, 829, 1025, 335, 59, 1099, 1437, 3, 274, 219, 3145, 293, 1437, 3, 340, 11, 5, 4116, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[85, 86, 87, 88, 89, 90, 91, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [41, 42, 43, 44, 45, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 20, 515, 16, 10, 1263, 7, 10, 735, 446, 2447, 3114, 2570, 11, 1437, 3, 494, 1437, 3, 2156, 26, 20204, 12, 14447, 102, 3523, 2156, 1031, 736, 9, 5, 1437, 3, 188, 469, 10294, 10410, 1437, 3, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[32, 33, 34, 35, 36, 37, 38, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [15, 16, 17, 18, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 9967, 21, 2654, 1269, 11, 9633, 2156, 8, 373, 10, 6788, 1727, 11, 7969, 61, 37, 685, 7, 5, 1437, 3, 19219, 1437, 3, 71, 5, 537, 2867, 41, 7169, 9804, 19, 5, 1437, 3, 496, 6035, 13, 35411, 1437, 3, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[21, 22, 23, 24, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [35, 36, 37, 38, 39, 40, 41, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 3, 10248, 381, 7188, 329, 3733, 118, 12, 24567, 11516, 1437, 3, 111, 574, 26770, 12, 784, 111, 25733, 387, 12, 8, 1437, 3, 289, 4917, 625, 12, 9167, 523, 3, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[24, 25, 26, 27, 28, 29, 30, 31, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 726, 12, 23730, 4113, 3592, 2002, 14, 2156, 444, 31, 145, 10, 29133, 2156, 4896, 30143, 857, 417, 895, 21, 2149, 13, 45518, 2806, 82, 8, 5200, 5, 2205, 2156, 12801, 14, 1437, 3, 37, 1437, 3, 683, 2162, 123, 1504, 3308, 2156, 8, 14, 37, 21, 5, 313, 37, 439, 7, 77, 1437, 3, 37, 1437, 3, 956, 10, 5010, 12373, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[55, 56, 57, 58, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [34, 35, 36, 37, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 0\n",
      "[0, 3, 11924, 4677, 254, 4992, 1437, 3, 21, 546, 182, 11371, 15371, 368, 9106, 23, 94, 186, 128, 29, 370, 10690, 370, 7348, 85, 4832, 18149, 12, 43195, 35294, 12, 33295, 17444, 1040, 1709, 2156, 98, 24, 128, 29, 129, 12365, 79, 2075, 88, 588, 12, 5367, 1437, 3, 272, 35748, 9103, 1437, 3, 579, 1063, 20684, 1464, 8939, 8, 9185, 274, 354, 7841, 102, 15, 69, 169, 89, 27785, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [49, 50, 51, 52, 53, 54, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 30\n",
      "[0, 125, 382, 8, 1362, 2320, 224, 24, 34, 40591, 7, 185, 814, 136, 1437, 3, 25164, 8722, 12, 242, 12, 38495, 15577, 1437, 3, 2156, 61, 839, 45518, 20, 1437, 3, 2938, 9, 5, 19769, 1437, 3, 2156, 12801, 13294, 14, 5, 2715, 5496, 115, 3364, 5616, 11, 28271, 63, 3575, 3429, 666, 479, 2] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] [1, 2] [[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [31, 32, 33, 34, 35, 36, 37, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]] [0, 0] [1, 1] 1\n"
     ]
    }
   ],
   "source": [
    "for b in features[0:10]:\n",
    "    print ( b.word_ids,\n",
    "         b.word_segment_ids,\n",
    "         b.word_attention_mask,\n",
    "         b.entity_ids,\n",
    "         b.entity_position_ids,\n",
    "         b.entity_segment_ids,\n",
    "         b.entity_attention_mask,\n",
    "         b.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.tokenizer.convert_tokens_to_ids([\"@\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_list)\n",
    "# model_weights= [0.34,0.33,0.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from luke.pretraining.model import LukePretrainingModel\n",
    "# path = \"saved_model/luke.bin\"\n",
    "# model = LukePretrainingModel.from_pretrained(path, cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/pretrain_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " encoder.layer.0.attention.self.query.weight   tensor([[-0.0029,  0.0352,  0.0007,  ...,  0.0023,  0.0595, -0.0426],\n",
      "        [-0.0248,  0.0529, -0.0145,  ..., -0.0303, -0.0143,  0.0116],\n",
      "        [ 0.0061,  0.0708, -0.0336,  ...,  0.0807,  0.0115, -0.0131],\n",
      "        ...,\n",
      "        [-0.0589,  0.0206, -0.0426,  ..., -0.0298,  0.0041,  0.0700],\n",
      "        [ 0.0421,  0.0225, -0.0608,  ..., -0.0552, -0.0157,  0.0173],\n",
      "        [-0.0184, -0.0457, -0.0103,  ...,  0.0474,  0.0225, -0.0182]]) \n",
      "\n",
      "2 \n",
      " encoder.layer.0.attention.self.query.bias   tensor([ 0.3121,  0.0556, -0.0751,  ..., -0.0704, -0.0500, -0.0664]) \n",
      "\n",
      "3 \n",
      " encoder.layer.0.attention.self.key.weight   tensor([[-0.0043, -0.0184, -0.0136,  ..., -0.0037,  0.0096, -0.0156],\n",
      "        [-0.0238, -0.0002,  0.0253,  ...,  0.0403,  0.0436, -0.0195],\n",
      "        [-0.0264, -0.0522, -0.0125,  ..., -0.0359,  0.0077,  0.0150],\n",
      "        ...,\n",
      "        [-0.0718, -0.0261, -0.0203,  ..., -0.0186,  0.0097,  0.1023],\n",
      "        [ 0.0157,  0.0065, -0.0171,  ..., -0.0038, -0.0090,  0.0435],\n",
      "        [-0.0091, -0.0628,  0.0415,  ...,  0.0466,  0.0149, -0.0468]]) \n",
      "\n",
      "4 \n",
      " encoder.layer.0.attention.self.key.bias   tensor([-0.0041, -0.0033, -0.0012,  ...,  0.0013,  0.0017,  0.0018]) \n",
      "\n",
      "5 \n",
      " encoder.layer.0.attention.self.value.weight   tensor([[ 0.0306, -0.0006, -0.0241,  ..., -0.0187,  0.0017,  0.0217],\n",
      "        [ 0.0565,  0.0432,  0.0015,  ..., -0.0156,  0.0920, -0.0204],\n",
      "        [-0.0151, -0.0429,  0.0127,  ..., -0.0512,  0.0012,  0.0675],\n",
      "        ...,\n",
      "        [-0.0101,  0.0082, -0.0115,  ...,  0.0363,  0.0256,  0.0110],\n",
      "        [-0.0032, -0.0139, -0.0513,  ...,  0.0377, -0.0338,  0.0291],\n",
      "        [ 0.0047, -0.0094, -0.0135,  ..., -0.0247,  0.0876, -0.0179]]) \n",
      "\n",
      "6 \n",
      " encoder.layer.0.attention.self.value.bias   tensor([-0.0014,  0.0024, -0.0083,  ..., -0.0226, -0.0209, -0.0350]) \n",
      "\n",
      "7 \n",
      " encoder.layer.0.attention.output.dense.weight   tensor([[ 0.0015,  0.0398, -0.0170,  ..., -0.0140, -0.0325, -0.0164],\n",
      "        [-0.0346,  0.0133, -0.0157,  ...,  0.0311, -0.0096,  0.0268],\n",
      "        [ 0.0271, -0.0743,  0.0193,  ..., -0.0323, -0.0068,  0.0988],\n",
      "        ...,\n",
      "        [ 0.0285,  0.0010,  0.0225,  ...,  0.0128, -0.0104, -0.0361],\n",
      "        [-0.0081,  0.0514, -0.0476,  ...,  0.0426,  0.0263, -0.0157],\n",
      "        [ 0.0468,  0.0232,  0.0929,  ...,  0.0274,  0.0088,  0.0074]]) \n",
      "\n",
      "8 \n",
      " encoder.layer.0.attention.output.dense.bias   tensor([-0.0135,  0.0296,  0.0857,  ...,  0.0735, -0.0072,  0.0103]) \n",
      "\n",
      "9 \n",
      " encoder.layer.0.attention.output.LayerNorm.weight   tensor([0.9795, 0.9904, 0.9729,  ..., 0.9834, 0.9905, 0.9973]) \n",
      "\n",
      "10 \n",
      " encoder.layer.0.attention.output.LayerNorm.bias   tensor([-0.4307,  0.2765, -0.0064,  ...,  0.0114,  0.3294, -0.2977]) \n",
      "\n",
      "11 \n",
      " encoder.layer.0.intermediate.dense.weight   tensor([[ 0.0584, -0.0646, -0.0934,  ...,  0.0051,  0.0196, -0.0153],\n",
      "        [ 0.0162, -0.0273,  0.0207,  ...,  0.0155, -0.0379,  0.1215],\n",
      "        [ 0.0367, -0.0660, -0.0006,  ...,  0.0293, -0.0267, -0.0219],\n",
      "        ...,\n",
      "        [ 0.0165, -0.0888,  0.0040,  ...,  0.0331, -0.0512, -0.0009],\n",
      "        [ 0.1342,  0.0519, -0.1299,  ..., -0.1486, -0.0311,  0.0218],\n",
      "        [ 0.0719, -0.0296, -0.0604,  ...,  0.0095, -0.0576, -0.0292]]) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d0829df1d511>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print (i, \"\\n\", key, \" \", torch.tensor(value),  \"\\n\")\n"
     ]
    }
   ],
   "source": [
    "# modelA = TheModelAClass(*args, **kwargs)\n",
    "model = torch.load(\"saved_model/luke.bin\",map_location=torch.device('cpu'))\n",
    "type(model)\n",
    "# model = torch.load(\"C:/prabhu/edu/code/w266/Luke/model/luke_20200528.tar\")\n",
    "# writer.add_graph(model)\n",
    "# writer.close()\n",
    "i=0\n",
    "for key, value in model.items():\n",
    "    i+=1\n",
    "    print (i, \"\\n\", key, \" \", torch.tensor(value),  \"\\n\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code works for loading the model from saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from luke.pretraining.model import EntityPredictionHeadTransform, LukePretrainingModel,EntityPredictionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert configuration {'return_dict': False, 'output_hidden_states': False, 'output_attentions': False, 'use_cache': True, 'torchscript': False, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'architectures': ['RobertaForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'decoder_start_token_id': None, 'task_specific_params': None, 'xla_device': None, 'model_type': 'roberta', 'vocab_size': 50265, 'hidden_size': 1024, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'hidden_act': 'gelu', 'intermediate_size': 4096, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 514, 'type_vocab_size': 1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'gradient_checkpointing': False}\n"
     ]
    }
   ],
   "source": [
    "path = \"saved_model/luke.bin\"\n",
    "\n",
    "\n",
    "bert_model_name = \"roberta-large\"\n",
    "\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
    "\n",
    "print(\"bert configuration\", bert_config.to_dict())\n",
    "\n",
    "config = LukeConfig(\n",
    "    entity_vocab_size=500000,\n",
    "    bert_model_name=bert_model_name,\n",
    "    entity_emb_size=256,\n",
    "    **bert_config.to_dict(),\n",
    ")\n",
    "model = LukePretrainingModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LukePretrainingModel(\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (entity_embeddings): EntityEmbeddings(\n",
       "    (entity_embeddings): Embedding(500000, 256, padding_idx=0)\n",
       "    (entity_embedding_dense): Linear(in_features=256, out_features=1024, bias=False)\n",
       "    (position_embeddings): Embedding(514, 1024)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=50265, bias=True)\n",
       "  )\n",
       "  (entity_predictions): EntityPredictionHead(\n",
       "    (transform): EntityPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=256, out_features=500000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "from torch import nn\n",
    "\n",
    "# model.to(device='cpu')\n",
    "# model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(path), strict=False)\n",
    "# model.load_state_dict(torch.load(path), device='cpu')\n",
    "# writer.add_graph(model)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to use trainer class -- didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_accumulation_steps = 64\n",
    "# num_train_steps_per_epoch = len(dataloader) // gradient_accumulation_steps\n",
    "# num_train_steps = int(num_train_steps_per_epoch * args.num_train_epochs)\n",
    "\n",
    "# print('train steps per epoch: ', num_train_steps_per_epoch ,  '  train steps: ', num_train_steps  )\n",
    "\n",
    "# best_dev_f1 = [-1]\n",
    "# best_weights = [None]\n",
    "\n",
    "# def step_callback(model, global_step):\n",
    "#     print('global  step: ', global_step )\n",
    "#     if global_step % num_train_steps_per_epoch == 0 and args.local_rank in (0, -1):\n",
    "#         epoch = int(global_step / num_train_steps_per_epoch - 1)\n",
    "#         dev_results = evaluate(args, model, fold=\"dev\")\n",
    "#         args.experiment.log_metrics({f\"dev_{k}_epoch{epoch}\": v for k, v in dev_results.items()}, epoch=epoch)\n",
    "#         results.update({f\"dev_{k}_epoch{epoch}\": v for k, v in dev_results.items()})\n",
    "#         tqdm.write(\"dev: \" + str(dev_results))\n",
    "\n",
    "#         if dev_results[\"f1\"] > best_dev_f1[0]:\n",
    "#             if hasattr(model, \"module\"):\n",
    "#                 best_weights[0] = {k: v.to(\"cpu\").clone() for k, v in model.module.state_dict().items()}\n",
    "#             else:\n",
    "#                 best_weights[0] = {k: v.to(\"cpu\").clone() for k, v in model.state_dict().items()}\n",
    "#             best_dev_f1[0] = dev_results[\"f1\"]\n",
    "#             results[\"best_epoch\"] = epoch\n",
    "\n",
    "#         model.train()\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     args, model=model, dataloader=dataloader, num_train_steps=num_train_steps, step_callback=step_callback\n",
    "# )\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring more on Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create optimizer.  Seperating LayerNorm weight and bias from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import WEIGHTS_NAME, AdamW, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "print( np.array(optimizer_parameters).shape)\n",
    "optimizer= AdamW(\n",
    "            optimizer_parameters,\n",
    "            lr= args.learning_rate,\n",
    "            eps= args.adam_eps,\n",
    "            betas=( args.adam_b1, args.adam_b2),\n",
    "            correct_bias=args.adam_correct_bias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: 0\n",
       "    eps: 1e-06\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0.01\n",
       "\n",
       "Parameter Group 1\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: 0\n",
       "    eps: 1e-06\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = 10 # temporarilty assigned some value, need to find right value\n",
    "warmup_steps = int(num_train_steps * args.warmup_proportion)\n",
    "if args.lr_schedule == \"warmup_linear\":\n",
    "    scheduler= get_linear_schedule_with_warmup(optimizer, warmup_steps, num_train_steps)\n",
    "if args.lr_schedule == \"warmup_constant\":\n",
    "    scheduler= get_constant_schedule_with_warmup(optimizer, warmup_steps)\n",
    "else:\n",
    "    RuntimeError(\"Unsupported scheduler: \" + args.lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x24390ab6610>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel data loader - Ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.parallel.DistributedDataParallel(\n",
    "#     model,\n",
    "#     device_ids=[args.local_rank],\n",
    "#     output_device=args.local_rank,\n",
    "#     find_unused_parameters=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_model_arguments( batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'word_ids': tensor([[    0,     3,  5991,   344,   324,  1437,     3,     8,  1437,     3,\n",
      "         43154,  8316,  3181,   493,  1437,     3,  1008,    11,     5,   200,\n",
      "           457,   296,     7,   483,   436,     7,    10,   132,    12,   288,\n",
      "           339,    81,   188,  3324,  2156,  2749,    62,    10,   297,  6156,\n",
      "          8430,   136,  8683,    15,   395,   479,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,    20,  5226,    34,    57,     5,   652,     9,  1437,     3,\n",
      "          3943,  1437,     3,   128,    29,  1953,    31,  1437,     3,   666,\n",
      "          1437,     3,     8,  1752,  2156,  8165,  4595,    15,     5,  1546,\n",
      "           128,    29,  9852,  2308,  1230,   340,   479,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 23784,     9, 38225, 15668,     8, 33424,  1635,    58, 14937,\n",
      "           198,    23,     5,  1172,     8,  1557, 14759,    23,  1437,     3,\n",
      "          6130,   660,  5317,   119,  1821,  1437,     3,    11,  1437,     3,\n",
      "          2361,  1437,     3,  2156,   234,     4,   725,     4,  2156,    15,\n",
      "           378,   363,   479,     2,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,  3687,     5,   275,    12,  6421,  7887,     7,   218,     5,\n",
      "         24680,    32,  1437,     3,   163,     4,   863,     4,  1811,   783,\n",
      "          8365,  1437,     3,  2156,     5,  1437,     3, 12221,   248,  4636,\n",
      "           139,  6446,  1544,  1437,     3,   128,    29,  3503,   232,  8347,\n",
      "            12,   338,  8231,  2234,  2156,     8,   344,     4,   387,     4,\n",
      "          3066,  4438,   219,  2156,     5, 12221,  6518, 23967,   128,  3503,\n",
      "          4534,     9,     5,    76,   479,     2]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 1,  2,  3,  4,  5,  6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[ 9, 10, 11, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [18, 19, 20, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[19, 20, 21, 22, 23, 24, 25, 26, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [29, 30, 31, 32, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[26, 27, 28, 29, 30, 31, 32, 33, 34, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([ 0,  3,  0, 15])}\n",
      "1 {'word_ids': tensor([[    0, 17374, 29082,  3504,  1437,     3,  4323,   212,   279,  8003,\n",
      "          1943,  1437,     3,  1602,     5,  1288,    25,  2105,   576,     5,\n",
      "         45518,  9644,  1202,  1068, 12801,    11,  1437,     3,  1600,  1437,\n",
      "             3,   128,    29,   866,   479,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  2497,  2949,    31,  1437,     3,  2431,  7596,  7889,  1437,\n",
      "             3,   111,  1437,     3,    65,  1437,     3,     9,     5, 16501,\n",
      "           720,  8232, 37378,   479,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  1944,  2642, 17233,   680, 44309,   877,  4832,  6623, 12214,\n",
      "          1405, 18370,   225,   359, 12355,  5576,   312,   368,   459,   254,\n",
      "           381, 22237,   225,   111,   574, 26770,    12,  1063,  4488,  3577,\n",
      "          2156,  7065,  1794,   242, 27461,   268,   111, 25733,   387,    12,\n",
      "          2156,  1811, 16717, 45518,  3864,    12, 33543, 12801,  1437,     3,\n",
      "          1259,  1437,     3,   111,   574, 26770,    12, 17667,  4057,  2156,\n",
      "          5583,  8187,   111, 25733,   387,    12,  2156,  7543, 13621,  4832,\n",
      "         11335,  3577,   359,   255,  3194,   242,  6753,   337,  3019,   111,\n",
      "           574, 26770,    12,  9185, 23510,   329,  6082,  2156,  1437,     3,\n",
      "         22607, 23905,  1437,     3,   111, 25733,   387,    12,  2156,  5293,\n",
      "          3178,   248,  6502,  1758,   111,   574, 26770,    12,  4856, 10987,\n",
      "         10690,  1634,  2156, 15990,  5366,   811,   111, 25733,   387,    12,\n",
      "          2156, 16732,  9925,  2865,   359, 13621, 14891,   111,   574, 26770,\n",
      "            12, 43579,   226,  5229,  2156, 18518,   381, 11760,   111, 25733,\n",
      "           387,    12,     8,    96,   102,   305,  1001,  4779,  7305,   111,\n",
      "           574, 26770,    12,  8904,   324,  3635, 10149,  2156,  6599,  2497,\n",
      "          5593,   111, 25733,   387,    12,   479,     2],\n",
      "        [    0,   252,   202,    33,   899,     7,     5, 24526, 29252,    11,\n",
      "          1287,  1422,     8,   258,    33,  1382,    15,  7585,     9,  7707,\n",
      "          4130, 21405,  2156,    10,  2015,   651,    14,  7235,    15,  4320,\n",
      "          3787, 12865,    91,   506,  1396,     8, 34333, 11254,  6370,  2156,\n",
      "          1437,     3,  2265, 20356, 14803,  1120,    90,  1437,     3,     8,\n",
      "          1437,     3, 20611,   763, 23992,  1437,     3,   479,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 5,  6,  7,  8,  9, 10, 11, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [27, 28, 29, 30, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[ 5,  6,  7,  8,  9, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [13, 14, 15, 16, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[89, 90, 91, 92, 93, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [49, 50, 51, 52, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[51, 52, 53, 54, 55, 56, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [41, 42, 43, 44, 45, 46, 47, 48, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([0, 0, 0, 0])}\n",
      "2 {'word_ids': tensor([[    0,     3,  1464, 30922,  1437,     3,    26,  1437,     3, 19150,\n",
      "           808,  2495,  1437,     3,   128,    29,   766,    16,  1887,     7,\n",
      "         35334,   561,  1825,   111,   574, 26770,    12, 45518,  1029, 12801,\n",
      "           111, 25733,   387,    12,     8,     5,  5862,  2136,    13,   301,\n",
      "          2156,   748,  3119,   479,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,   163,  9993,    12,   698,  1922, 15274,    12,  5525,   182,\n",
      "          6003,  4979,  1864,  4832,  9163, 13907,   240,    13, 12920,    38,\n",
      "          1400,    19,  1437,     3, 14627,    96,  6342,  2088, 10081,  1437,\n",
      "             3,   111,   574, 26770,    12,  1437,     3,   384,  2371,  1437,\n",
      "             3,   111, 25733,   387,    12,  5799,  2156,  5533,   164,    15,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 45518,   287,  2255,  1799,  2156, 10066,  2081,     8,  4601,\n",
      "           227,     5,   256,  4454,   510,     8,     5, 12416,  1643,     9,\n",
      "           436,   111,   574, 26770,    12, 37389,   111, 25733,   387,    12,\n",
      "            16,  1233,    13, 11606,     5,  9526,  3405,   227,     5,  1437,\n",
      "             3,    80,  1437,     3,   749,  2156, 12801,  1437,     3,    37,\n",
      "          1437,     3,   355,   479,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0,    85,    16,   132,   107,    92,     8,    34,    70,     9,\n",
      "             5, 16201,     9,  1207,    11,     5,  9869,  1437,     3,  1995,\n",
      "          6920,  1437,     3,   435,  2156,   364,     4,   571,     4,  7358,\n",
      "          3716,  2156,   593,     7,   538, 15316,  6560,  2156,   290,  2940,\n",
      "          2156, 34288,  2156, 37326,  2156,   624,   112,  7245,    31,  1995,\n",
      "          6920, 11984,  2156,   593,     7,   171,   451,   111,  6869,  2156,\n",
      "         15968,  2156,  1437,     3,  3797,  1437,     3,  2156, 24298, 26188,\n",
      "          2156, 15307,  4086,  2156, 14424,  2156,  8714,  2156, 15297,  2156,\n",
      "         10354,  2156,  8397,   175,   479,     2]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 8,  9, 10, 11, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 1,  2,  3,  4,  5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[36, 37, 38, 39, 40, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [23, 24, 25, 26, 27, 28, 29, 30, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[48, 49, 50, 51, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [40, 41, 42, 43, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[63, 64, 65, 66, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [18, 19, 20, 21, 22, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([0, 1, 0, 0])}\n",
      "3 {'word_ids': tensor([[    0,     3, 13667,   272,  6343,  1437,     3,  2156,    10,   320,\n",
      "          1172,  1437,     3, 16736,  1437,     3,    31,  8570,   413,  2156,\n",
      "         20520,  2156,    54, 24177,  3039,  4685,     8,  2942,    10,  1087,\n",
      "             7,  8415,     5, 12819, 21192,     8,  2042, 19710,     8,   146,\n",
      "            24,    10,   632,  2221,  2156,   962,   502,   262,     9, 29367,\n",
      "          2088,  1144,  2988,    23,   208,  4748,   219,  4604,  2392,    11,\n",
      "           663,  5815,   479,     2,     1,     1,     1,     1],\n",
      "        [    0, 12701, 19724,   128,    29,   458,     9,  3890,  9973,    16,\n",
      "             5,  1152,     9,    10,  4482,   432,    11,    61,    24, 21379,\n",
      "            63,  2384,     8,   822,  1781,   480,  1437,     3,  9973, 11863,\n",
      "          1437,     3,     8,  6129,  6237,   217,  1437,     3,  2805,  3658,\n",
      "          1437,     3,  2156, 19337,     8, 22640,    12,  9474,   480,     8,\n",
      "            63,  4782,  6768,  2156,    19, 10307,   128,    29,  3890,     8,\n",
      "             5,  6129,  4836, 17826,     8, 20054,   479,     2],\n",
      "        [    0,   616,     5,  1437,     3,   496,  6586,  5264,  1785,  1437,\n",
      "             3,  3447,    14,  1762,     9,   681,    11,    10,  5964,  6559,\n",
      "          1153,  1726,     5,   668,  2156,     5,   792,    26,     5,  1437,\n",
      "             3,  1853,  5512, 29276,  5264,  4237,  1437,     3,    21,  2149,\n",
      "            25,   157,   142,    24,    34,   626,    10,  2129,   633,   442,\n",
      "           686,  2353,   451,    32,  1522,   479,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 45518,   370,    40,    45,    28, 33389,   142,    84,   872,\n",
      "            64,    45,    28,   110,  2364,  2156,  5988,    87,    24,    74,\n",
      "            28,    84,  2364,     7,   342,  1437,     3, 12401,  1631,  1437,\n",
      "             3,    50,     5,  7616,  3043,    11,    65,     9,    84,  1139,\n",
      "         32657,  2156, 12801,    26,  1738,   525,  1250,   254,  2156,    10,\n",
      "          3097,    54,  8147,    13,     5,  7837,     9,     5,  1437,     3,\n",
      "         10164,  2475,  1437,     3,   479,     2,     1,     1]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 1,  2,  3,  4,  5,  6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[37, 38, 39, 40, 41, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [27, 28, 29, 30, 31, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[30, 31, 32, 33, 34, 35, 36, 37, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 4,  5,  6,  7,  8,  9, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[59, 60, 61, 62, 63, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [26, 27, 28, 29, 30, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([41,  0,  0,  0])}\n",
      "4 {'word_ids': tensor([[    0,  1190,  4461,  2156,  1437,     3,  2169,   104, 30445,  1437,\n",
      "             3,  1565,  2156,    26,  1437,     3, 18548,   254,  1437,     3,\n",
      "             8,  8519,    33,    57,   964,    13,   291,   107,   479,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 45518,    20,   340,    66,     9,  4109,    34,   300,   961,\n",
      "           259, 30204,    49,  3885,  2156, 12801,    26,  1206,  5469, 11294,\n",
      "          2156,   736,     9,     5,  1437,     3,   188, 10372,  2534,     9,\n",
      "         16226,  1437,     3,    23,  1437,     3,   312,     4,   660,  5317,\n",
      "           119,  1821,  1437,     3,   479,     2],\n",
      "        [    0,   286,   103,   107,  2052,     7,     5,  1437,     3, 14873,\n",
      "          1437,     3, 34883,     9,    39,   641,  2156,  2657,  1006,     7,\n",
      "           120,  1134,   751,     5,  1437,     3, 33614,  1437,     3,     7,\n",
      "           185,    41,  2171,   774,    11,    49,   637,   136, 45518,  2677,\n",
      "          2990,  4270,   479, 12801,     2,     1],\n",
      "        [    0,    20,  2225,    34,   610,  6973, 12818,  1003,     7,    80,\n",
      "            97,  3395,  1437,     3,  6416, 15892,  1253,  1437,     3,    54,\n",
      "            58,   684,    13,  4003,    12,  7755, 24103,  4128,  4832,  3713,\n",
      "         23917,  1250,     8,  1437,     3,  7600,  2610,  5412,  1437,     3,\n",
      "           479,     2,     1,     1,     1,     1]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 5,  6,  7,  8,  9, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [15, 16, 17, 18, 19, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[35, 36, 37, 38, 39, 40, 41, 42, 43, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [25, 26, 27, 28, 29, 30, 31, 32, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[25, 26, 27, 28, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 8,  9, 10, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[34, 35, 36, 37, 38, 39, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [13, 14, 15, 16, 17, 18, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([ 0, 14,  0,  0])}\n",
      "5 {'word_ids': tensor([[    0,    20,   641,     9,  1659,    26,    24,    74,   295,    75,\n",
      "          1539,    14,   432,  2156,    53,    24,   222,  2870,    10,  2672,\n",
      "          1437,     3,   302,  1437,     3,     7,  1803,   344,  3297,    31,\n",
      "          2159,     5,  1226,   128,    29,   887,    12,  8377,  6829, 10655,\n",
      "          2156,  1437,     3,   496, 26888,   221,  7361,   944,     4,  1437,\n",
      "             3,     9,  3110,   412,  2156,  4630,   479,     2,     1],\n",
      "        [    0,    20,  1515,  4114,  1437,     3,  1063,  4937,  1437,     3,\n",
      "            26,    15,    63,  6494,  1082,   294,    14,  1515, 14913, 12557,\n",
      "         20989,  1437,     3, 16704,    12,  1000, 37041, 20716,  1178,  1437,\n",
      "             3,  2156,    54,    16,    10,   593,  1441,     9,  1515,  3125,\n",
      "           692, 11364,   229, 12746,  1396,  2156,    21,  5796,     7,   369,\n",
      "          1101,     7,   492,  1131,   575,     7,  1636,   479,     2],\n",
      "        [    0,    20,  4149,    16,     5,   665,    11,    10,   651,     9,\n",
      "         21417,    13, 25888,  2156,    54,    21, 12850,     9, 30499,   659,\n",
      "             7,    69,  1437,     3,    80,  1437,     3,  1928,  7250,    11,\n",
      "           419,   644,  3691,    71,  1437,     3,    69,  1437,     3,    78,\n",
      "          2536,   474, 13207,   479,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  1702,    42,    15,   622,  4832,  1702,  1437,     3,  2412,\n",
      "           661,  1437,     3,   333,  1437,     3, 12523,   374,    20,  3664,\n",
      "          1437,     3,    16,  1996,    63,   453,     7, 10745,    13,  1895,\n",
      "           148,  4282,  1284,   128,    29,  6185,  1901,    23,  4465,   128,\n",
      "            29,    96,  3677,   876,  4754,    23,     5,   253,     9,     5,\n",
      "          1557,  8825,   479,     2,     1,     1,     1,     1,     1]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[42, 43, 44, 45, 46, 47, 48, 49, 50, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [21, 22, 23, 24, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[ 5,  6,  7,  8,  9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [22, 23, 24, 25, 26, 27, 28, 29, 30, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[35, 36, 37, 38, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [23, 24, 25, 26, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[15, 16, 17, 18, 19, 20, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [ 8,  9, 10, 11, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([ 0,  0,  0, 11])}\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(dataloader):\n",
    "    print(step, batch)\n",
    "    if step > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'word_ids': tensor([[    0,   382,  2096,  1863,     9,   331,  1437,     3, 10730, 11247,\n",
      "          1437,     3,    21, 14589,    15,     5,  2570,    30,    10, 27622,\n",
      "          8090,   259,    15,  1437,     3,   294,  1437,     3,  2156,    10,\n",
      "           183,   137,     5,   563,    21,  2633,     7,  2604,  1863,  1292,\n",
      "          5981, 11488,    12, 16956,  2156,     5,   331,   641,    26,   479,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 34628,  1313,  5457,   270,  5141,  2235,  2156,  3287,   270,\n",
      "          4282,  1284,  2156,  1437,     3,  1863,     9,  4545,  1437,     3,\n",
      "          2156,  1437,     3,   610,  9153,  1437,     3,  2156,  4753,     4,\n",
      "           479,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    0, 45518,   436,  1991,     5,  1437,     3, 15400,   534,  1437,\n",
      "             3,   111,   574, 26770,    12,     5,  2248,    12, 17409, 16909,\n",
      "         13256,   462,  4733,   826,   111, 25733,   387,    12,    64,   465,\n",
      "            10,   169,     7,  2506,    10,  2394,   227,  1748,   786,  4892,\n",
      "           462, 37551,     8,   111,   574, 26770,    12,     5,   111, 25733,\n",
      "           387,    12,  7053,   304,     9,  1007,  2156, 12801,  1437,     3,\n",
      "         27268,  1437,     3,    26,   479,     2],\n",
      "        [    0,  3507,  1437,     3,  1655, 14449,  1437,     3, 22882,   196,\n",
      "           184,     7, 14575,    92,   301,    88,     5,  4452,     8,   172,\n",
      "          1437,     3,  8677,  1437,     3,  2156,    15,  2541,    31,   786,\n",
      "            12, 10925,  6912,   315,  2156,  2322,    11, 16133,  1580,     7,\n",
      "          1498,     5,   372,  5111,   479,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]]), 'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'entity_ids': tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2],\n",
      "        [1, 2]]), 'entity_attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), 'entity_position_ids': tensor([[[ 7,  8,  9, 10, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [24, 25, 26, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[22, 23, 24, 25, 26, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [14, 15, 16, 17, 18, 19, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[ 6,  7,  8,  9, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [59, 60, 61, 62, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
      "\n",
      "        [[ 3,  4,  5,  6,  7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "         [21, 22, 23, 24, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]), 'label': tensor([ 0, 41,  0,  0])}\n",
      "outputs {'loss': tensor(0.)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "global_step = 0\n",
    "tr_loss = 0.0\n",
    "\n",
    "num_workers = torch.cuda.device_count()\n",
    "\n",
    "def maybe_no_sync(step):\n",
    "    if (\n",
    "        hasattr(model, \"no_sync\")\n",
    "        and num_workers > 1\n",
    "        and (step + 1) % self.args.gradient_accumulation_steps != 0\n",
    "    ):\n",
    "        return model.no_sync()\n",
    "    else:\n",
    "        return contextlib.ExitStack()\n",
    "counter = 0;\n",
    "model.train()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with tqdm(total=num_train_steps, disable=args.local_rank not in (-1, 0)) as pbar:\n",
    "    while True:\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            inputs = {k: v.to(args.device) for k, v in _create_model_arguments(batch).items()}\n",
    "            outputs = model(**inputs)\n",
    "            print('inputs:', inputs)\n",
    "            print('outputs', outputs)\n",
    "            counter += 1\n",
    "            if counter > 0:\n",
    "                break\n",
    "        break\n",
    "#             loss = outputs[0]\n",
    "#             if args.gradient_accumulation_steps > 1:\n",
    "#                 loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "#             with maybe_no_sync(step):\n",
    "#                 if args.fp16:\n",
    "#                     with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                         scaled_loss.backward()\n",
    "#                 else:\n",
    "#                     loss.backward()\n",
    "\n",
    "#             tr_loss += loss.item()\n",
    "#             if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "#                 if args.max_grad_norm != 0.0:\n",
    "#                     if args.fp16:\n",
    "#                         torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "#                     else:\n",
    "#                         torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 model.zero_grad()\n",
    "\n",
    "#                 pbar.set_description(\"epoch: %d loss: %.7f\" % (epoch, loss.item()))\n",
    "#                 pbar.update()\n",
    "#                 global_step += 1\n",
    "\n",
    "#                 if step_callback is not None:\n",
    "#                     step_callback(model, global_step)\n",
    "\n",
    "#                 if (\n",
    "#                     args.local_rank in (-1, 0)\n",
    "#                     and args.output_dir\n",
    "#                     and args.save_steps > 0\n",
    "#                     and global_step % args.save_steps == 0\n",
    "#                 ):\n",
    "#                     output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "#                     if hasattr(model, \"module\"):\n",
    "#                         torch.save(model.module.state_dict(), os.path.join(output_dir, WEIGHTS_NAME))\n",
    "#                     else:\n",
    "#                         torch.save(model.state_dict(), os.path.join(output_dir, WEIGHTS_NAME))\n",
    "\n",
    "#                 if global_step == num_train_steps:\n",
    "#                     break\n",
    "# USED FOR TESTING\n",
    "#         if global_step == num_train_steps:\n",
    "#             break\n",
    "#         epoch += 1\n",
    "\n",
    "# logger.info(\"global_step = %s, average loss = %s\", global_step, tr_loss / global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example1 =  {'word_ids': tensor([[    0,     3,   229,  8629,  1222,  1437,     3,  2156,    10,  1437,\n",
    "             3,  4423,  1437,     3,  6239,     8,  1030,  3313,     9,  1600,\n",
    "          2156,    21,    11,     5,   609,     9,  1959,    10,  1859,  6239,\n",
    "            77,    37,    21,  1128,    11,  1752,    11,   628,  5155,   479,\n",
    "             2,     1,     1,     1,     1,     1,     1],\n",
    "        [    0,   286,  1246,  2156,    13,   107,    89,   128,    29,    57,\n",
    "             5, 38162,   470,   788,   268,  1544,   111,   574, 26770,    12,\n",
    "          1437,     3,    83,  3813,  1437,     3,   111, 25733,   387,    12,\n",
    "          4248,     5,    55,  1611, 20391,   730,   128,    29,  1437,     3,\n",
    "          2573,   788,   268,  1437,     3,   479,     2],\n",
    "        [    0,    96,  1285,     7,  1437,     3,    69,  1437,     3,   979,\n",
    "          2156,  1437,     3,    79,  1437,     3,    16,  5601,    30,    10,\n",
    "         21002,     8,    10,   372,    12, 11377, 26243,   479,     2,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1],\n",
    "        [    0,   370,    32,  9180,    14,     5,  1437,     3,  1148,     9,\n",
    "           391,  1704,  4466,  1890,  2485,  1437,     3,    16,  1826,    10,\n",
    "           529,     9,    63,  1505,  1674,   111,   574, 26770,    12,  1437,\n",
    "             3,  9841,  1437,     3,   111, 25733,   387,    12,    31,   601,\n",
    "            12,   844,   772,  3010,   479,     2,     1]]), \n",
    "            'word_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), \n",
    "             'word_segment_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n",
    "             'entity_ids': tensor([[1, 2],\n",
    "        [1, 2],\n",
    "        [1, 2],\n",
    "        [1, 2]]), \n",
    "             'entity_attention_mask': tensor([[1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1],\n",
    "        [1, 1]]), \n",
    "             'entity_position_ids': tensor([[[ 1,  2,  3,  4,  5,  6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "         [10, 11, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
    "\n",
    "        [[21, 22, 23, 24, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "         [39, 40, 41, 42, 43, 44, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
    "\n",
    "        [[12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "         [ 5,  6,  7,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]],\n",
    "\n",
    "        [[30, 31, 32, 33, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "         [ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, -1, -1, -1, -1, -1, -1, -1,\n",
    "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]]), 'entity_segment_ids': tensor([[0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 0]]), \n",
    "             'label': tensor([31,  0,  0,  0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
