{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import WEIGHTS_NAME\n",
    "\n",
    "from utils.entity_vocab import MASK_TOKEN\n",
    "\n",
    "from exp_utils import set_seed\n",
    "from exp_utils.trainer import Trainer, trainer_args\n",
    "from re_model import LukeForRelationClassification\n",
    "from re_utils import HEAD_TOKEN, TAIL_TOKEN, convert_examples_to_features, DatasetProcessor\n",
    "from transformers.tokenization_roberta import RobertaTokenizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "metadata_folder = \"luke_model/\"\n",
    "\n",
    "class obj(object):\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "with open(os.path.join(metadata_folder, \"metadata.json\")) as f:\n",
    "    model_config = obj(json.load(f)[\"model_config\"])\n",
    "\n",
    "print(model_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params:\n",
    "    def __init__(self, model_config):\n",
    "        self.data_dir = \"data/tacred/json\"\n",
    "        self.do_train = \"--no-train\"\n",
    "        self.train_batch_size = 4\n",
    "        self.num_train_epochs = 5.0\n",
    "        self.do_val = \"--no-eval\"\n",
    "        self.eval_batch_size = 128\n",
    "        self.seed = 42\n",
    "        self.bert_model_name = \"roberta-large\"\n",
    "        self.max_mention_length = 30\n",
    "        self.local_rank = -1\n",
    "        self.tokenizer =  RobertaTokenizer.from_pretrained(self.bert_model_name)\n",
    "        self.model_config = model_config\n",
    "        self.model_weights = {\"embeddings.word_embeddings.weight\":0.25, \"entity_embeddings.entity_embeddings.weight\":0.25}\n",
    "        \n",
    "#         self.tokenizer = {\"max_len\": 512, \"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"sep_token\": \"</s>\", \"pad_token\": \"<pad>\", \"cls_token\": \"<s>\", \"mask_token\": \"<mask>\", \"init_inputs\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params(model_config)\n",
    "args.tokenizer.pad_token_id\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, fold=\"train\"):\n",
    "\n",
    "    processor = DatasetProcessor()\n",
    "    if fold == \"train\":\n",
    "        examples = processor.get_train_examples(args.data_dir)\n",
    "    elif fold == \"dev\":\n",
    "        examples = processor.get_dev_examples(args.data_dir)\n",
    "    else:\n",
    "        examples = processor.get_test_examples(args.data_dir)\n",
    "\n",
    "    label_list = processor.get_label_list(args.data_dir)\n",
    "\n",
    "    bert_model_name = args.bert_model_name\n",
    "\n",
    "    cache_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_\" + \"_\".join((args.bert_model_name.split(\"-\")[0], str(args.max_mention_length), fold)) + \".pkl\",\n",
    "    )\n",
    "    print('cache_file', cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cache_file)\n",
    "        features = torch.load(cache_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file\")\n",
    "        features = convert_examples_to_features(examples, label_list, args.tokenizer, args.max_mention_length)\n",
    "\n",
    "        if args.local_rank in (-1, 0):\n",
    "            torch.save(features, cache_file)\n",
    "\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        def create_padded_sequence(attr_name, padding_value):\n",
    "            tensors = [torch.tensor(getattr(o, attr_name), dtype=torch.long) for o in batch]\n",
    "            return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "        return dict(\n",
    "            word_ids=create_padded_sequence(\"word_ids\", args.tokenizer.pad_token_id),\n",
    "            word_attention_mask=create_padded_sequence(\"word_attention_mask\", 0),\n",
    "            word_segment_ids=create_padded_sequence(\"word_segment_ids\", 0),\n",
    "            entity_ids=create_padded_sequence(\"entity_ids\", 0),\n",
    "            entity_attention_mask=create_padded_sequence(\"entity_attention_mask\", 0),\n",
    "            entity_position_ids=create_padded_sequence(\"entity_position_ids\", -1),\n",
    "            entity_segment_ids=create_padded_sequence(\"entity_segment_ids\", 0),\n",
    "            label=torch.tensor([o.label for o in batch], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    if fold in (\"dev\", \"test\"):\n",
    "        dataloader = DataLoader(features, batch_size=args.eval_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    else:\n",
    "        if args.local_rank == -1:\n",
    "            sampler = RandomSampler(features)\n",
    "        else:\n",
    "            sampler = DistributedSampler(features)\n",
    "        dataloader = DataLoader(features, sampler=sampler, batch_size=args.train_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader, examples, features, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.model_config.vocab_size += 2\n",
    "# word_emb = args.model_weights[\"embeddings.word_embeddings.weight\"]\n",
    "# head_emb = word_emb[args.tokenizer.convert_tokens_to_ids([\"@\"])[0]].unsqueeze(0)\n",
    "# tail_emb = word_emb[args.tokenizer.convert_tokens_to_ids([\"#\"])[0]].unsqueeze(0)\n",
    "# args.model_weights[\"embeddings.word_embeddings.weight\"] = torch.cat([word_emb, head_emb, tail_emb])\n",
    "# args.tokenizer.add_special_tokens(dict(additional_special_tokens=[HEAD_TOKEN, TAIL_TOKEN]))\n",
    "\n",
    "# entity_emb = args.model_weights[\"entity_embeddings.entity_embeddings.weight\"]\n",
    "# mask_emb = entity_emb[args.entity_vocab[MASK_TOKEN]].unsqueeze(0).expand(2, -1)\n",
    "# args.model_config.entity_vocab_size = 3\n",
    "# args.model_weights[\"entity_embeddings.entity_embeddings.weight\"] = torch.cat([entity_emb[:1], mask_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 81/68124 [00:00<01:24, 809.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_file data/tacred/json\\cached_roberta_30_train.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 68124/68124 [00:27<00:00, 2501.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dataloader, examples, features, label_list = load_and_cache_examples(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader, list, list, list)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader), type(examples), type(features), type(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68124,), (68124,), (42,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(examples).shape, np.array(features).shape, np.array(label_list).shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_relation', 'org:alternate_names', 'org:city_of_headquarters', 'org:country_of_headquarters', 'org:dissolved', 'org:founded', 'org:founded_by', 'org:member_of', 'org:members', 'org:number_of_employees/members', 'org:parents', 'org:political/religious_affiliation', 'org:shareholders', 'org:stateorprovince_of_headquarters', 'org:subsidiaries', 'org:top_members/employees', 'org:website', 'per:age', 'per:alternate_names', 'per:cause_of_death']\n"
     ]
    }
   ],
   "source": [
    "print(label_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.tokenizer.convert_tokens_to_ids([\"@\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_list)\n",
    "model_weights= [0.34,0.33,0.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LukeForRelationClassification(args, num_labels)\n",
    "# model.load_state_dict(model_weights, strict=False)\n",
    "# model.to(args.device)\n",
    "\n",
    "# num_train_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n",
    "# num_train_steps = int(num_train_steps_per_epoch * args.num_train_epochs)\n",
    "\n",
    "# best_dev_f1 = [-1]\n",
    "# best_weights = [None]\n",
    "\n",
    "# def step_callback(model, global_step):\n",
    "#     if global_step % num_train_steps_per_epoch == 0 and args.local_rank in (0, -1):\n",
    "#         epoch = int(global_step / num_train_steps_per_epoch - 1)\n",
    "#         dev_results = evaluate(args, model, fold=\"dev\")\n",
    "#         args.experiment.log_metrics({f\"dev_{k}_epoch{epoch}\": v for k, v in dev_results.items()}, epoch=epoch)\n",
    "#         results.update({f\"dev_{k}_epoch{epoch}\": v for k, v in dev_results.items()})\n",
    "#         tqdm.write(\"dev: \" + str(dev_results))\n",
    "\n",
    "#         if dev_results[\"f1\"] > best_dev_f1[0]:\n",
    "#             if hasattr(model, \"module\"):\n",
    "#                 best_weights[0] = {k: v.to(\"cpu\").clone() for k, v in model.module.state_dict().items()}\n",
    "#             else:\n",
    "#                 best_weights[0] = {k: v.to(\"cpu\").clone() for k, v in model.state_dict().items()}\n",
    "#             best_dev_f1[0] = dev_results[\"f1\"]\n",
    "#             results[\"best_epoch\"] = epoch\n",
    "\n",
    "#         model.train()\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     args, model=model, dataloader=train_dataloader, num_train_steps=num_train_steps, step_callback=step_callback\n",
    "# )\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/pretrain_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " encoder.layer.0.attention.self.query.weight   tensor([[-0.0029,  0.0352,  0.0007,  ...,  0.0023,  0.0595, -0.0426],\n",
      "        [-0.0248,  0.0529, -0.0145,  ..., -0.0303, -0.0143,  0.0116],\n",
      "        [ 0.0061,  0.0708, -0.0336,  ...,  0.0807,  0.0115, -0.0131],\n",
      "        ...,\n",
      "        [-0.0589,  0.0206, -0.0426,  ..., -0.0298,  0.0041,  0.0700],\n",
      "        [ 0.0421,  0.0225, -0.0608,  ..., -0.0552, -0.0157,  0.0173],\n",
      "        [-0.0184, -0.0457, -0.0103,  ...,  0.0474,  0.0225, -0.0182]]) \n",
      "\n",
      "2 \n",
      " encoder.layer.0.attention.self.query.bias   tensor([ 0.3121,  0.0556, -0.0751,  ..., -0.0704, -0.0500, -0.0664]) \n",
      "\n",
      "3 \n",
      " encoder.layer.0.attention.self.key.weight   tensor([[-0.0043, -0.0184, -0.0136,  ..., -0.0037,  0.0096, -0.0156],\n",
      "        [-0.0238, -0.0002,  0.0253,  ...,  0.0403,  0.0436, -0.0195],\n",
      "        [-0.0264, -0.0522, -0.0125,  ..., -0.0359,  0.0077,  0.0150],\n",
      "        ...,\n",
      "        [-0.0718, -0.0261, -0.0203,  ..., -0.0186,  0.0097,  0.1023],\n",
      "        [ 0.0157,  0.0065, -0.0171,  ..., -0.0038, -0.0090,  0.0435],\n",
      "        [-0.0091, -0.0628,  0.0415,  ...,  0.0466,  0.0149, -0.0468]]) \n",
      "\n",
      "4 \n",
      " encoder.layer.0.attention.self.key.bias   tensor([-0.0041, -0.0033, -0.0012,  ...,  0.0013,  0.0017,  0.0018]) \n",
      "\n",
      "5 \n",
      " encoder.layer.0.attention.self.value.weight   tensor([[ 0.0306, -0.0006, -0.0241,  ..., -0.0187,  0.0017,  0.0217],\n",
      "        [ 0.0565,  0.0432,  0.0015,  ..., -0.0156,  0.0920, -0.0204],\n",
      "        [-0.0151, -0.0429,  0.0127,  ..., -0.0512,  0.0012,  0.0675],\n",
      "        ...,\n",
      "        [-0.0101,  0.0082, -0.0115,  ...,  0.0363,  0.0256,  0.0110],\n",
      "        [-0.0032, -0.0139, -0.0513,  ...,  0.0377, -0.0338,  0.0291],\n",
      "        [ 0.0047, -0.0094, -0.0135,  ..., -0.0247,  0.0876, -0.0179]]) \n",
      "\n",
      "6 \n",
      " encoder.layer.0.attention.self.value.bias   tensor([-0.0014,  0.0024, -0.0083,  ..., -0.0226, -0.0209, -0.0350]) \n",
      "\n",
      "7 \n",
      " encoder.layer.0.attention.output.dense.weight   tensor([[ 0.0015,  0.0398, -0.0170,  ..., -0.0140, -0.0325, -0.0164],\n",
      "        [-0.0346,  0.0133, -0.0157,  ...,  0.0311, -0.0096,  0.0268],\n",
      "        [ 0.0271, -0.0743,  0.0193,  ..., -0.0323, -0.0068,  0.0988],\n",
      "        ...,\n",
      "        [ 0.0285,  0.0010,  0.0225,  ...,  0.0128, -0.0104, -0.0361],\n",
      "        [-0.0081,  0.0514, -0.0476,  ...,  0.0426,  0.0263, -0.0157],\n",
      "        [ 0.0468,  0.0232,  0.0929,  ...,  0.0274,  0.0088,  0.0074]]) \n",
      "\n",
      "8 \n",
      " encoder.layer.0.attention.output.dense.bias   tensor([-0.0135,  0.0296,  0.0857,  ...,  0.0735, -0.0072,  0.0103]) \n",
      "\n",
      "9 \n",
      " encoder.layer.0.attention.output.LayerNorm.weight   tensor([0.9795, 0.9904, 0.9729,  ..., 0.9834, 0.9905, 0.9973]) \n",
      "\n",
      "10 \n",
      " encoder.layer.0.attention.output.LayerNorm.bias   tensor([-0.4307,  0.2765, -0.0064,  ...,  0.0114,  0.3294, -0.2977]) \n",
      "\n",
      "11 \n",
      " encoder.layer.0.intermediate.dense.weight   tensor([[ 0.0584, -0.0646, -0.0934,  ...,  0.0051,  0.0196, -0.0153],\n",
      "        [ 0.0162, -0.0273,  0.0207,  ...,  0.0155, -0.0379,  0.1215],\n",
      "        [ 0.0367, -0.0660, -0.0006,  ...,  0.0293, -0.0267, -0.0219],\n",
      "        ...,\n",
      "        [ 0.0165, -0.0888,  0.0040,  ...,  0.0331, -0.0512, -0.0009],\n",
      "        [ 0.1342,  0.0519, -0.1299,  ..., -0.1486, -0.0311,  0.0218],\n",
      "        [ 0.0719, -0.0296, -0.0604,  ...,  0.0095, -0.0576, -0.0292]]) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-d0829df1d511>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print (i, \"\\n\", key, \" \", torch.tensor(value),  \"\\n\")\n"
     ]
    }
   ],
   "source": [
    "# modelA = TheModelAClass(*args, **kwargs)\n",
    "model = torch.load(\"saved_model/luke.bin\",map_location=torch.device('cpu'))\n",
    "type(model)\n",
    "# model = torch.load(\"C:/prabhu/edu/code/w266/Luke/model/luke_20200528.tar\")\n",
    "# writer.add_graph(model)\n",
    "# writer.close()\n",
    "i=0\n",
    "for key, value in model.items():\n",
    "    i+=1\n",
    "    print (i, \"\\n\", key, \" \", torch.tensor(value),  \"\\n\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code works for loading the model from saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from luke.pretraining.model import EntityPredictionHeadTransform, LukePretrainingModel,EntityPredictionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert configuration {'return_dict': False, 'output_hidden_states': False, 'output_attentions': False, 'use_cache': True, 'torchscript': False, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'architectures': ['RobertaForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'decoder_start_token_id': None, 'task_specific_params': None, 'xla_device': None, 'model_type': 'roberta', 'vocab_size': 50265, 'hidden_size': 1024, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'hidden_act': 'gelu', 'intermediate_size': 4096, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 514, 'type_vocab_size': 1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'gradient_checkpointing': False}\n"
     ]
    }
   ],
   "source": [
    "path = \"saved_model/luke.bin\"\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForPreTraining,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from luke.model import LukeConfig\n",
    "from luke.optimization import LukeAdamW\n",
    "from luke.pretraining.batch_generator import LukePretrainingBatchGenerator, MultilingualBatchGenerator\n",
    "from luke.pretraining.dataset import WikipediaPretrainingDataset\n",
    "from luke.pretraining.model import LukePretrainingModel\n",
    "from luke.utils.model_utils import ENTITY_VOCAB_FILE\n",
    "\n",
    "bert_model_name = \"roberta-large\"\n",
    "\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
    "\n",
    "print(\"bert configuration\", bert_config.to_dict())\n",
    "\n",
    "config = LukeConfig(\n",
    "    entity_vocab_size=500000,\n",
    "    bert_model_name=bert_model_name,\n",
    "    entity_emb_size=256,\n",
    "    **bert_config.to_dict(),\n",
    ")\n",
    "model = LukePretrainingModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LukePretrainingModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"lm_head.decoder.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-eb76a5f595b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# model.to(device='cpu')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LukePretrainingModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\", \"lm_head.decoder.bias\". "
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# model.to(device='cpu')\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
