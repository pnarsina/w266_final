{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import config\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68124"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_params = config.VocabParameters()\n",
    "training_params = config.TrainingParameters()\n",
    "eval_params = config.EvalParameters()\n",
    "\n",
    "with open(vocab_params.data_dir+ '/train.json') as infile:\n",
    "    json_data = json.load(infile)\n",
    "len(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>relation</th>\n",
       "      <th>token</th>\n",
       "      <th>subj_start</th>\n",
       "      <th>subj_end</th>\n",
       "      <th>obj_start</th>\n",
       "      <th>obj_end</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>stanford_pos</th>\n",
       "      <th>stanford_ner</th>\n",
       "      <th>stanford_head</th>\n",
       "      <th>stanford_deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42795</th>\n",
       "      <td>61b3a65fb960f284ebac</td>\n",
       "      <td>03c67d9ee4bf4ed33cbeddaa3a7b82cc</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Red, Sox, 12, ,, Athletics, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[NNP, NNP, CD, ,, NNP, CD]</td>\n",
       "      <td>[ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...</td>\n",
       "      <td>[2, 0, 2, 2, 2, 5]</td>\n",
       "      <td>[compound, ROOT, nummod, punct, appos, nummod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46757</th>\n",
       "      <td>61b3a65fb9080a05b4ee</td>\n",
       "      <td>15df2fc6a9a895432237cb2bdfcbd1b5</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Thomas, ', assertion, of, 85, %, reporters, v...</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...</td>\n",
       "      <td>[PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...</td>\n",
       "      <td>[3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...</td>\n",
       "      <td>[nmod:poss, case, nsubj, case, compound, amod,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36591</th>\n",
       "      <td>61b3a65fb9883fc52f01</td>\n",
       "      <td>274e368f381c1476fe0da7f201bfc331</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Kerry, did, his, duty, and, did, it, well, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2, 0, 4, 2, 2, 2, 6, 6, 2]</td>\n",
       "      <td>[nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22262</th>\n",
       "      <td>61b3a65fb937b50fc05a</td>\n",
       "      <td>409fa10efff702a41701bdddab89a2dd</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[This, August, ,, Moschella, 's, name, came, u...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "      <td>[DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...</td>\n",
       "      <td>[DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...</td>\n",
       "      <td>[det, ROOT, punct, nmod:poss, case, nsubj, acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>61b3a5f2e85a8088c7bb</td>\n",
       "      <td>78d7e406b6911492f6f7f122d1f112ad</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[Sharpton, is, president, of, the, National, A...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...</td>\n",
       "      <td>[3, 3, 0, 8, 8, 8, 8, 3, 3]</td>\n",
       "      <td>[nsubj, cop, ROOT, case, det, compound, compou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                             docid  \\\n",
       "42795  61b3a65fb960f284ebac  03c67d9ee4bf4ed33cbeddaa3a7b82cc   \n",
       "46757  61b3a65fb9080a05b4ee  15df2fc6a9a895432237cb2bdfcbd1b5   \n",
       "36591  61b3a65fb9883fc52f01  274e368f381c1476fe0da7f201bfc331   \n",
       "22262  61b3a65fb937b50fc05a  409fa10efff702a41701bdddab89a2dd   \n",
       "644    61b3a5f2e85a8088c7bb  78d7e406b6911492f6f7f122d1f112ad   \n",
       "\n",
       "                        relation  \\\n",
       "42795                no_relation   \n",
       "46757                no_relation   \n",
       "36591                no_relation   \n",
       "22262                no_relation   \n",
       "644    org:top_members/employees   \n",
       "\n",
       "                                                   token  subj_start  \\\n",
       "42795                    [Red, Sox, 12, ,, Athletics, 2]           0   \n",
       "46757  [Thomas, ', assertion, of, 85, %, reporters, v...          41   \n",
       "36591     [Kerry, did, his, duty, and, did, it, well, .]           2   \n",
       "22262  [This, August, ,, Moschella, 's, name, came, u...          17   \n",
       "644    [Sharpton, is, president, of, the, National, A...           5   \n",
       "\n",
       "       subj_end  obj_start  obj_end     subj_type           obj_type  \\\n",
       "42795         1          4        4  ORGANIZATION       ORGANIZATION   \n",
       "46757        43         31       31  ORGANIZATION               DATE   \n",
       "36591         2          0        0        PERSON             PERSON   \n",
       "22262        20         39       40  ORGANIZATION  STATE_OR_PROVINCE   \n",
       "644           7          0        0  ORGANIZATION             PERSON   \n",
       "\n",
       "                                            stanford_pos  \\\n",
       "42795                         [NNP, NNP, CD, ,, NNP, CD]   \n",
       "46757  [NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...   \n",
       "36591          [NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]   \n",
       "22262  [DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...   \n",
       "644             [NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]   \n",
       "\n",
       "                                            stanford_ner  \\\n",
       "42795  [ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...   \n",
       "46757  [PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...   \n",
       "36591                   [PERSON, O, O, O, O, O, O, O, O]   \n",
       "22262  [DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...   \n",
       "644    [PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...   \n",
       "\n",
       "                                           stanford_head  \\\n",
       "42795                                 [2, 0, 2, 2, 2, 5]   \n",
       "46757  [3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...   \n",
       "36591                        [2, 0, 4, 2, 2, 2, 6, 6, 2]   \n",
       "22262  [2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...   \n",
       "644                          [3, 3, 0, 8, 8, 8, 8, 3, 3]   \n",
       "\n",
       "                                         stanford_deprel  \n",
       "42795     [compound, ROOT, nummod, punct, appos, nummod]  \n",
       "46757  [nmod:poss, case, nsubj, case, compound, amod,...  \n",
       "36591  [nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...  \n",
       "22262  [det, ROOT, punct, nmod:poss, case, nsubj, acl...  \n",
       "644    [nsubj, cop, ROOT, case, det, compound, compou...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(json_data )\n",
    "train_df_sorted = train_df.sort_values(by=['docid','id'], ascending = True)\n",
    "train_df_sorted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizer for BIO BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to convert work toekns to POS, NER and word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the TACRED data to vectors using above helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from above dataframe.  Each row in the dataframe represents a  \n",
    "    \n",
    "\"\"\"\n",
    "max_length = 50\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "pos_column = 'stanford_pos'\n",
    "ner_column = 'stanford_ner'\n",
    "token_column = 'token'\n",
    "\n",
    "\n",
    "for ind in train_df.index:    \n",
    "    word_list = train_df[token_column][ind]\n",
    "    ner_list = train_df[ner_column][ind]\n",
    "    pos_list = train_df[pos_column][ind]\n",
    "\n",
    "    for i in range(0,len(word_list)):\n",
    "\n",
    "        word = word_list[i]\n",
    "        ner = ner_list[i]\n",
    "        pos = pos_list[i]\n",
    "        addDict = addWord(word, pos, ner)\n",
    "    \n",
    "        sentenceTokens += addDict['wordToken']\n",
    "        posTokens += addDict['posToken']\n",
    "        nerTokens += addDict['nerToken']        \n",
    "\n",
    "#     print(sentenceTokens, posTokens, nerTokens, \"\\n\")\n",
    "    \n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "    \n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        posTokens = posTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding\n",
    "\n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "\n",
    "    posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    sentenceList.append(sentence)\n",
    "\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    posTokenList.append(posTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    sentence = ''\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    posTokens = ['[posCLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    sentence += ' ' + word\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "sentLengthList = sentLengthList[2:]\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "posTokenList = posTokenList[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 1.8000e+01, 1.5000e+01, 4.1000e+01, 5.2000e+01,\n",
       "        8.3000e+01, 1.0400e+02, 1.8400e+02, 1.8300e+02, 2.2500e+02,\n",
       "        2.8800e+02, 2.8300e+02, 3.0700e+02, 4.2600e+02, 4.4500e+02,\n",
       "        0.0000e+00, 4.9200e+02, 5.8300e+02, 6.6000e+02, 6.8500e+02,\n",
       "        6.7700e+02, 7.3600e+02, 9.6100e+02, 9.5000e+02, 1.0360e+03,\n",
       "        1.1930e+03, 1.2540e+03, 1.3570e+03, 1.3160e+03, 1.3940e+03,\n",
       "        1.4160e+03, 0.0000e+00, 1.4130e+03, 1.5450e+03, 1.4980e+03,\n",
       "        1.5340e+03, 1.6630e+03, 1.5620e+03, 1.5780e+03, 1.7290e+03,\n",
       "        1.5580e+03, 1.5800e+03, 1.5840e+03, 1.5740e+03, 1.4380e+03,\n",
       "        1.3910e+03, 1.4520e+03, 2.7658e+04]),\n",
       " array([ 4.    ,  4.9375,  5.875 ,  6.8125,  7.75  ,  8.6875,  9.625 ,\n",
       "        10.5625, 11.5   , 12.4375, 13.375 , 14.3125, 15.25  , 16.1875,\n",
       "        17.125 , 18.0625, 19.    , 19.9375, 20.875 , 21.8125, 22.75  ,\n",
       "        23.6875, 24.625 , 25.5625, 26.5   , 27.4375, 28.375 , 29.3125,\n",
       "        30.25  , 31.1875, 32.125 , 33.0625, 34.    , 34.9375, 35.875 ,\n",
       "        36.8125, 37.75  , 38.6875, 39.625 , 40.5625, 41.5   , 42.4375,\n",
       "        43.375 , 44.3125, 45.25  , 46.1875, 47.125 , 48.0625, 49.    ]),\n",
       " <a list of 48 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nerCLS]\n",
      "O\n",
      "ORGANIZATION\n",
      "nerX\n",
      "MISC\n",
      "[nerSEP]\n",
      "[nerPAD]\n",
      "DATE\n",
      "PERSON\n",
      "NUMBER\n",
      "LOCATION\n",
      "ORDINAL\n",
      "DURATION\n",
      "SET\n",
      "MONEY\n",
      "TIME\n",
      "PERCENT\n"
     ]
    }
   ],
   "source": [
    "x =nerClasses['tag'].unique()\n",
    "x\n",
    "for a in x:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>O</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>O</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tag  cat  occurences\n",
       "102   O    0           0\n",
       "103   O    1           0\n",
       "104   O    2           0\n",
       "105   O    3           0\n",
       "106   O    4           0\n",
       "107   O    5           0\n",
       "108   O    6     1600623\n",
       "109   O    7           0\n",
       "110   O    8           0\n",
       "111   O    9           0\n",
       "112   O   10           0\n",
       "113   O   11           0\n",
       "114   O   12           0\n",
       "115   O   13           0\n",
       "116   O   14           0\n",
       "117   O   15           0\n",
       "118   O   16           0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution[nerDistribution['tag'] == 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now split data into train, test and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])\n",
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 47758, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [bert_inputs_train_k, labels_train_k]\n",
    "test_all = [bert_inputs_test_k, labels_test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./biobert_train_tacred_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"./biobert_test_tacred_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./biobert_train_tacred_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_train_k, labels_train_k = train_all = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"./biobert_test_tacred_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_test_k, labels_test_k = test_all = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 10,  6,  2,  2,  6,  6, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 47758, 50), (47758, 50), (3, 20364, 50), (20364, 50))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bert_inputs_train_k).shape, labels_train_k.shape,np.array(bert_inputs_test_k).shape, labels_test_k.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       ...,\n",
       "       [13,  6,  6, ..., 16, 15, 14],\n",
       "       [13, 10, 16, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14]], dtype=int8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 1970274510448 acquired on C:\\Users\\pnars/.cache\\torch\\transformers\\d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee556d6171f4551ac1cbd28c1a188c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 1970274510448 released on C:\\Users\\pnars/.cache\\torch\\transformers\\d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# frozen_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "frozen_model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in frozen_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sampler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-e885edfe2ef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Then package it into a DataLoader that can be batched and processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sampler'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler\n",
    "\n",
    "num_epochs=3\n",
    "train_batch_size = 50\n",
    "\n",
    "# Prepare data by processing features into inputs\n",
    "all_input_ids = torch.tensor(bert_inputs_train_k[0], dtype=torch.long)\n",
    "all_input_mask = torch.tensor(bert_inputs_train_k[1], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor(bert_inputs_train_k[2], dtype=torch.long)\n",
    "all_label = torch.tensor(labels_train_k, dtype=torch.long)\n",
    "\n",
    "# Package it all nicely into a TensorDataset \n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
    "\n",
    "# Then package it into a DataLoader that can be batched and processed \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size= train_batch_size)\n",
    "\n",
    "global_step = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  print(\"epoch\", datetime.now(pytz.timezone('US/Pacific')).strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "  for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    # Create a batch \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "    # Calculate loss from forward feed method \n",
    "    output = model(input_ids=input_ids, \n",
    "                 attention_mask=input_mask, \n",
    "                 token_type_ids=segment_ids,\n",
    "                 labels=label_ids,\n",
    "                 return_dict=True)\n",
    "    loss=output[\"loss\"]\n",
    "\n",
    "    # back prop\n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % args.gradient_accumulation_steps == 0:   # Number of updates steps to accumulate before performing a backward/update pass.\n",
    "      # update \n",
    "      optimizer.step()\n",
    "      # reset gradient \n",
    "      model.zero_grad()\n",
    "      global_step += 1\n",
    "\n",
    "      # Show the answer choice with the highest score for each question \n",
    "      train_predictions = torch.argmax(torch.nn.functional.softmax(output[\"logits\"]), dim=1)\n",
    "      # Accuracy against train data\n",
    "      train_accuracy = float(sum(label_ids==train_predictions))/len(label_ids)\n",
    "\n",
    "      # log loss \n",
    "      writer.add_scalar('training loss', \n",
    "                      loss.item(), \n",
    "                      global_step)\n",
    "      # Log accuracy against train data\n",
    "      writer.add_scalar('train accuracy', train_accuracy, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom Loss and accuracy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Loss function for the NER Model and default loss function will include padded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #For now, using simple loss function\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom accuracy funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(phase, running_loss, running_corrects):\n",
    "\n",
    "    epoch_loss = running_loss / len(image_datasets[phase])\n",
    "    epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "    return (epoch_loss, epoch_acc)\n",
    "\n",
    "\n",
    "# def custom_acc_orig_tokens(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "#     y_true: Shape: (batch x (max_length) )\n",
    "#     y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "#     returns: accuracy\n",
    "#     \"\"\"\n",
    "\n",
    "#     #get labels and predictions\n",
    "    \n",
    "#     y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "#     mask = (y_label < 13)\n",
    "#     y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "#     y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "#                                                     [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "#     y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "#     return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Otpimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                       step_size=7, \n",
    "                                       gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training phase\n",
    "This will be the training operation performed at each epoch\n",
    "\n",
    "* <b>scheduler.step()</b> will set up the scheduler for each step in order to decay the learning rate\n",
    "* <b>model.train()</b> will set the pre-trained model into training mode. This is only available for pre-trained models\n",
    "* <b>running_loss</b> will keep track of the loss at each iteration\n",
    "* <b>running_corrects</b> keeps a count of the number of correct predictions which will be used to calculate the accuracy of the model\n",
    "<br />\n",
    "* By setting <b>torch.set_grad_enabled(True)</b> we are enabling Autograd\n",
    "* <b>outputs</b> is the list probabilities for each possible label for the batch of images (which are the inputs). We use torch.max() to get the index of the highest probability label for each image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_train(model, criterion, optimizer, scheduler):\n",
    "    \n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    \n",
    "    for inputs, labels in dataloaders['train']:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    calculate_accuracy('train', running_loss, running_corrects) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the test phase\n",
    "This will be the test operation performed at each epoch\n",
    "\n",
    "* <b>model.eval()</b> will set the pre-trained model into evaluation mode. This is only available for pre-trained models\n",
    "* <b>running_loss</b> will keep track of the loss at each iteration\n",
    "* <b>running_corrects</b> keeps a count of the number of correct predictions which will be used to calculate the accuracy of the model\n",
    "<br />\n",
    "* By setting <b>torch.no_grad()</b> we are disabling Autograd\n",
    "* <b>outputs</b> is the list probabilities for each possible label for the batch of images (which are the inputs). We use torch.max() to get the index of the highest probability label for each image in the batch\n",
    "<br />\n",
    "\n",
    "Once the accuracy is calculated, we check to see if the accuracy in this has improved since the previous epoch. If not, we do not adjust the weights of the model. If so, we set the weights of the model to be the ones calculated in this epoch and return those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_test(model, criterion, optimizer):\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    global best_acc\n",
    "    \n",
    "    for inputs, labels in dataloaders['test']:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss, epoch_acc = calculate_accuracy('test', running_loss, running_corrects)\n",
    "    \n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We build upon our pre-trained model\n",
    "* we initialize the <b>best_model_wts</b> from the weights of the pre-trained model\n",
    "* perform the training and testing and update the model weights if it has supplied improved accuracy in the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        phase_train(model, criterion, optimizer, scheduler)\n",
    "        best_model_wts = phase_test(model, criterion, optimizer)\n",
    "        print()\n",
    "    \n",
    "    print('Best test Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the build_model function\n",
    "We set the number of epochs to 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(model, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    exp_lr_scheduler, \n",
    "                    num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets check how our model performs\n",
    "We will take one batch from test datasets (and we have batches of 8 images) and we will predict the correct label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    inputs, labels = iter(dataloaders['test']).next()\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0,1,1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4415,  0.4157,  0.4380,  ...,  0.2047,  0.5829,  1.1126],\n",
       "         [-0.1455,  0.2898,  0.1529,  ...,  0.0967,  0.4842,  0.5950],\n",
       "         [-0.1501,  0.2418, -0.0092,  ...,  0.1350,  0.7261,  0.8567],\n",
       "         ...,\n",
       "         [-0.4696,  0.3954,  0.2498,  ...,  0.1800,  0.4797,  0.6985],\n",
       "         [-0.2440,  0.2130, -0.0440,  ...,  0.2516,  0.2903,  0.4101],\n",
       "         [-0.4415,  0.4157,  0.4380,  ...,  0.2047,  0.5829,  1.1126]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "# # If you have a GPU, put everything on cuda\n",
    "# tokens_tensor = tokens_tensor.to('cuda')\n",
    "# segments_tensors = segments_tensors.to('cuda')\n",
    "# model.to('cuda')\n",
    "\n",
    "# # model.to('cpu')\n",
    "# # Predict hidden states features for each layer\n",
    "# with torch.no_grad():\n",
    "#     encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_train(model, criterion, optimizer, scheduler):\n",
    "    \n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    \n",
    "    for inputs, labels in dataloaders['train']:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(labels, outputs )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    calculate_accuracy('train', running_loss, running_corrects) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
