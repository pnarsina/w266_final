{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['protest'],\n",
       " 'posToken': ['VB'],\n",
       " 'nerToken': ['O'],\n",
       " 'tokenLength': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('protest', 'VB', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import config\n",
    "vocab_params = config.VocabParameters()\n",
    "training_params = config.TrainingParameters()\n",
    "eval_params = config.EvalParameters()\n",
    "\n",
    "with open(vocab_params.data_dir+ '/train.json') as infile:\n",
    "    json_data = json.load(infile)\n",
    "len(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>relation</th>\n",
       "      <th>token</th>\n",
       "      <th>subj_start</th>\n",
       "      <th>subj_end</th>\n",
       "      <th>obj_start</th>\n",
       "      <th>obj_end</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>stanford_pos</th>\n",
       "      <th>stanford_ner</th>\n",
       "      <th>stanford_head</th>\n",
       "      <th>stanford_deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42795</th>\n",
       "      <td>61b3a65fb960f284ebac</td>\n",
       "      <td>03c67d9ee4bf4ed33cbeddaa3a7b82cc</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Red, Sox, 12, ,, Athletics, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[NNP, NNP, CD, ,, NNP, CD]</td>\n",
       "      <td>[ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...</td>\n",
       "      <td>[2, 0, 2, 2, 2, 5]</td>\n",
       "      <td>[compound, ROOT, nummod, punct, appos, nummod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46757</th>\n",
       "      <td>61b3a65fb9080a05b4ee</td>\n",
       "      <td>15df2fc6a9a895432237cb2bdfcbd1b5</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Thomas, ', assertion, of, 85, %, reporters, v...</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...</td>\n",
       "      <td>[PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...</td>\n",
       "      <td>[3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...</td>\n",
       "      <td>[nmod:poss, case, nsubj, case, compound, amod,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36591</th>\n",
       "      <td>61b3a65fb9883fc52f01</td>\n",
       "      <td>274e368f381c1476fe0da7f201bfc331</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Kerry, did, his, duty, and, did, it, well, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2, 0, 4, 2, 2, 2, 6, 6, 2]</td>\n",
       "      <td>[nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22262</th>\n",
       "      <td>61b3a65fb937b50fc05a</td>\n",
       "      <td>409fa10efff702a41701bdddab89a2dd</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[This, August, ,, Moschella, 's, name, came, u...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "      <td>[DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...</td>\n",
       "      <td>[DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...</td>\n",
       "      <td>[det, ROOT, punct, nmod:poss, case, nsubj, acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>61b3a5f2e85a8088c7bb</td>\n",
       "      <td>78d7e406b6911492f6f7f122d1f112ad</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[Sharpton, is, president, of, the, National, A...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...</td>\n",
       "      <td>[3, 3, 0, 8, 8, 8, 8, 3, 3]</td>\n",
       "      <td>[nsubj, cop, ROOT, case, det, compound, compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>61b3a37935aa6ae21928</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:origin</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NATIONALITY</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32520</th>\n",
       "      <td>61b3afb926759a7aef5a</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41668</th>\n",
       "      <td>61b3afb926aa0b82acad</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62446</th>\n",
       "      <td>61b3a65fb9b37d516ec9</td>\n",
       "      <td>85b9cca690e98657db2480fb91e05489</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Washington, ,, DC, :, American, Psychiatric, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]</td>\n",
       "      <td>[LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...</td>\n",
       "      <td>[0, 1, 1, 1, 7, 7, 1, 7, 1]</td>\n",
       "      <td>[ROOT, punct, appos, punct, compound, compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66891</th>\n",
       "      <td>61b3a65fb9b8efc052b6</td>\n",
       "      <td>AFP_ENG_19941018.0328.LDC2007T07</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[President, Bill, Clinton, 's, top, defense, a...</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...</td>\n",
       "      <td>[O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...</td>\n",
       "      <td>[compound, compound, nmod:poss, case, amod, ns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                             docid  \\\n",
       "42795  61b3a65fb960f284ebac  03c67d9ee4bf4ed33cbeddaa3a7b82cc   \n",
       "46757  61b3a65fb9080a05b4ee  15df2fc6a9a895432237cb2bdfcbd1b5   \n",
       "36591  61b3a65fb9883fc52f01  274e368f381c1476fe0da7f201bfc331   \n",
       "22262  61b3a65fb937b50fc05a  409fa10efff702a41701bdddab89a2dd   \n",
       "644    61b3a5f2e85a8088c7bb  78d7e406b6911492f6f7f122d1f112ad   \n",
       "4138   61b3a37935aa6ae21928  84e924385dc7fb52b0417306a8500cb1   \n",
       "32520  61b3afb926759a7aef5a  84e924385dc7fb52b0417306a8500cb1   \n",
       "41668  61b3afb926aa0b82acad  84e924385dc7fb52b0417306a8500cb1   \n",
       "62446  61b3a65fb9b37d516ec9  85b9cca690e98657db2480fb91e05489   \n",
       "66891  61b3a65fb9b8efc052b6  AFP_ENG_19941018.0328.LDC2007T07   \n",
       "\n",
       "                        relation  \\\n",
       "42795                no_relation   \n",
       "46757                no_relation   \n",
       "36591                no_relation   \n",
       "22262                no_relation   \n",
       "644    org:top_members/employees   \n",
       "4138                  per:origin   \n",
       "32520                  per:title   \n",
       "41668                  per:title   \n",
       "62446                no_relation   \n",
       "66891                no_relation   \n",
       "\n",
       "                                                   token  subj_start  \\\n",
       "42795                    [Red, Sox, 12, ,, Athletics, 2]           0   \n",
       "46757  [Thomas, ', assertion, of, 85, %, reporters, v...          41   \n",
       "36591     [Kerry, did, his, duty, and, did, it, well, .]           2   \n",
       "22262  [This, August, ,, Moschella, 's, name, came, u...          17   \n",
       "644    [Sharpton, is, president, of, the, National, A...           5   \n",
       "4138    [She, is, an, American, actress, and, singer, .]           0   \n",
       "32520   [She, is, an, American, actress, and, singer, .]           0   \n",
       "41668   [She, is, an, American, actress, and, singer, .]           0   \n",
       "62446  [Washington, ,, DC, :, American, Psychiatric, ...           4   \n",
       "66891  [President, Bill, Clinton, 's, top, defense, a...          42   \n",
       "\n",
       "       subj_end  obj_start  obj_end     subj_type           obj_type  \\\n",
       "42795         1          4        4  ORGANIZATION       ORGANIZATION   \n",
       "46757        43         31       31  ORGANIZATION               DATE   \n",
       "36591         2          0        0        PERSON             PERSON   \n",
       "22262        20         39       40  ORGANIZATION  STATE_OR_PROVINCE   \n",
       "644           7          0        0  ORGANIZATION             PERSON   \n",
       "4138          0          3        3        PERSON        NATIONALITY   \n",
       "32520         0          6        6        PERSON              TITLE   \n",
       "41668         0          4        4        PERSON              TITLE   \n",
       "62446         6          7        7  ORGANIZATION             NUMBER   \n",
       "66891        44         39       39        PERSON             PERSON   \n",
       "\n",
       "                                            stanford_pos  \\\n",
       "42795                         [NNP, NNP, CD, ,, NNP, CD]   \n",
       "46757  [NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...   \n",
       "36591          [NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]   \n",
       "22262  [DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...   \n",
       "644             [NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]   \n",
       "4138                   [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "32520                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "41668                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "62446             [NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]   \n",
       "66891  [NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...   \n",
       "\n",
       "                                            stanford_ner  \\\n",
       "42795  [ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...   \n",
       "46757  [PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...   \n",
       "36591                   [PERSON, O, O, O, O, O, O, O, O]   \n",
       "22262  [DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...   \n",
       "644    [PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...   \n",
       "4138                         [O, O, O, MISC, O, O, O, O]   \n",
       "32520                        [O, O, O, MISC, O, O, O, O]   \n",
       "41668                        [O, O, O, MISC, O, O, O, O]   \n",
       "62446  [LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...   \n",
       "66891  [O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "                                           stanford_head  \\\n",
       "42795                                 [2, 0, 2, 2, 2, 5]   \n",
       "46757  [3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...   \n",
       "36591                        [2, 0, 4, 2, 2, 2, 6, 6, 2]   \n",
       "22262  [2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...   \n",
       "644                          [3, 3, 0, 8, 8, 8, 8, 3, 3]   \n",
       "4138                            [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "32520                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "41668                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "62446                        [0, 1, 1, 1, 7, 7, 1, 7, 1]   \n",
       "66891  [3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...   \n",
       "\n",
       "                                         stanford_deprel  \n",
       "42795     [compound, ROOT, nummod, punct, appos, nummod]  \n",
       "46757  [nmod:poss, case, nsubj, case, compound, amod,...  \n",
       "36591  [nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...  \n",
       "22262  [det, ROOT, punct, nmod:poss, case, nsubj, acl...  \n",
       "644    [nsubj, cop, ROOT, case, det, compound, compou...  \n",
       "4138      [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "32520     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "41668     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "62446  [ROOT, punct, appos, punct, compound, compound...  \n",
       "66891  [compound, compound, nmod:poss, case, amod, ns...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(json_data )\n",
    "train_df_sorted = train_df.sort_values(by=['docid','id'], ascending = True)\n",
    "train_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from above dataframe.  Each row in the dataframe represents a  \n",
    "    \n",
    "\"\"\"\n",
    "max_length = 50\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "pos_column = 'stanford_pos'\n",
    "ner_column = 'stanford_ner'\n",
    "token_column = 'token'\n",
    "\n",
    "\n",
    "for ind in train_df.index:    \n",
    "    word_list = train_df[token_column][ind]\n",
    "    ner_list = train_df[ner_column][ind]\n",
    "    pos_list = train_df[pos_column][ind]\n",
    "\n",
    "    for i in range(0,len(word_list)):\n",
    "\n",
    "        word = word_list[i]\n",
    "        ner = ner_list[i]\n",
    "        pos = pos_list[i]\n",
    "        addDict = addWord(word, pos, ner)\n",
    "    \n",
    "        sentenceTokens += addDict['wordToken']\n",
    "        posTokens += addDict['posToken']\n",
    "        nerTokens += addDict['nerToken']        \n",
    "\n",
    "#     print(sentenceTokens, posTokens, nerTokens, \"\\n\")\n",
    "    \n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "    \n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        posTokens = posTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding\n",
    "\n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "\n",
    "    posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    sentenceList.append(sentence)\n",
    "\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    posTokenList.append(posTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    sentence = ''\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    posTokens = ['[posCLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    sentence += ' ' + word\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "sentLengthList = sentLengthList[2:]\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "posTokenList = posTokenList[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 1.8000e+01, 1.5000e+01, 4.1000e+01, 5.2000e+01,\n",
       "        8.3000e+01, 1.0400e+02, 1.8400e+02, 1.8300e+02, 2.2500e+02,\n",
       "        2.8800e+02, 2.8300e+02, 3.0700e+02, 4.2600e+02, 4.4500e+02,\n",
       "        0.0000e+00, 4.9200e+02, 5.8300e+02, 6.6000e+02, 6.8500e+02,\n",
       "        6.7700e+02, 7.3600e+02, 9.6100e+02, 9.5000e+02, 1.0360e+03,\n",
       "        1.1930e+03, 1.2540e+03, 1.3570e+03, 1.3160e+03, 1.3940e+03,\n",
       "        1.4160e+03, 0.0000e+00, 1.4130e+03, 1.5450e+03, 1.4980e+03,\n",
       "        1.5340e+03, 1.6630e+03, 1.5620e+03, 1.5780e+03, 1.7290e+03,\n",
       "        1.5580e+03, 1.5800e+03, 1.5840e+03, 1.5740e+03, 1.4380e+03,\n",
       "        1.3910e+03, 1.4520e+03, 2.7658e+04]),\n",
       " array([ 4.    ,  4.9375,  5.875 ,  6.8125,  7.75  ,  8.6875,  9.625 ,\n",
       "        10.5625, 11.5   , 12.4375, 13.375 , 14.3125, 15.25  , 16.1875,\n",
       "        17.125 , 18.0625, 19.    , 19.9375, 20.875 , 21.8125, 22.75  ,\n",
       "        23.6875, 24.625 , 25.5625, 26.5   , 27.4375, 28.375 , 29.3125,\n",
       "        30.25  , 31.1875, 32.125 , 33.0625, 34.    , 34.9375, 35.875 ,\n",
       "        36.8125, 37.75  , 38.6875, 39.625 , 40.5625, 41.5   , 42.4375,\n",
       "        43.375 , 44.3125, 45.25  , 46.1875, 47.125 , 48.0625, 49.    ]),\n",
       " <a list of 48 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nerCLS]\n",
      "O\n",
      "ORGANIZATION\n",
      "nerX\n",
      "MISC\n",
      "[nerSEP]\n",
      "[nerPAD]\n",
      "DATE\n",
      "PERSON\n",
      "NUMBER\n",
      "LOCATION\n",
      "ORDINAL\n",
      "DURATION\n",
      "SET\n",
      "MONEY\n",
      "TIME\n",
      "PERCENT\n"
     ]
    }
   ],
   "source": [
    "x =nerClasses['tag'].unique()\n",
    "x\n",
    "for a in x:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405474</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405475</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405476</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405953</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405965</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12584 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  cat  sym\n",
       "526      DURATION    1    1\n",
       "718      DURATION    1    1\n",
       "719      DURATION    1    1\n",
       "831      DURATION    1    1\n",
       "834      DURATION    1    1\n",
       "...           ...  ...  ...\n",
       "3405474  DURATION    1    1\n",
       "3405475  DURATION    1    1\n",
       "3405476  DURATION    1    1\n",
       "3405953  DURATION    1    1\n",
       "3405965  DURATION    1    1\n",
       "\n",
       "[12584 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses[nerClasses['cat']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, '[nerCLS]', 6, 'O', 8, 'ORGANIZATION', 16, 'nerX', 3, 'MISC',\n",
       "       15, '[nerSEP]', 14, '[nerPAD]', 0, 'DATE', 10, 'PERSON', 5,\n",
       "       'NUMBER', 2, 'LOCATION', 7, 'ORDINAL', 1, 'DURATION', 11, 'SET', 4,\n",
       "       'MONEY', 12, 'TIME', 9, 'PERCENT'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values = nerClasses[[\"cat\", \"tag\"]].values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000024B30595D90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXWklEQVR4nO3df5DcdX3H8eerCYwhh8E0ZotJbKKNWCTQmhWsP+oeaHtga/oHRiJFY6E3dAzVTlTi6Oh0nKlpKRYcwfQG09RWuWkx1QhRpMgVrUYxVAmBQjNA8QISAY29aKWH7/6x39Nls3f73b398b1PXo+Zm9x3v5/P7ovd5bXf++5+96uIwMzM5r5f6ncAMzPrDBe6mVkiXOhmZolwoZuZJcKFbmaWCBe6mVkiXOhmZolwoZvlJOkhSa/tdw6z6bjQzcwS4UK3Y5KkFZJ2Svq+pCckfUzSCyV9OVt+XNKnJJ2Ujf8H4PnA5yVNSHpPf/8LzI4mH/pvxxpJ84A7gS8D7weeBsrA94BVwO3As4HPAHdGxDuzeQ8Bl0TEv/YhtllTfd1Cl7Rd0iFJd+ccv17SPZL2S/p0t/NZss4Enge8OyKORMT/RsRXI+JARNwSET+NiO8DHwFe09+oZvnN7/Pt7wA+Bnyy2UBJq4H3Aq+MiB9IWtrlbJauFcB/R8Rk7YXZc+qjwKuBE6lu8Pyg9/HM2tPXLfSIuB14svaybD/mFyXtlfQVSS/OVv0xcE1E/CCbe6jHcS0d3wWeL6l+g+bDQACnR8SzgT8EVLPe+yet0Ir4pugIcFlErAXeBVybXf4i4EWS/l3SHklDfUtoc903gUeBrZIWSnqWpFdS3SqfAH4oaRnw7rp5jwEv6G1Us/wKVeiSBoBXAP8s6dvA3wInZ6vnA6uBCrABuG7qEwhmrYiIp4HfB34NeBgYB94E/DnwUuAwcBOws27qh4H3S/qhpHf1LrFZPn3/lIuklcCNEXGapGcD90XEyQ3GbQP2RMSObPlWYEtE3NHDuGZmhVWoLfSI+BHwoKQ3AqjqjGz1Z4HB7PIlVHfBPNCXoGZmBdTvjy1eD3wdOEXSuKSLgQuBiyV9B9gPrMuG3ww8Ieke4DaqHzl7oh+5zcyKqO+7XMzMrDMKtcvFzMza17cDi5YsWRIrV65sa+6RI0dYuHBhZwN1QFFzQXGzOVdrnKs1Kebau3fv4xHx3IYrI6IvP2vXro123XbbbW3P7aai5ooobjbnao1ztSbFXMC3Yppe9S4XM7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLRtNDznFVIUkXSt7MzCf1bZyOamVkeebbQdwDTfvd49hW21wJviIiXAG/sTDQzM2tF00KPBmcVqvNmYGdEPJyN95mEzMz6INeXc9V+Z3mDdVcBxwEvoXrGl6sjouE5QiUNA8MApVJp7ejoaFuhJyYmGBgYaGtuNxU1F/Qm276Dh1ueU1oASxcv6kKa2SnqY+lcrUkx1+Dg4N6IKDda14nvcpkPrAXOARYAX5e0JyLurx8YESNUTzFHuVyOSqXS1g2OjY3R7txuKmou6E22jVtuannO5jWTrC/gfVbUx9K5WnOs5epEoY8Dj0fEEeCIpNuBM4CjCt3MzLqnEx9b/BzwaknzJZ0AnAXc24HrNTOzFjTdQs/OKlQBlkgaBz5IdZ85EbEtIu6V9EXgLuBnwHURMe1HHM3MrDuaFnpEbMgx5grgio4kMjOztvhIUTOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRDQtdEnbJR2SNONp5SS9TNLTks7vXDwzM8srzxb6DmBopgGS5gF/CdzcgUxmZtaGpoUeEbcDTzYZdhnwGeBQJ0KZmVnrFBHNB0krgRsj4rQG65YBnwbOBj6RjbthmusZBoYBSqXS2tHR0bZCT0xMMDAw0NbcbipqLuhNtn0HD7c8p7QAli5e1IU0s1PUx9K5WpNirsHBwb0RUW60bv6sUlVdBVweEU9LmnFgRIwAIwDlcjkqlUpbNzg2Nka7c7upqLmgN9k2brmp5Tmb10yyvoD3WVEfS+dqzbGWqxOFXgZGszJfApwnaTIiPtuB6zYzs5xmXegRsWrqd0k7qO5ycZmbmfVY00KXdD1QAZZIGgc+CBwHEBHbuprOzMxya1roEbEh75VFxMZZpTEzs7b5SFEzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRTQtd0nZJhyTdPc36CyXdlf18TdIZnY9pZmbN5NlC3wEMzbD+QeA1EXE68CFgpAO5zMysRXlOQXe7pJUzrP9azeIeYPnsY5mZWas6vQ/9YuALHb5OMzPLQRHRfFB1C/3GiDhthjGDwLXAqyLiiWnGDAPDAKVSae3o6GgbkWFiYoKBgYG25nZTUXNBb7LtO3i45TmlBbB08aIupJmdoj6WztWaFHMNDg7ujYhyo3VNd7nkIel04Drg3OnKHCAiRsj2sZfL5ahUKm3d3tjYGO3O7aai5oLeZNu45aaW52xeM8n6At5nRX0snas1x1quWe9ykfR8YCdwUUTcP/tIZmbWjqZb6JKuByrAEknjwAeB4wAiYhvwAeCXgWslAUxO9+eAmZl1T55PuWxosv4S4JKOJTIzs7b4SFEzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0Q0LXRJ2yUdknT3NOsl6aOSDki6S9JLOx/TzMyaybOFvgMYmmH9ucDq7GcY+PjsY5mZWauaFnpE3A48OcOQdcAno2oPcJKkkzsV0MzM8lFENB8krQRujIjTGqy7EdgaEV/Nlm8FLo+IbzUYO0x1K55SqbR2dHS0rdATExMMDAy0NbebipoLepNt38HDLc8pLYClixd1Ic3sFPWxdK7WpJhrcHBwb0SUG62bP6tUVWpwWcNXiYgYAUYAyuVyVCqVtm5wbGyMdud2U1FzQW+ybdxyU8tzNq+ZZH0B77OiPpbO1ZpjLVcnPuUyDqyoWV4OPNKB6zUzsxZ0otB3AW/JPu3ycuBwRDzages1M7MWNN3lIul6oAIskTQOfBA4DiAitgG7gfOAA8CPgbd1K6yZmU2vaaFHxIYm6wN4e8cSmZlZW3ykqJlZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZInIVuqQhSfdJOiBpS4P1iyR9XtJ3JO2X5NPQmZn1WNNClzQPuAY4FzgV2CDp1LphbwfuiYgzqJ5/9EpJx3c4q5mZzSDPFvqZwIGIeCAingJGgXV1YwI4UZKAAeBJYLKjSc3MbEaqnuN5hgHS+cBQRFySLV8EnBURm2rGnAjsAl4MnAi8KSJuanBdw8AwQKlUWjs6OtpW6ImJCQYGBtqa201FzQW9ybbv4OGW55QWwNLFi7qQZnaK+lg6V2tSzDU4OLg3IsqN1s3PMV8NLqt/Ffhd4NvA2cALgVskfSUifvSMSREjwAhAuVyOSqWS4+aPNjY2Rrtzu6mouaA32TZuOeo1vKnNayZZX8D7rKiPpXO15ljLlWeXyziwomZ5OfBI3Zi3ATuj6gDwINWtdTMz65E8hX4HsFrSquyNzguo7l6p9TBwDoCkEnAK8EAng5qZ2cya7nKJiElJm4CbgXnA9ojYL+nSbP024EPADkn7qO6iuTwiHu9ibjMzq5NnHzoRsRvYXXfZtprfHwF+p7PRzMysFT5S1MwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsES50M7NEuNDNzBLhQjczS4QL3cwsEbnOWCRpCLia6inorouIrQ3GVICrgOOAxyPiNR3MaWY2rZVbbmp4+eY1k2ycZt2Uh7a+vhuR+qJpoUuaB1wDvA4YB+6QtCsi7qkZcxJwLTAUEQ9LWtqtwGZm1lieXS5nAgci4oGIeAoYBdbVjXkzsDMiHgaIiEOdjWlmZs0oImYeIJ1Pdcv7kmz5IuCsiNhUM2ZqV8tLgBOBqyPikw2uaxgYBiiVSmtHR0fbCj0xMcHAwEBbc7upqLmgN9n2HTzc8pzSAli6eFEX0sxOUR9L52psuudeaQE89pOZ565Z1vvn32zur8HBwb0RUW60Ls8+dDW4rP5VYD6wFjgHWAB8XdKeiLj/GZMiRoARgHK5HJVKJcfNH21sbIx253ZTUXNBb7I121fZyOY1k6wv4H1W1MfSuRqb7rm3ec0kV+6bueYeurDShUQz69b9lafQx4EVNcvLgUcajHk8Io4ARyTdDpwB3I+ZmfVEnn3odwCrJa2SdDxwAbCrbszngFdLmi/pBOAs4N7ORjUzs5k03UKPiElJm4CbqX5scXtE7Jd0abZ+W0TcK+mLwF3Az6h+tPHubgY3M7NnyvU59IjYDeyuu2xb3fIVwBWdi2ZmZq3wkaJmZolwoZuZJSLXLhczM3um6b5uII8dQws7mOQXvIVuZpYIF7qZWSJc6GZmiXChm5klwoVuZpYIF7qZWSJc6GZmiXChm5klwoVuZpYIF7qZWSJc6GZmiXChm5klwoVuZpYIF7qZWSJyFbqkIUn3STogacsM414m6WlJ53cuopmZ5dG00CXNA64BzgVOBTZIOnWacX9J9dyjZmbWY3m20M8EDkTEAxHxFDAKrGsw7jLgM8ChDuYzM7OcFBEzD6juPhmKiEuy5YuAsyJiU82YZcCngbOBTwA3RsQNDa5rGBgGKJVKa0dHR9sKPTExwcDAQFtzu6mouaA32fYdPNzynNICWLp4URfSzE5RH0vnamy6515pATz2k5nnrlnW3vOvnef7lFWL5rV9fw0ODu6NiHKjdXlOQacGl9W/ClwFXB4RT0uNhmeTIkaAEYByuRyVSiXHzR9tbGyMdud2U1FzQW+ybWzjlFyb10yyvoD3WVEfS+dqbLrn3uY1k1y5b+aae+jCSkdvM48dQwu7cn/lKfRxYEXN8nLgkboxZWA0K/MlwHmSJiPisx1JaWZmTeUp9DuA1ZJWAQeBC4A31w6IiFVTv0vaQXWXi8vczKyHmhZ6RExK2kT10yvzgO0RsV/Spdn6bV3OaGZmOeTZQicidgO76y5rWOQRsXH2sczMrFU+UtTMLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBEudDOzRLjQzcwS4UI3M0uEC93MLBG5Cl3SkKT7JB2QtKXB+gsl3ZX9fE3SGZ2PamZmM2la6JLmAdcA5wKnAhsknVo37EHgNRFxOvAhYKTTQc3MbGZ5ttDPBA5ExAMR8RQwCqyrHRARX4uIH2SLe4DlnY1pZmbNKCJmHiCdDwxFxCXZ8kXAWRGxaZrx7wJePDW+bt0wMAxQKpXWjo6OthV6YmKCgYGBtuZ2U1FzQW+y7Tt4uOU5pQWwdPGiLqSZnaI+ls7V2HTPvdICeOwnM89ds6y95187z/cpqxbNa/v+Ghwc3BsR5Ubr5ueYrwaXNXwVkDQIXAy8qtH6iBgh2x1TLpejUqnkuPmjjY2N0e7cbipqLuhNto1bbmp5zuY1k6wv4H1W1MfSuRqb7rm3ec0kV+6bueYeurDS0dvMY8fQwq7cX3kKfRxYUbO8HHikfpCk04HrgHMj4onOxDMzs7zy7EO/A1gtaZWk44ELgF21AyQ9H9gJXBQR93c+ppmZNdN0Cz0iJiVtAm4G5gHbI2K/pEuz9duADwC/DFwrCWByun08ZmbWHXl2uRARu4HddZdtq/n9EuCoN0HNzKx3chW6mfXXyuwNuM1rJlt6M+6hra/vViQrIB/6b2aWCG+h2zFpZZOt3Jm2hL3Va0XlLXQzs0S40M3MEuFCNzNLhAvdzCwRLnQzs0S40M3MEuFCNzNLhAvdzCwRPrDIzAqj2QFfqdxmt8zJQt938HDbXy7f7lF+eR706Y4u9JGFZtYL3uViZpYIF7qZWSJc6GZmiZiT+9Cte1J6g8jsWJOr0CUNAVdTPQXddRGxtW69svXnAT8GNkbEnR3OOme1W5J+M9XMWtG00CXNA64BXgeMA3dI2hUR99QMOxdYnf2cBXw8+7dwvAVqZqnKs4V+JnAgIh4AkDQKrANqC30d8MmICGCPpJMknRwRj3Y8seVS/8LV6qnL5oq59LnlufYXVyv/nbXPr7n235kSVTt4hgHS+cBQdiJoJF0EnBURm2rG3AhsjYivZsu3ApdHxLfqrmsYGM4WTwHuazP3EuDxNud2U1FzQXGzOVdrnKs1Keb61Yh4bqMVebbQ1eCy+leBPGOIiBFgJMdtzhxI+lZElGd7PZ1W1FxQ3GzO1Rrnas2xlivPxxbHgRU1y8uBR9oYY2ZmXZSn0O8AVktaJel44AJgV92YXcBbVPVy4LD3n5uZ9VbTXS4RMSlpE3Az1Y8tbo+I/ZIuzdZvA3ZT/cjiAaofW3xb9yIDHdht0yVFzQXFzeZcrXGu1hxTuZq+KWpmZnODD/03M0uEC93MLBFzrtAlDUm6T9IBSVv6nQdA0gpJt0m6V9J+Se/od6ZakuZJ+o/seIFCyA4+u0HSf2b322/1OxOApD/LHsO7JV0v6Vl9yrFd0iFJd9dctljSLZL+K/v3OQXJdUX2ON4l6V8knVSEXDXr3iUpJC3pda6Zskm6LOuy/ZL+qhO3NacKveZrCM4FTgU2SDq1v6kAmAQ2R8SvAy8H3l6QXFPeAdzb7xB1rga+GBEvBs6gAPkkLQP+FChHxGlUPwRwQZ/i7ACG6i7bAtwaEauBW7PlXtvB0bluAU6LiNOB+4H39joUjXMhaQXVry15uNeBauygLpukQapH2J8eES8B/roTNzSnCp2aryGIiKeAqa8h6KuIeHTqy8gi4n+oltOy/qaqkrQceD1wXb+zTJH0bOC3gU8ARMRTEfHD/qb6ufnAAknzgRPo0/EUEXE78GTdxeuAv89+/3vgD3oaisa5IuJLETGZLe6hehxK33Nl/gZ4Dw0OdOyVabL9CdWj63+ajTnUiduaa4W+DPhuzfI4BSnOKZJWAr8JfKO/SX7uKqpP6J/1O0iNFwDfB/4u2xV0naSF/Q4VEQepbik9DDxK9XiKL/U31TOUpo7vyP5d2uc8jfwR8IV+hwCQ9AbgYER8p99ZGngR8GpJ35D0b5Je1okrnWuFnusrBvpF0gDwGeCdEfGjAuT5PeBQROztd5Y684GXAh+PiN8EjtCf3QfPkO2TXgesAp4HLJT0h/1NNXdIeh/V3Y+fKkCWE4D3AR/od5ZpzAeeQ3UX7buBf8q+hnxW5lqhF/YrBiQdR7XMPxURO/udJ/NK4A2SHqK6e+psSf/Y30hA9XEcj4ipv2JuoFrw/fZa4MGI+H5E/B+wE3hFnzPVekzSyQDZvx35M70TJL0V+D3gwijGwS0vpPrC/J3s+b8cuFPSr/Q11S+MAzuj6ptU/4Ke9Zu2c63Q83wNQc9lr6yfAO6NiI/0O8+UiHhvRCyPiJVU76svR0Tftzgj4nvAdyWdkl10Ds/8OuZ+eRh4uaQTssf0HArwZm2NXcBbs9/fCnyuj1l+LjsBzuXAGyLix/3OAxAR+yJiaUSszJ7/48BLs+deEXwWOBtA0ouA4+nAt0LOqULP3niZ+hqCe4F/ioj9/U0FVLeEL6K6Bfzt7Oe8focquMuAT0m6C/gN4C/6nIfsL4YbgDuBfVT//+jLoeOSrge+DpwiaVzSxcBW4HWS/ovqJze2znQdPcz1MeBE4Jbsub+tILkKYZps24EXZB9lHAXe2om/bHzov5lZIubUFrqZmU3PhW5mlggXuplZIlzoZmaJcKGbmSXChW5mlggXuplZIv4fW35nOINOkxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  6,  8, 16,  3, 15, 14,  0, 10,  5,  2,  7,  1, 11,  4, 12,  9],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses['cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>O</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>O</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tag  cat  occurences\n",
       "102   O    0           0\n",
       "103   O    1           0\n",
       "104   O    2           0\n",
       "105   O    3           0\n",
       "106   O    4           0\n",
       "107   O    5           0\n",
       "108   O    6     1600623\n",
       "109   O    7           0\n",
       "110   O    8           0\n",
       "111   O    9           0\n",
       "112   O   10           0\n",
       "113   O   11           0\n",
       "114   O   12           0\n",
       "115   O   13           0\n",
       "116   O   14           0\n",
       "117   O   15           0\n",
       "118   O   16           0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution[nerDistribution['tag'] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>58622</td>\n",
       "      <td>58622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DURATION</th>\n",
       "      <td>12584</td>\n",
       "      <td>12584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>61239</td>\n",
       "      <td>61239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>25240</td>\n",
       "      <td>25240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONEY</th>\n",
       "      <td>9224</td>\n",
       "      <td>9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMBER</th>\n",
       "      <td>31327</td>\n",
       "      <td>31327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>1600623</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDINAL</th>\n",
       "      <td>4689</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>140654</td>\n",
       "      <td>140654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT</th>\n",
       "      <td>5065</td>\n",
       "      <td>5065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>152696</td>\n",
       "      <td>152696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SET</th>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerCLS]</th>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerPAD]</th>\n",
       "      <td>623605</td>\n",
       "      <td>623605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerSEP]</th>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerX</th>\n",
       "      <td>539711</td>\n",
       "      <td>539711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cat      sym\n",
       "tag                           \n",
       "DATE            58622    58622\n",
       "DURATION        12584    12584\n",
       "LOCATION        61239    61239\n",
       "MISC            25240    25240\n",
       "MONEY            9224     9224\n",
       "NUMBER          31327    31327\n",
       "O             1600623  1600623\n",
       "ORDINAL          4689     4689\n",
       "ORGANIZATION   140654   140654\n",
       "PERCENT          5065     5065\n",
       "PERSON         152696   152696\n",
       "SET              1101     1101\n",
       "TIME             3476     3476\n",
       "[nerCLS]        68122    68122\n",
       "[nerPAD]       623605   623605\n",
       "[nerSEP]        68122    68122\n",
       "nerX           539711   539711"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses.groupby(\"tag\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now split into Train,Test and Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 47758, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       ...,\n",
       "       [13,  6,  6, ..., 16, 15, 14],\n",
       "       [13, 10, 16, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14]], dtype=int8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerLabels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699283638178562"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = nerDistribution[nerDistribution['tag'] == 'O']['occurences'].sum()\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 17]['occurences'].sum()\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [bert_inputs_train_k, labels_train_k]\n",
    "test_all = [bert_inputs_test_k, labels_test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_train_k, labels_train_k = train_all = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_test_k, labels_test_k = test_all = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 10,  6,  2,  2,  6,  6, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.1971407, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing customer loss function\n",
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "# Nice to have eager execution now...\n",
    "print(custom_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47758, 50)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13  )\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "print(custom_acc_orig_tokens(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 68122, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer, numclasses):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(numclasses, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. Model Runs/Experiments<a id=\"runs\"/>\n",
    "\n",
    "### V.1. With BERT-Layer Re-Training<a id=\"retrain\"/>\n",
    "\n",
    "It is time to run the first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_train_k[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: Tensor(\"tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0\", shape=(None, 50, 768), dtype=float32)\n",
      "pred:  Tensor(\"ner/truediv:0\", shape=(None, 50, 17), dtype=float32)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 50, 768), (N 108310272   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      196864      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,511,505\n",
      "Trainable params: 108,511,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 50, 768), (N 108310272   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      196864      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,511,505\n",
      "Trainable params: 108,511,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# retrain all layers\n",
    "numclasses = 17\n",
    "model = ner_model(max_length + 1, train_layers=-1, optimizer = adam_customized, numclasses = numclasses)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo0AAAHBCAYAAAAM34HHAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deXxcZb3H8e/JUkCQFoFU6AbY5UKRggj0BVXoJnLrhCK0NElLLbQwUXaLskwuYEFAE60IpE2qqL1pYiOCiRZFGqRY2oLFFMGuFiYt1QwKM1cEoctz/6hnmKzPzCSZMzP5vF+vebU5c5bfWZ6Zb845z4ljjDECAAAAupHjdQEAAABIf4RGAAAAWBEaAQAAYEVoBAAAgFVe+wF/+9vfdNNNN2n//v1e1AP0idzcXH33u9/Vxz/+8T6ZP+0GSN6cOXPk8/n6bP633367duzY0WfzB7JRZ+2yw5nGpqYm1dXVpawoIBXq6urU1NTUZ/On3QDJqa+v7/O2c99996m+vr5PlwFkk67aZYczja6VK1f2aUFAKjmOk5Ll0G6AxJSUlKRkOTU1NSouLk7JsoBM11W75J5GAAAAWBEaAQAAYEVoBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYNUrobGsrExlZWW9MauUi6f2UCikuro6FRYWpqgq9AeZ3G68QDsEvMd3Zv+W53UBvSESiWjQoEEyxvTJ/O+8804tWbKkV+fpOE6nw/tqHbrTfvulU23oO33dbnrb/Pnz1djY2OfLSafjn7aJvsB3Zs/053bZK6Fx0aJFvTGbpK1ZsybpaeOpvbKystcbgDEmeuBJUjgc1sCBA3t1GfFqv/2MMQqFQho8eLAkb2vLZpncbrzQ0NDQ5Ydzb6JtItvxndkz/bldZvw9jZFIRNXV1V6XkZTYg8qrA6yr7VdQUBD9f7Ye/P1ZJrebVKBtIltlctunXXqvx6Gx/b0L7X9ubGyU4zgqLCxUS0tLdJzGxsboONXV1XIcR6Wlpdq2bVt03o7jRF9dDSsvL49esmo/bqK1uyKRiOrq6qJ1x9YUq6KiQo7jqLq6WqFQqM2yk71fLZO2n8ttRO70ZWVlCoVC0e3jvioqKqLTxL4Xu17u8MLCQjU1NXVY30gkotLS0oy/FzCb2o1ba2lpabRWt/3EDpO6PlZiddeu2mtqaup0fWmbB9E2k2c7DrvaJq6mpiYVFhZGt2/scd6TNtTdsuM5RvnOpF32iGmnpqbGdDK4Sz6fz0iKThP787p164wxxgSDQSPJ+P1+Yw5e5O8wTjgcNn6/30gyW7duNcYY09ra2mbesfOKHdb+52Rrjx3u9/tNOBw2xhhTW1vbYbzy8nITDAajtQcCgTbvBwIBEwgErDW0n286bb94t6u73NbW1g61rlu3rs3PsXw+n2ltbY3W6vP5TG1trTHGmNWrVxtJprm5ucM2aW5u7nR+3ZFkampqEpomEf213TQ3Nxtj2u7nruo3pvtjxRh7u+psvaqqqqLHUTzzoG2mV9ssLi42xcXFCU2TqETbv+0Y6m6bGGNMQ0NDm2Mg9jtEUo/aUCL7o6t58J1Ju7Tpql32ODQa03FDdbbh4hmnubnZSDLl5eU9nleytbuN3T2IjDl4cHW2zNgvKvdg6+ny4x2Wiu0X73YNBALdfiCVl5cbSdEPDLdW92A35sMPmfbLdz9E3Hm6H0qJSvRLI1H9vd3EO8x2rNjaVez47Y+heOcRL9rmQX3dNtMxNNqOoXi3Sfv3E93/nQ1Ldtl8Z9IuE5ERobG355VM7e5vAPGOV1tbm/RO6Wy+8Q5LxfZLdLsGg8HowR47ndswq6qqosNif+s0pu1vi+1fydTS2bpka2js7XklU3siw4zp+lixtSt3/HXr1nX5mzNts6N0bpvpGBptx5Btm3T2PZLM/u9sWDL7g+9M2mWiCI09qL2z4Vu3bm2z02J/U+nJ8uMdlm4NoKqqyvh8PrN169ZOp3M/MMLhcPSyQCLL6mkDSPRLI1H9vd0kMqy7Y8XWrtzh7m/Z7qWmWLTNttK9baZjaIz3OOyK+6XvnhlK9oxWZ8OS2R98Z9IuE5VRodF270U6NACXe69Aso2gLxpAb20/23Z1l+N+gbu/BXU2XeyHaENDQ4cve3ea2EscidRik+iXRqL6e7uJd1g8x4oxXber2PHde6JiL3nFM4++XkfaZmLSMTS6bMdhV9vEmIOXbd2zSLH3nrWfR6LDktkffGfSLhOVEaHRTdwNDQ09nleytVdVVRnpw5uTu1tm7Cl2dwf3dPnxDkvF9utuu65bty76IRjv/NwPCp/P1+E9d7sHAoHodm1tbY1+qPS0AST7pRGv/t5ukl1mou0qdvxwOBy9Ab/9MmmbmdM20zE02o4h2zZpaGiwXoJN9vhKZn/wnUm7TFSfhcbY3katra1tfnZXJPamWPesgPuzuxHd3lTtN0773k1uzyLpw98O3FPesRssmdqN+bCnlM/ni/4W4PZKil2mu7Pccdx7E1zx9ASL3S6xOz0dtl9nvchc7jzcDwl3+mAw2OZUe/szQO50sfdpdLYvYl/BYLDbWuKV6JdGovpru+nsuI3t3dd+mO1Y6a5ddbZct73GHlO0zcxqm+kaGrs7hrrbJu70nb38fn+X7T3eNhTv/ujuGOU7k3Zp02ehsavGEVtwd8Niu4dXVVV1+O0sGAxG33d/G3BP9bsb2P2NJRAIdHmpKp7aY5fpHjhuI2+/zNgDRup4mt3WAGzbzcvtF29t7rLaT+/2DIu9adfl3sPRmWAwGL3kGDt97DI7+40rHol+aSSqv7ebeIfZjpXu2lVn84v9copdLm0zc9pmuobG7o4hY7reJsaYDo89iX253y3JtqHulp1sO0xku3RVD9+Z/aNdOv9ZQNSKFStUUlKidoN7nftAzL5eTrbKxO0XiUR06623qrKyMuXLdhxHNTU1Ki4u7pP5027gysR95GXbLCkpkSTV1NT02TL6uv23t23bNh166KEaPnx4h+FjxozJqGMjW9AuE9NVu8z4PyOIzLFy5UrNmDHD6zIAtEPb7D11dXUaPXp0h8AoSYMHD1Ztba0HVSETpWO79CQ0tv9zSkhMJm2/srKyNn/6aNKkSV6XlLEyab/3V5m0j2ibfWPFihWqrq7u8Kf/tm3bppUrV2rWrFkeVdZ/0S57jyehcfDgwZ3+v7fE/u3G7l6Zqq+3X29yf9uuqqrSokWLPK4ms9Fu0h9tE8uXL9dHP/pR3XfffW3+tvDu3bu1YMECr8vrVLa3fdpl78nzYqF9fU9BJt2zkIxMWr8FCxak7QdlpqHdpL9M2oa0zb4xcOBAzZo1S7NmzfLkXrRkZNJxm4xMWr90b5fc0wgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAACrvK7emDlzZirrQJYIhULKycnRMccc43UpnqDdAImpr69XcXFxny+npKRETzzxRJ8vpze9//77amlp0ciRI+U4jtfloB/pql3m3nXXXXfFDjj22GP1xhtvyBiTqtqQRV555RVt2rRJf/3rX5Wbm6sjjzwyLT7sTjvtNJWWluqII47ok/nTbtLbmjVrdNhhh+nwww/3uhS0M3bsWJWUlGjMmDF9towPPvhAxx13XJ/Nv7e9+eab+tOf/qSNGzfqH//4h4YMGaJDDjnE67LQj3TVLh3Dtxx62fr167V48WL9/Oc/19FHH61rrrlGfr9fH//4x70uDf2U4ziqqalJyRktIBmRSETLly/XkiVL9Oqrr+rss8+W3+/XrFmzdNhhh3ldHiCJexrRB8aPH6+6ujq99tprmjdvnh5++GGNGDFCV1xxhf7whz94XR4ApI0//vGPuuaaazR06FDdeuutGj9+vP7whz9ow4YNmjdvHoERaYXQiD4zZMgQffOb39SuXbv0yCOPaNOmTTrrrLM0YcIErVy5Uvv27fO6RABIuX//+9/6yU9+ovHjx+tTn/qUnnvuOd1777164403tGzZMp155plelwh0itCIPnfooYfqqquu0qZNm9TU1KRjjz1WxcXFOvHEE3X//ffrH//4h9clAkCf2759uxYuXKghQ4Zo/vz5OuGEE/TMM8/o1Vdf1fXXX6+BAwd6XSLQLUIjUmrixIl6/PHHtX37ds2cOVMPPPCAhg0bpquvvlqvvPKK1+UBQK/at2+fHn/8cV144YUaM2aM6uvrdfPNN6ulpUV1dXW64IIL0qKzIBAPQiM8ceKJJ6qiokK7du3St7/9ba1Zs0af/OQnNWXKFP3iF7/QgQMHvC4RAJK2Z88e3X333TrhhBN02WWXKScnR7/4xS+0c+dO3XHHHXQMREYiNMJTRxxxhL7yla9o8+bNevLJJ5Wfn69LLrlEo0eP1ne/+11FIhGvSwSAuBhj9PTTT+vSSy/VCSecoIcfflizZ8/W9u3b9eSTT8rn8yk3N9frMoGkERqRFhzH0ec//3k9+eST2rx5sy688EL9z//8j4YNG6brrrtO27Zt87pEAOjUW2+9pe985zv6r//6L02dOlWhUEiPPvqodu3apfvvv18nnXSS1yUCvYLQiLQzZswYPfzww9q1a5fuuusu/epXv9LJJ5+sadOm6amnnuIB2gDSwgsvvKB58+Zp6NChuuuuuzR58mS9/PLLeu6551RSUsIDuZF1CI1IW4MGDdLNN9+s7du367HHHtO7776rCy+8UKeccoqWLFmif/3rX16XCKCf+de//qVly5bpU5/6lM455xz98Y9/1He+8x3t2bNHjzzyiD75yU96XSLQZwiNSHu5ubmaPn26nnnmGW3atEnnnXeebrrpJg0dOlRf+9rX9Prrr3tdIoAs9+c//1k33HCDhgwZomuvvVZjx47V73//ezU3N8vv9/fZnygF0gmhERnltNNO07Jly7Rr1y7dcsstWrFihUaOHKlLL71Uzz77rNflAcgie/fu1U9/+lNNnDhRY8eO1S9/+Uvdcccd2r17t5YvX67zzjvP6xKBlCI0IiMdc8wxuv322/Xaa6+ppqZGe/bs0QUXXKAzzjhDjz76qN5//32vSwSQoYLBoO644w4NGzZMJSUlGjhwoH79619r+/btuuWWW3TMMcd4XSLgCUIjMlp+fr4uv/xyrVu3Ti+88IJOOeUU+f1+DRs2TGVlZdqzZ4/XJQLIAAcOHNCqVatUWFioT3ziE3r00Ue1YMECvfbaa3riiSd04YUXKieHr0z0b7QAZI2zzjpLNTU1ev311+X3+1VVVaUTTjhBJSUl2rBhg9flAUhDoVBIDzzwgEaOHKkvfOELeuedd1RbW6tgMKhFixZp2LBhXpcIpA1CI7LOcccdp2984xtqaWlRVVWVNm/erPHjx2v8+PGqra3V3r17vS4RgMd+//vfq7i4WMOHD9f999+vwsJCbd68WU1NTZoxY4by8/O9LhFIO4RGZK1DDjlEX/rSl/TSSy/p2Wef1bBhwzRnzhydcMIJuvfee/Xmm296XSKAFPq///s/PfLIIzrttNP0mc98Rjt27NDDDz+sN954Q4sXL9aYMWO8LhFIa4RG9Auf/exnVV9fr7/85S+aPXu2KioqNHz4cF155ZXatGmT1+UB6EObNm1SaWmphgwZooULF+qss87SCy+8oBdeeEFXXXWVPvKRj3hdIpARCI3oV0aMGKEHHnhAu3fv1uLFi7VhwwadfvrpuuCCC/T4449r//79XpcIoBe8//77Wr58uc4991ydfvrpeuaZZ3TPPfdoz549+sEPfqCzzjrL6xKBjENoRL/0kY98RNdcc41eeeUVPfXUUzriiCN02WWXaeTIkSovL1c4HPa6RABJ2LFjh2655RYNHTpUV111lYYOHarVq1dr8+bNuuGGGzRo0CCvSwQyFqER/ZrjOJo6dap++ctfasuWLSosLNQ3vvENDR06VF/+8pe1ZcsWr0sEYLF//3498cQT+vznP68xY8bopz/9qa6//noFg0GtXLlSkyZNkuM4XpcJZDxCI/Afo0aN0ve+9z3t3r1b9957r5566imdcsopuvDCC/Xkk0/qwIEDXpcIIMZf//pXLVq0SCeccIIuvfRSSdLPf/5zvfbaayorK9Nxxx3ncYVAdiE0Au0ceeSRuuGGG7Rt2zb94he/0IEDBzRt2jSdfPLJevjhh/XPf/7T6xKBfssYE30szogRI/Tggw+qqKhIW7du1a9//WtdfPHFys3N9bpMICsRGoEu5OTkyOfz6be//a1efvllXXDBBfra176mYcOG6eabb9bOnTu9LhHoN95++21973vf08knn6zJkydHO7Ts3r1b3/rWtzRy5EivSwSyHqERiMOpp56qpUuXqqWlRbfddpsee+wxjRo1StOnT1dTU5PX5QFZ68UXX9RVV12lIUOGKBAIaOLEiWpubtbatWs1Z84cHXLIIV6XCPQbhEYgAUcffbS+/vWv6y9/+Yvq6ur0j3/8Q5MnT9Zpp52m6upqvffee16XCGS8d999N/pYnLPPPlsvvviiKioq9MYbb6iyslLjxo3zukSgXyI0AknIy8vTjBkz9Nxzz2njxo0644wzdN1112n48OG67bbbtHv3bq9LBDLO5s2bdeONN2rIkCH6yle+otGjR+u5557Tyy+/rNLSUh155JFelwj0a4RGoIc+9alP6cc//rGCwaCuvfZa/ehHP9JJJ52kyy+/XM8//7zX5QFpbe/evaqvr9ekSZM0duxYNTQ06NZbb9WuXbtUU1OjCRMmeF0igP8gNAK9ZPDgwbrzzjsVDAb1wx/+UDt37tR5552ns846S//7v/+rDz74wOsSgbSxa9cuBQIBjRgxQkVFRTriiCP0q1/9Sjt27NDXv/51HXvssV6XCKAdQiPQywYMGKDZs2frxRdf1Nq1a3XSSSdp3rx5GjFihO6++261trZ6XSLgiQMHDujXv/61pk+frhNPPFE/+MEPNG/ePP3lL39RQ0ODLrroIuXk8LUEpCtaJ9CHzj33XP30pz/Va6+9pi996Ut66KGHNGLECM2dO1cvvfSS1+UBKfH3v/9d3/72tzVq1ChddNFFikQiqqmpUUtLi+69916NGDHC6xIBxIHQCKTA0KFDdd9996mlpUXf//739cc//lFnnnmmPvOZz6i+vl779u3zukSg161du1azZ8/W0KFDde+99+oLX/iC/vznP+uZZ57R5Zdfrvz8fK9LBJAAQiOQQocddpgWLFigl19+WU8//bSOPvpozZo1S5/4xCf0wAMP6K233vK6RKBH/vnPf6qyslKnn366JkyYoC1btuihhx7SG2+8EX04N4DMRGgEPDJ58mQ98cQT2r59uy699FLdd999GjZsmK655hq98sorXpcHJORPf/qTvvzlL2vIkCH66le/qjPOOEMbNmzQH/7wB82fP1+HH3641yUC6CHHGGO8LgLAwTM0P/nJT/Tggw9q+/btmjx5sq6//npNmzaNzgEJeOyxx3Tbbbfp+OOPjw5bu3atxowZo2OOOUaSFA6HNWHCBD300ENelZkV3n//ff3sZz9TZWWl1q5dq9GjR8vv92vu3Ln62Mc+5nV5AHoZoRFIMwcOHNBvfvMbLV68WL/97W/1iU98Qtdee63mzZvHw43jUFZWpnvuuSeucfn4S87OnTu1dOlSPfroowqHwyosLJTf79fkyZPlOI7X5QHoI4RGII1t2bJFDz74oH7yk58oJydH8+bN07XXXqtRo0Z5XVra+vOf/6yxY8d2O05+fr7uuOMO3XnnnSmqKvPt379fq1at0iOPPKKnnnpKxx13nBYsWKAFCxa0OasLIHsRGoEMEA6HtWzZMj388MNqaWnRRRddpBtuuEFTpkzhzE4nTj31VL366qvdjrNlyxaNGTMmRRVlrr/97W/6wQ9+oKVLl2r37t2aOnWq/H6/CgsLlZub63V5AFKIG6WADDBo0CAtXLhQO3bsUH19vd555x197nOf06mnnqqlS5fq3Xff7Xb6HTt2yHGcuC/bZrrZs2d3+TgXx3F02mmn9bvA+NZbb2nChAmqra21jmuMiT4WZ/jw4frud7+ryy+/XFu3btVvfvMbXXLJJQRGoB8iNAIZJDc3V1/84hf1u9/9Ts3NzTrnnHN04403aujQofr617+uYDDY6XRLly6VdPB+v9tuuy3r7+UrKirq8tmXubm5mjt3boor8lYwGNQ555yjtWvX6tprr+1yvEgkogcffFBjx47VpEmTFAwGtWzZMu3evTv6cG4A/ReXp4EM9+abb2rp0qWqrKxUa2urLrnkEl133XX67Gc/K0l67733VFBQoHfeeUeSlJOToyuvvFJLlizJ6rNF48eP14svvqgDBw60Ge44jnbt2qUhQ4Z4VFlqNTc3a+rUqYpEItq7d68k6Xe/+53OP//86DgbN25UZWWlamtrlZOTo+LiYpWWlur000/3qmwAaYgzjUCGO/bYYxUIBPT6669r+fLl2rVrl84//3ydeeaZ+tGPfqQf/ehHeu+996LjHzhwQI8++qguu+wyvf/++x5W3rfmzp3b4X7PnJwcnXvuuf0mMD799NM677zzFA6Ho4ExPz9fjzzyiN577z398Ic/1DnnnKNPf/rTWr9+vb71rW9p9+7dWrp0KYERQAecaQSy0IYNG/Tggw+qvr5eRxxxhCKRSIczbnl5eTrvvPP0y1/+UkcccYRHlfadN998U8cdd5z2798fHZabm6tHHnlEV199tYeVpcby5ct15ZVX6sCBA53u+8MPP1z//ve/dckll6i0tDR6ZhoAukJoBLLYihUrVFJS0uX7+fn5Gjt2bPRPGmabqVOn6plnnokGx7y8PLW2tmb9g6fvv/9+3X777V3eu5qXl6fCwkJVVlaqoKAgxdUByFRcngayWE1NjfLy8rp8f+/evXr11Vc1fvx47dq1K4WVpcYVV1wRDU65ubmaOnVqVgfG/fv36ytf+Uq3gVGS9u3bp/Xr10f/Qg4AxIPQCGSpHTt26Mknn+yyF7Fr7969CgaDGj9+vLZs2ZKi6lJj+vTp0UfvGGM0e/ZsjyvqO++9956++MUvaunSpXH1jt+zZ49WrVqVgsoAZAtCI5ClvvnNb8b9aJ29e/cqFArp3HPP1caNG/u4stT56Ec/qmnTpkmSBgwYoIsvvtjjivrGW2+9pQsuuECrVq1qcw9ndxzH0aJFi/q4MgDZpOvrVkA/sm/fPjU0NMT9hZsJ3EfsDBgwQB988EGH93NycpSTkyPHcXTgwAHt27dPb7/9tj796U/rlltu0VlnnZXqkvvESSedFP03G8+shcPhDh17YvetpGhnmNhfIowxeuGFF7Ry5cqM/6tC48eP17Bhw7wuA8h6dIQBJD3xxBO65JJLvC4DQBLmzZunH/7wh16XAWQ9zjQCUvTP8PE7FJBZSkpKsvp5o0A64Z5GAAAAWBEaAQAAYEVoBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYEVoBPrA+vXrVVpaKsdxVFpaqk2bNlmnCYVCqqurU2FhYQoqzAyJbhO2Yfpg3wFZyAAwNTU1JtHmEA6HO51m9erVRpIJBoPGGGNqa2uNz+ezzs/v9xtJCdeRiK5qTleJbpNktqE7fm+/bPMuLy83VVVVSW2XzrTft+2Xt27dui6nXbduXaf190Qq9p0xxhQXF5vi4uJkSgSQIM40Aklas2ZNp8Pr6+slScOHD5ckzZo1Sw0NDdb5VVZW9l5xXeiq5nSV6DZJdhvW1tbKGBN9uWKH1dbWRoeFw+FOxzHGaPXq1W3ea21t7XTcM844Q1dffbXq6uqSqrm99vvWGKNgMBj9+cc//nGX08a+19ra2mYbJCtV+w5A6hAagSREIhFVV1d3+t6SJUtSXE18uqu5v5s1a5Z1nIsuuij6/4EDB3Y53qRJk9r8XFBQ0O14K1asiKfEbnW1b91fXMrLy7VkyRK1tLR0GKelpUUjR4601gsAhEYgCeXl5WpsbJQkOY7T5uVq/3MiQqGQKioqovdEdvZlHztOYWGhmpqaosMbGxtVWFioSCSi0tJSlZWVdVpzIvXE3m/W2NjYoba6urou641EItH3HcdRdXW1QqFQh+XEjldYWKht27ZZt0/sunemrKxMZWVlXb4fezauOwMHDrSO627TRM7UufskVm/v2ylTpkiSnn/++Q7Lev7556Pvd8bLfQcgzaT6ejiQjpK5p1Fd3H/V1fBE5unef9ba2mp8Pp+RZFpbW6PjucNra2uNMR/eR9nc3Bwd351Pc3Oz8fv9Paotdp7Nzc3GmA/vg/P7/dF6g8FgdFj76d3799zafT6fCYfDHcbz+/3R4bW1tR1q7m7dO1vHQCBgAoFAQusbz3ZqP4677onMT1J0PVy9vW/dYe49g+3Zpvdy38WDexqB1CE0Aib9QmOsrVu3GkltOk24X8jtp3XDkTuf9l/svV1bPMPcYBAbet3AGRuYGhoajCSzdevW6DC3c0fs/OJd955IJDS2fyUybiAQ6LCPenvfusPc/RDbIaa5udmsXr26y+kzYd8RGoHU4fI0kOZGjx4tSbr66qujw9z74NpfFr/nnnvaTNvdvXep4nYMir1X7uSTT5bU9n6+VatWSfpwfaXO64933VPF/KdjSzyXud1xTUwHmTlz5rS53NtX+9a9hzK208vPfvazDvdgxsr2fQcgMY4xvdBNDshwK1asUElJSUL3onV1/1oy97UlOk/bMlJVWzzDerpOvbXuiYhnHl2te2fTdDW/UCikwYMHKxAIaNGiRXEtO9F9G1tTXV2dioqKFAwGdeihh6qpqSnaCSje/dvZcC/3XUlJiSSppqYm7mkAJIczjUCG8Pv9HYZ11dkgnfh8PknqtPNEZ+sUr3Rc90SDqnsGr7MzbX2xfueee66kg51fmpqaoj93pT/tOwB2hEYgzbl/Teb887PiwpsAABlvSURBVM+PDquqqpIkLV++XJFIRNKHvVLTTXFxsSRp586d0WFuzTNmzIgOc9fJ9tdzMmndbdxe5rEBrC/Xb/jw4QoEAioqKtIbb7wRfSRPV9h3ANrou9slgcyRTEeY2F7N5eXlxpiDHQv0n5v5YzsFJDpPt3OC29vUnb+rtbW1044VwWCwzXvx1ByP2Hm6HTBih7kdJTobFg6Hoz1u3WG1tbUdeli7vY99Pl/0r+m4HTGkD3tkx7vu7rIS7T3d2Tzai+3k0b5DSjzbzpiDHZwCgUCHY6U39607fux6uMeo22O5u3X2et/Fg44wQOoQGgGTXGh0v3wDgUCXX4bJ/F62evXqaADw+/3RANleMBiMhg6/3x/9so5ddvs/X9i+5nh1tk7xDjPmYFioqqqKDq+tre00bAWDweijYfx+f5tHtMTWG8+6u8tPJDTGsw8T2c9djevum6qqqmjt7bdDT/dtdzXGhj7b+ni57+JBaARSh44wgJLrCAPAe3SEAVKHexoBAABgRWgEAACAVZ7XBQDZLt6/8ezFpfF0rg0AkF4IjUAfS+fAlc61AQDSC5enAQAAYEVoBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYJXndQFAOqmvr/e6BAAJqK+v14wZM7wuA+gXCI2ApJEjR0qSZs6c6XElABJ14oknel0C0C84xhjjdREA0Jccx1FNTY2Ki4u9LgUAMhb3NAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwCrP6wIAoDft3LlTTz/9dIfhTU1Neuedd6I/jxo1ShMnTkxlaQCQ0RxjjPG6CADoLdddd50eeugh5efnR4cdOHBAjuPIcRxJ0t69eyVJfPwBQPy4PA0gq0ybNk3SwWDovvbv3699+/ZFf87Pz9eVV17pcaUAkFkIjQCyypQpU3TUUUd1O87evXs1a9asFFUEANmB0Aggq+Tl5amoqKjN5en2jj76aE2aNCmFVQFA5iM0Asg6RUVF0fsW2xswYIBmz56t3NzcFFcFAJmNjjAAso4xRkOHDtWePXs6fX/9+vU655xzUlwVAGQ2zjQCyDqO4+iKK67o9BL10KFDdfbZZ3tQFQBkNkIjgKw0a9asDpeo8/PzNXfu3OijdwAA8ePyNICsNWrUKO3YsaPNsFdffVWnnHKKRxUBQObiTCOArPWlL32pzSXqk08+mcAIAEkiNALIWkVFRdq3b5+kg5emr7jiCo8rAoDMxeVpAFntzDPP1EsvvSTHcfTaa69pxIgRXpcEABmJM40Aspp7dnHcuHEERgDoAc40Av3QIYccog8++MDrMtCHNmzYwKOFAPSqPK8LAJB6H3zwgaZPn67i4mKvS0mJPXv26OMf/7hycvrHxZWZM2dqx44dhEYAvYrQCPRTM2bM0IwZM7wuAwCQIfrHr90AAADoEUIjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAJISCoVUV1enwsJCr0sBAKRAntcFAMhMd955p5YsWeJ1GQlzHKfL98rLyzV69Gh99rOf1cCBA1NYFQCkP840AkhKZWWl1yUkxRij1tbW6M/hcFjGGBljNGXKFFVXV2vOnDkKhUIeVgkA6YfQCKDfKSgoiP4/9oziuHHjtGzZMknS/PnzFYlEUl4bAKQrQiOAuEQiEdXV1clxHBUWFmrbtm2djhcKhVRRUREdr6mpKTo89h7IxsbG6DgtLS1t5uFOX11drVAo1OGSclfLkKSysjKVlZUlvZ4FBQW68cYb1djYqDVr1qTVugGApwyAfkeSqampSWgan89n/H6/CYfDxhhjamtrjSQT+zHS2tpqfD6fqa2tNcYYs3r1aiPJNDc3G5/PFx1/3bp1xhhjgsGgkWT8fn90HuXl5SYYDBpjjAmHwyYQCMS9DGOMCQQCJhAIxLUNuvoIDIfDHepKh3WLVzL7FwBsCI1AP5RoqGhoaDCSzNatW6PD3GAVG3rcINl+WW6I6yyotR8mybS2tkZ/bm1tTWgZ8eouNHb2fqatG6ERQG/j8jQAq1WrVkmSRo8eHR3WWe/iFStWSDrYQ9l9SdI999wT97L8fr8GDx6suro6RSIRFRQUyBjTq8tIRjavGwDEg9AIwCreR+s0NjZKUrQ3cuwrXjfddJN8Pp+Kioo0aNAgVVRU9PoybNwOMIFAoFeXmw7rBgDJIjQC6HVddZKJx+jRo9XQ0KDm5mb5/X4tXLiwQ7jq6TJsNm7cKEmaOHFiry43HdYNAJJFaARgVVVVJUnatGlTXOMtX748erbO7Q0cL8dxFIlENG7cOFVWVqq5uVkLFy7s1WV0JxQKafHixfL5fJo0aVKvLtfrdQOAHknlDZQA0oMS7Cjh9gT2+XzR3r9uz17F9BB2O3a0fwWDwTbvuT2wYzvTuB1E9J+OH+5ygsGgKS8vj9bS3TKMia/3dOxy3VqMMdGe0D6fr02HlXRZt3glun8BIB6caQRgNXz4cAWDQQ0ZMkQjRoxQaWmpTj31VPl8PtXW1uruu++WdPAZh8FgMHovoN/vVzAY1PDhwzV48ODo/AYNGtTmX0lt3r/uuutUX18vx3FUX1+vr371q9H3ultGPBzHabPcQYMGRTudPP3007r99tvV0NDQ5gHgmbJuANCXHGO4wxrobxzHUU1NjYqLi70uBX2A/QugL3CmEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYEVoBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWjjHGeF0EgNRyHMfrEtDHHn/8cU2fPt3rMgBkkTyvCwCQes8//7x2797tdRkpM3PmTF1//fWaMGGC16WkRG5urr7whS94XQaALMOZRgBZz3Ec1dTUqLi42OtSACBjcU8jAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAACrPK8LAIDe9vbbb3cY9q9//avN8MMPP1wDBgxIZVkAkNEcY4zxuggA6C233nqrHnjgAet4AwYM0Pvvv5+CigAgO3B5GkBWOemkk+Iab9SoUX1cCQBkF0IjgKxy2WWXKS+v+ztvcnNzdfPNN6eoIgDIDoRGAFnlYx/7mKZOnarc3Nwux8nJydEXv/jFFFYFAJmP0Agg68yePVtd3a6dl5eniy66SIMGDUpxVQCQ2QiNALLOxRdf3GXP6P3792vOnDkprggAMh+hEUDWOfzwwzV9+nTl5+d3eO/QQw/VtGnTPKgKADIboRFAViopKdHevXvbDMvPz9ell16qww47zKOqACBzERoBZKXPfe5zOvLII9sM27t3r0pKSjyqCAAyG6ERQFYaMGCALr/88jaXqI866ihNmTLFw6oAIHMRGgFkrdhL1Pn5+Zo1a5b1GY4AgM7xZwQBZK0DBw7o+OOPV2trqyTpueee04QJEzyuCgAyE2caAWStnJyc6D2Mxx9/vM477zyPKwKAzMV1GsADf/vb33TTTTdp//79XpeS9d5++21JB886Xn755R5X0z/MmTNHPp/P6zIA9DLONAIeaGpqUl1dnddl9AtHHXWUTj31VI0bN87rUvqF+vp6jm0gS3GmEfDQypUrvS4B6FU80gjIXpxpBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYEVoBAAAgBWhEQAAAFaERiCDhUIh1dXVqbCw0OtSAABZjtAIZLA777xTRUVFamxs9LqUHolEInIcJ6lpQ6GQysrK5DiOHMdRXV1dp/Pu7NV+3O50NQ/HcVRRUaHGxkZFIpGk1iHd9GR/AMhehEYgg1VWVnpdQq9Ys2ZNUtOFQiHt3LlTixYtkjFGtbW1KioqUkVFRXSczZs3dzn9pEmT4l6WMUatra3Rn8PhsIwxMsZoypQpqq6u1pw5cxQKhZJal3SS7P4AkN0IjQA8FYlEVF1dndS0O3fu1Pjx46M/z5o1S5K0cOHC6LDXX39dwWAwGvDc8BcIBFRQUJDQ8mLHHzhwYPT/48aN07JlyyRJ8+fPz+gzjj3ZHwCyG6ERyCCRSER1dXVyHEeFhYXatm1bm/dDoZAaGxtVWFioSCSi0tJSlZWVdTq94ziqrq5uc2YsdnpJqq6uluM4Ki0t7bCseOYXewm3q2Hl5eXRy+vtx7WJDYxuPZIUCASiwyZNmqThw4e3Ga+pqUmXXXZZm2FlZWVttlWiCgoKdOONN6qxsTF6pq6/7Q8A2Y3QCGSQOXPm6Nlnn1U4HFZDQ4NeeumlNu/Pnz9fhYWFamxs1ObNm+X3+/X3v/+9zfT//Oc/o2fbGhsb25wZGzx4cHT69evXa8GCBQqHw5KkMWPGdAgqtvnFXs51BYPBNj8vWrQo+n/3TGAyWlpaVF5eHq3L1dnZxGeffVbjxo1LajndOfPMMyVJq1atktS/9weALGQApFxNTY1JtPk1NDQYSWbr1q3RYeFw2EhqMy/353A43Gb61atXG0mmtbU1OmzdunVGkqmtre0wfazm5mYjyZSXl/fK/LqqOVnBYDA6j/Z1ttfc3NymvkTZau3v+6O4uNgUFxcnNS2A9MaZRiBDuGevRo8eHR0We19de+3fq6+vl9T2zNvJJ58sSVqxYkW3y3bPysXeK9iT+fW24cOHyxij5uZmBQIBLVy4sMv78n72s58l1AGmt/Sn/QEgOznGcO0BSLUVK1aopKQkoUt/7r1l7adpPzze8Xo6fU/Gi3deydi2bZvGjBnT6fxCoZC+//3vt7kEm6juao1EIho0aJACgUB0Gf1tf5SUlEiSampqEp4WQHrjTCPQT/h8Pknq9JEwfr8/rnnEjtcb8+sLsWdi2+usA0xv2rhxoyRp4sSJ1nH7y/4AkD0IjUCGqKqqkiRt2rQpqemLi4slHXxMjcvtIDFjxoxup3U7XPz3f/93r8yvL7k11NbWdnivrzrASAfD2uLFi+Xz+eK6/N1f9geA7EFoBDLEhRdeKOngo2FaWlokHTxz5iotLe32wdIXXXSRfD6fvvnNb0bHe/LJJ+X3+zsNOe5fS4lEIlq+fLl8Pl/0bFYi83PPcrlBZ/369W1qltqeJYt9MLdNYWGhKioqotsjEomovLxcgUAg+sxG16ZNm3T++ed3Oa94HrkT+/zF2P9v2rRJ8+fPl6To8xrd9elKNu4PAFkuNf1tAMRKpve0MQd7Cfv9fiPJ+P1+09raanw+n6mtrTWtra1tehD7fL4O07e2tpqqqqroOLW1tR169brvNTc3G5/PZySZqqqqDuPFO79gMBidT0NDgzHGtKnZmA97AwcCgTa9f23cHuXuq7y83Kxbt67TcW3zDgQCJhAIdPl+7HLav7pabn/bH8bQexrIZnSEATyQTEeYVOnNTinouUzbH3SEAbIXl6cBAABgRWgEENX+T9jBW+wPAOkkz+sCAKSPwYMHt/m/V5dE4/17x5lyyTZZ6bI/AEAiNAKIkS6hJF3q8BrbAUA64fI0AAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArPK8LgDoz2bOnOl1CUCvqq+vV3FxsddlAOgDnGkEPDBp0iTNmjXL6zL6jTVr1igUCnldRr8wY8YMjm0gSznGGON1EQDQlxzHUU1NDWfAAKAHONMIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArxxhjvC4CAHrLY489pttuu03HH398dNjatWs1ZswYHXPMMZKkcDisCRMm6KGHHvKqTADIOIRGAFmlrKxM99xzT1zj8vEHAPHj8jSArFJUVGQdJz8/X3fddVffFwMAWYQzjQCyzqmnnqpXX32123G2bNmiMWPGpKgiAMh8nGkEkHVmz56t/Pz8Tt9zHEennXYagREAEkRoBJB1ioqKtG/fvk7fy83N1dy5c1NcEQBkPi5PA8hK48eP14svvqgDBw60Ge44jnbt2qUhQ4Z4VBkAZCbONALISnPnzpXjOG2G5eTk6NxzzyUwAkASCI0AstJll13WYZjjOLriiis8qAYAMh+hEUBWOvbYYzVx4kTl5uZGhzmO02mYBADYERoBZK0rrrgi+gDv3NxcTZ06VR/72Mc8rgoAMhOhEUDWmj59evTRO8YYzZ492+OKACBzERoBZK2PfvSjmjZtmiRpwIABuvjiiz2uCAAyV57XBQBIX+vWrdPu3bu9LqNHTjrppOi/q1at8riansnNzVVhYaHy8vjoBpB6PKcRQJfaP7IG3nv88cc1ffp0r8sA0A/x6yqAbtXU1Ki4uNjrMqCDIf7dd9/1ugwA/RT3NAIAAMCK0AgAAAArQiMAAACsCI0AAACwIjQCAADAitAIAAAAK0IjAAAArAiNAAAAsCI0AgAAwIrQCAAAACtCIwAAAKwIjQAAALAiNAIAAMCK0AgAAAArQiMAAACsCI0AsoLjOF2+Kioq1NjYqEgk4nWZAJCxCI0AsoIxRq2trdGfw+GwjDEyxmjKlCmqrq7WnDlzFAqFPKwSADIXoRFA1igoKIj+f+DAgdH/jxs3TsuWLZMkzZ8/nzOOAJAEQiOAXhEKhVRXV6fCwkJJUmNjoxzHUWFhoVpaWjqMW1FREX2/qakpOryxsVGFhYWKRCIqLS1VWVmZJKmsrCz6/2QUFBToxhtvVGNjo9asWRN3PfGukzt9dXW1QqGQHMeJaxkAkCkIjQB6xfz581VUVKTGxkatX79ePp9PwWBQjY2Nuu+++6LjhUIhzZ8/X0OGDJExRjfeeKMmT56sTZs2af78+SosLFRjY6M2b94sv9+vv//9771W45lnnilJWrVqVdz1xLNOFRUVmjFjhowxmjlzpr7//e+3WW53ywCAjGEAoAuSTE1NTULjt/9YaT+stra203ECgUCb8cPhcNI1d/fRlmw93c1DkmltbY3+3NramtAy4pXo/gCA3sSZRgAptWLFCklteztL0j333NNmvNh7EtOhnu74/X4NHjxYdXV1ikQiKigokDGmV5cBAF4jNAJIqcbGRkmK9myOffU1twNMIBDo1Xpuuukm+Xw+FRUVadCgQaqoqGjzvpfrDAC9hdAIwBPbtm1L+TI3btwoSZo4cWKH93pSz+jRo9XQ0KDm5mb5/X4tXLiwQ3Ds6TIAwGuERgApVVVVJUlavnx59Myf27O4L4VCIS1evFg+n0+TJk3q1Xocx1EkEtG4ceNUWVmp5uZmLVy4sFeXAQBeIzQC6BWxD812g1Hs8xDd9y+++GJJB+/nGzRokBzH0eDBgzVjxoxuH7wdzyN3YpcX+3+3J7Sk6PMaXfHW0906SVJ5eXn0MTxHHXWUysvL41oGAGQKQiOAXjF48ODo/wcNGtTm39j3CwoKFAwGo/cV+v1+BYNBDR8+vM083GcjxstxnDbLc8OZ4zh6+umndfvtt6uhoaHNA8ATqae7dZKk6667TvX19XIcR/X19frqV78a1zIAIFM4hjuxAXTBcRzV1NSouLjY61Ig9gcAb3GmEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWhEYAAABYERoBAABgRWgEAACAFaERAAAAVoRGAAAAWBEaAQAAYEVoBAAAgBWhEQAAAFaERgAAAFgRGgEAAGBFaAQAAIAVoREAAABWeV4XACC91dfXKz8/3+syAAAec4wxxusiAKSnQw45RB988IHXZSDGhg0bdPbZZ3tdBoB+iNAIAAAAK+5pBAAAgBWhEQAAAFaERgAAAFgRGgEAAGD1/2hSD/SSA7LGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 47758, 50), (47758, 50), (3, 20364, 50), (20364, 50))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bert_inputs_train_k).shape, labels_train_k.shape,np.array(bert_inputs_test_k).shape, labels_test_k.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2985/2985 [==============================] - 1348s 451ms/step - loss: 1.0187 - custom_acc_orig_tokens: 0.7589 - custom_acc_orig_non_other_tokens: 0.7589 - val_loss: 0.9944 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 2/5\n",
      "2985/2985 [==============================] - 1348s 452ms/step - loss: 0.9152 - custom_acc_orig_tokens: 0.7799 - custom_acc_orig_non_other_tokens: 0.7799 - val_loss: 0.2994 - val_custom_acc_orig_tokens: 0.9143 - val_custom_acc_orig_non_other_tokens: 0.9143\n",
      "Epoch 3/5\n",
      "2985/2985 [==============================] - 1343s 450ms/step - loss: 0.2378 - custom_acc_orig_tokens: 0.9298 - custom_acc_orig_non_other_tokens: 0.9298 - val_loss: 0.1612 - val_custom_acc_orig_tokens: 0.9503 - val_custom_acc_orig_non_other_tokens: 0.9503\n",
      "Epoch 4/5\n",
      "2985/2985 [==============================] - 1339s 449ms/step - loss: 0.1673 - custom_acc_orig_tokens: 0.9491 - custom_acc_orig_non_other_tokens: 0.9491 - val_loss: 0.1353 - val_custom_acc_orig_tokens: 0.9585 - val_custom_acc_orig_non_other_tokens: 0.9585\n",
      "Epoch 5/5\n",
      "2985/2985 [==============================] - 1344s 450ms/step - loss: 0.1438 - custom_acc_orig_tokens: 0.9563 - custom_acc_orig_non_other_tokens: 0.9563 - val_loss: 0.1232 - val_custom_acc_orig_tokens: 0.9624 - val_custom_acc_orig_non_other_tokens: 0.9624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29249c137c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poch 1/5\n",
    "2985/2985 [==============================] - 1348s 451ms/step - loss: 1.0187 - custom_acc_orig_tokens: 0.7589 - custom_acc_orig_non_other_tokens: 0.7589 - val_loss: 0.9944 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
    "Epoch 2/5\n",
    "2985/2985 [==============================] - 1348s 452ms/step - loss: 0.9152 - custom_acc_orig_tokens: 0.7799 - custom_acc_orig_non_other_tokens: 0.7799 - val_loss: 0.2994 - val_custom_acc_orig_tokens: 0.9143 - val_custom_acc_orig_non_other_tokens: 0.9143\n",
    "Epoch 3/5\n",
    "2985/2985 [==============================] - 1343s 450ms/step - loss: 0.2378 - custom_acc_orig_tokens: 0.9298 - custom_acc_orig_non_other_tokens: 0.9298 - val_loss: 0.1612 - val_custom_acc_orig_tokens: 0.9503 - val_custom_acc_orig_non_other_tokens: 0.9503\n",
    "Epoch 4/5\n",
    "2985/2985 [==============================] - 1339s 449ms/step - loss: 0.1673 - custom_acc_orig_tokens: 0.9491 - custom_acc_orig_non_other_tokens: 0.9491 - val_loss: 0.1353 - val_custom_acc_orig_tokens: 0.9585 - val_custom_acc_orig_non_other_tokens: 0.9585\n",
    "Epoch 5/5\n",
    "2985/2985 [==============================] - 1344s 450ms/step - loss: 0.1438 - custom_acc_orig_tokens: 0.9563 - custom_acc_orig_non_other_tokens: 0.9563 - val_loss: 0.1232 - val_custom_acc_orig_tokens: 0.9624 - val_custom_acc_orig_non_other_tokens: 0.9624\n",
    "<tensorflow.python.keras.callbacks.History at 0x29249c137c0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2. Predictions & Confusion Matrix<a id=\"confusion\" />\n",
    "\n",
    "\n",
    "Let us look and see how well the model performs. We use the test here. (It probably would be better to split the data into train/validation/test, we are somewhat casual here).\n",
    "\n",
    "First, get all of the predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20364, 50, 17)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels\n",
      "[13  6  3  3  6  6  6  6  6  6  6  8  6  6  6  6  6  6  6  6  6  2  6  6\n",
      " 16 16 10 10 16  6  6  6  6  1 16 16  6  6  6  6  6  6  8  8  8  8  6  6\n",
      " 15 14]\n"
     ]
    }
   ],
   "source": [
    "print('True labels')\n",
    "print(nerLabels_test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels\n",
      "[ 6  6  3  3  6  6  6  6  6  6  6  8  6  6  6  6  6  6  6  6  6  2  6  6\n",
      "  6  6 10 10 10  6  6  6  6  6  6  7  6  6  6  6  6  6  8  8  8  8  6  6\n",
      "  6  6]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted labels')\n",
    "print(np.argmax(result, axis=2)[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now getting confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 17:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19467,   4981,  25063,   6410,   5122,  12074, 782318,   1437,\n",
       "        77512,   2227,  79194,    173,   2222,      0,      0,      0,\n",
       "            0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x291328082b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cm, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To release GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda \n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for RoBerta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel\n",
    "def ner_model_ROBERTa(max_input_length, train_layers, optimizer, numclasses):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFRobertaModel.from_pretrained('roberta-large')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(numclasses, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: Tensor(\"tf_roberta_model/roberta/encoder/layer_._23/output/LayerNorm/batchnorm/add_1:0\", shape=(None, 50, 1024), dtype=float32)\n",
      "pred:  Tensor(\"ner/truediv:0\", shape=(None, 50, 17), dtype=float32)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 50, 1024), ( 355359744   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      262400      tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_74[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,626,513\n",
      "Trainable params: 355,626,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 50, 1024), ( 355359744   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      262400      tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_74[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,626,513\n",
      "Trainable params: 355,626,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "numclasses = 17\n",
    "model_Roberta = ner_model_ROBERTa(max_length + 1, train_layers=-1, optimizer = adam_customized, numclasses = numclasses)\n",
    "\n",
    "model_Roberta.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo0AAAHBCAYAAAAM34HHAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3de3wcdb3/8ffkUkCQFoFU6A2RNgeKFEWgD1qF3kRO3aUIhSZpqYUWNkq5WZRL8gMtCGiiFYG0SRW1J01sRCDRokiDFEtbsJgg2KuFTUt1F4XdI4L0ku/vD86sm2Q3s5tsdnY3r+fjsY82s3P5zOx8d96Zme/EMsYYAQAAAL3Ic7sAAAAAZD5CIwAAABwRGgEAAOCI0AgAAABHBd0H/O1vf9NNN92kQ4cOuVEPMCDy8/P1ve99Tx/96EcHZP60G6Dv5s2bJ4/HM2Dzv/3227Vr164Bmz+Qi2K1yx5nGltbW9XY2Ji2ooB0aGxsVGtr64DNn3YD9E1TU9OAt517771XTU1NA7oMIJfEa5c9zjTa1qxZM6AFAelkWVZalkO7AZJTVlaWluXU19ertLQ0LcsCsl28dsk9jQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOEpJaKysrFRlZWUqZpV2idQeDAbV2Ngor9ebpqowGGRzu3ED7RBwH8fMwa3A7QJSIRwOa9iwYTLGDMj877zzTi1fvjyl87QsK+bwgVqH3nTffplUGwbOQLebVFu4cKFaWloGfDmZtP/TNjEQOGb2z2BulykJjUuXLk3FbPps/fr1fZ42kdprampS3gCMMZEdT5JCoZCGDh2a0mUkqvv2M8YoGAxq+PDhktytLZdlc7txQ3Nzc9wv51SibSLXcczsn8HcLrP+nsZwOKy6ujq3y+iT6J3KrR0s3vYrKiqK/D9Xd/7BLJvbTTrQNpGrsrnt0y7d1+/Q2P3ehe4/t7S0yLIseb1edXR0RMZpaWmJjFNXVyfLslReXq4dO3ZE5m1ZVuQVb1hVVVXkklX3cZOt3RYOh9XY2BipO7qmaNXV1bIsS3V1dQoGg12W3df71bJp+9nsRmRPX1lZqWAwGNk+9qu6ujoyTfR70etlD/d6vWptbe2xvuFwWOXl5Vl/L2AutRu71vLy8kitdvuJHibF31ei9dauumttbY25vrTND9A2+85pP4y3TWytra3yer2R7Ru9n/enDfW27ET2UY6ZtMt+Md3U19ebGIPj8ng8RlJkmuifN27caIwxxu/3G0nG5/MZ88FF/h7jhEIh4/P5jCSzfft2Y4wxgUCgy7yj5xU9rPvPfa09erjP5zOhUMgYY0xDQ0OP8aqqqozf74/UXlFR0eX9iooKU1FR4VhD9/lm0vZLdLvayw0EAj1q3bhxY5efo3k8HhMIBCK1ejwe09DQYIwxZt26dUaSaWtr67FN2traYs6vN5JMfX19UtMkY7C2m7a2NmNM1885Xv3G9L6vGOPcrmKtV21tbWQ/SmQetM3MapulpaWmtLQ0qWmSlWz7d9qHetsmxhjT3NzcZR+IPoZI6lcbSubziDcPjpm0Syfx2mW/Q6MxPTdUrA2XyDhtbW1Gkqmqqur3vPpau93Y7Z3ImA92rljLjD5Q2Ttbf5ef6LB0bL9Et2tFRUWvX0hVVVVGUuQLw67V3tmN+c+XTPfl218i9jztL6VkJXvQSNZgbzeJDnPaV5zaVfT43fehROeRKNrmBwa6bWZiaHTahxLdJt3fT/bzjzWsr8vmmEm7TEZWhMZUz6svtdu/ASQ6XkNDQ58/lFjzTXRYOrZfstvV7/dHdvbo6eyGWVtbGxkW/VunMV1/W+z+6kstsdYlV0NjqufVl9qTGWZM/H3FqV3Z42/cuDHub860zZ4yuW1mYmh02oectkms40hfPv9Yw/ryeXDMpF0mi9DYj9pjDd++fXuXDy36N5X+LD/RYZnWAGpra43H4zHbt2+POZ39hREKhSKXBZJZVn8bQLIHjWQN9naTzLDe9hWndmUPt3/Lti81RaNtdpXpbTMTQ2Oi+2E89kHfPjPU1zNasYb15fPgmEm7TFZWhUaney8yoQHY7HsF+toIBqIBpGr7OW1Xezn2Adz+LSjWdNFfos3NzT0O9vY00Zc4kqnFSbIHjWQN9naT6LBE9hVj4rer6PHte6KiL3klMo+BXkfaZnIyMTTanPbDeNvEmA8u29pnkaLvPes+j2SH9eXz4JhJu0xWVoRGO3E3Nzf3e159rb22ttZI/7k5ubdlRp9itz/g/i4/0WHp2H69bdeNGzdGvgQTnZ/9ReHxeHq8Z2/3ioqKyHYNBAKRL5X+NoC+HjQSNdjbTV+XmWy7ih4/FApFbsDvvkzaZva0zUwMjU77kNM2aW5udrwE29f9qy+fB8dM2mWyBiw0Rvc2CgQCXX62VyT6plj7rID9s70R7d5U3TdO995Nds8i6T+/HdinvKM3WF9qN+Y/PaU8Hk/ktwC7V1L0Mu0Pyx7HvjfBlkhPsOjtEv2hZ8L2i9WLzGbPw/6SsKf3+/1dTrV3PwNkTxd9n0aszyL65ff7e60lUckeNJI1WNtNrP02undf92FO+0pv7SrWcu32Gr1P0Tazq21mamjsbR/qbZvY08d6+Xy+uO090TaU6OfR2z7KMZN26WTAQmO8xhFdcG/DoruH19bW9vjtzO/3R963fxuwT/XbG9j+jaWioiLupapEao9epr3j2I28+zKjdxip52l2pwbgtN3c3H6J1mYvq/v0ds+w6Jt2bfY9HLH4/f7IJcfo6aOXGes3rkQke9BI1mBvN4kOc9pXemtXseYXfXCKXi5tM3vaZqaGxt72IWPibxNjTI/HnkS/7GNLX9tQb8vuaztMZrvEq4dj5uBol9b/LSBi9erVKisrU7fBKWc/EHOgl5OrsnH7hcNh3XrrraqpqUn7si3LUn19vUpLSwdk/rQb2LLxM3KzbZaVlUmS6uvrB2wZA93+u9uxY4cOP/xwjR49usfw4uLirNo3cgXtMjnx2mXW/xlBZI81a9Zo9uzZbpcBoBvaZuo0NjZq3LhxPQKjJA0fPlwNDQ0uVIVslInt0pXQ2P3PKSE52bT9Kisru/zpo6lTp7pdUtbKps99sMqmz4i2OTBWr16turq6Hn/6b8eOHVqzZo3mzJnjUmWDF+0ydVwJjcOHD4/5/1SJ/tuNvb2y1UBvv1Syf9uura3V0qVLXa4mu9FuMh9tE6tWrdKHP/xh3XvvvV3+tvDevXu1aNEit8uLKdfbPu0ydQrcWOhA31OQTfcs9EU2rd+iRYsy9osy29BuMl82bUPa5sAYOnSo5syZozlz5rhyL1pfZNN+2xfZtH6Z3i65pxEAAACOCI0AAABwRGgEAACAI0IjAAAAHBEaAQAA4IjQCAAAAEeERgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI0IjAAAAHBEaAQAA4Kgg3huXX355OutAjggGg8rLy9Nxxx3ndimuoN0AyWlqalJpaemAL6esrEyPP/74gC8nld5//311dHTolFNOkWVZbpeDQSReu8y/66677ooecPzxx+uNN96QMSZdtSGHvPLKK2pvb9df//pX5efn6+ijj86IL7szzjhD5eXlOuqoowZk/rSbzLZ+/XodccQROvLII90uBd2MHz9eZWVlKi4uHrBl7N+/XyeccMKAzT/V3nzzTf3pT3/Sli1b9I9//EMjRozQYYcd5nZZGETitUvLcJRDim3atEnLli3TL37xCx177LG69tpr5fP59NGPftTt0jBIWZal+vr6tJzRAvoiHA5r1apVWr58uV599VWdc8458vl8mjNnjo444gi3ywMkcU8jBsDEiRPV2Nio1157TQsWLNBDDz2kMWPG6Morr9Qf/vAHt8sDgIzxxz/+Uddee61GjhypW2+9VRMnTtQf/vAHbd68WQsWLCAwIqMQGjFgRowYoW9961vas2ePHn74YbW3t+vss8/W5MmTtWbNGh08eNDtEgEg7f7973/rpz/9qSZOnKhPfepTeu6553TPPffojTfe0MqVK3XWWWe5XSIQE6ERA+7www/X1Vdfrfb2drW2tur4449XaWmpPvaxj+m+++7TP/7xD7dLBIABt3PnTi1ZskQjRozQwoULddJJJ+mZZ57Rq6++quuvv15Dhw51u0SgV4RGpNWUKVP02GOPaefOnbr88st1//33a9SoUbrmmmv0yiuvuF0eAKTUwYMH9dhjj+nCCy9UcXGxmpqadPPNN6ujo0ONjY264IILMqKzIJAIQiNc8bGPfUzV1dXas2ePvvOd72j9+vX6xCc+oenTp+uJJ55QZ2en2yUCQJ/t27dP3/jGN3TSSSfpsssuU15enp544gnt3r1bd9xxBx0DkZUIjXDVUUcdpa985SvaunWrnnzySRUWFuqSSy7RuHHj9L3vfU/hcNjtEgEgIcYYPf3007r00kt10kkn6aGHHtLcuXO1c+dOPfnkk/J4PMrPz3e7TKDPCI3ICJZl6fOf/7yefPJJbd26VRdeeKH+3//7fxo1apQWL16sHTt2uF0iAMT01ltv6bvf/a7+67/+SzNmzFAwGNQjjzyiPXv26L777tPJJ5/sdolAShAakXGKi4v10EMPac+ePbrrrrv0q1/9Sqeeeqpmzpypp556igdoA8gIL7zwghYsWKCRI0fqrrvu0rRp0/Tyyy/rueeeU1lZGQ/kRs4hNCJjDRs2TDfffLN27typRx99VO+++64uvPBCnXbaaVq+fLn+9a9/uV0igEHmX//6l1auXKlPfepTOvfcc/XHP/5R3/3ud7Vv3z49/PDD+sQnPuF2icCAITQi4+Xn52vWrFl65pln1N7erkmTJummm27SyJEj9bWvfU2vv/662yUCyHF//vOfdcMNN2jEiBG67rrrNH78eP3+979XW1ubfD7fgP2JUiCTEBqRVc444wytXLlSe/bs0S233KLVq1frlFNO0aWXXqpnn33W7fIA5JADBw7oZz/7maZMmaLx48frl7/8pe644w7t3btXq1at0qRJk9wuEUgrQiOy0nHHHafbb79dr732murr67Vv3z5dcMEF+uQnP6lHHnlE77//vtslAshSfr9fd9xxh0aNGqWysjINHTpUv/71r7Vz507dcsstOu6449wuEXAFoRFZrbCwUFdccYU2btyoF154Qaeddpp8Pp9GjRqlyspK7du3z+0SAWSBzs5OrV27Vl6vVx//+Mf1yCOPaNGiRXrttdf0+OOP68ILL1ReHodMDG60AOSMs88+W/X19Xr99dfl8/lUW1urk046SWVlZdq8ebPb5QHIQMFgUPfff79OOeUUfeELX9A777yjhoYG+f1+LV26VKNGjXK7RCBjEBqRc0444QR985vfVEdHh2pra7V161ZNnDhREydOVENDgw4cOOB2iQBc9vvf/16lpaUaPXq07rvvPnm9Xm3dulWtra2aPXu2CgsL3S4RyDiERuSsww47TF/60pf00ksv6dlnn9WoUaM0b948nXTSSbrnnnv05ptvul0igDT63//9Xz388MM644wz9JnPfEa7du3SQw89pDfeeEPLli1TcXGx2yUCGY3QiEHhs5/9rJqamvSXv/xFc+fOVXV1tUaPHq2rrrpK7e3tbpcHYAC1t7ervLxcI0aM0JIlS3T22WfrhRde0AsvvKCrr75aH/rQh9wuEcgKhEYMKmPGjNH999+vvXv3atmyZdq8ebPOPPNMXXDBBXrsscd06NAht0sEkALvv/++Vq1apfPOO09nnnmmnnnmGd19993at2+ffvjDH+rss892u0Qg6xAaMSh96EMf0rXXXqtXXnlFTz31lI466ihddtllOuWUU1RVVaVQKOR2iQD6YNeuXbrllls0cuRIXX311Ro5cqTWrVunrVu36oYbbtCwYcPcLhHIWoRGDGqWZWnGjBn65S9/qW3btsnr9eqb3/ymRo4cqS9/+cvatm2b2yUCcHDo0CE9/vjj+vznP6/i4mL97Gc/0/XXXy+/3681a9Zo6tSpsizL7TKBrEdoBP7P2LFj9f3vf1979+7VPffco6eeekqnnXaaLrzwQj355JPq7Ox0u0QAUf76179q6dKlOumkk3TppZdKkn7xi1/otddeU2VlpU444QSXKwRyC6ER6Oboo4/WDTfcoB07duiJJ55QZ2enZs6cqVNPPVUPPfSQ/vnPf7pdIjBoGWMij8UZM2aMHnjgAZWUlGj79u369a9/rYsvvlj5+flulwnkJEIjEEdeXp48Ho9++9vf6uWXX9YFF1ygr33taxo1apRuvvlm7d692+0SgUHj7bff1ve//32deuqpmjZtWqRDy969e/Xtb39bp5xyitslAjmP0Agk4PTTT9eKFSvU0dGh2267TY8++qjGjh2rWbNmqbW11e3ygJz14osv6uqrr9aIESNUUVGhKVOmqK2tTRs2bNC8efN02GGHuV0iMGgQGoEkHHvssfr617+uv/zlL2psbNQ//vEPTZs2TWeccYbq6ur03nvvuV0ikPXefffdyGNxzjnnHL344ouqrq7WG2+8oZqaGk2YMMHtEoFBidAI9EFBQYFmz56t5557Tlu2bNEnP/lJLV68WKNHj9Ztt92mvXv3ul0ikHW2bt2qG2+8USNGjNBXvvIVjRs3Ts8995xefvlllZeX6+ijj3a7RGBQIzQC/fSpT31KP/nJT+T3+3Xdddfpxz/+sU4++WRdccUVev75590uD8hoBw4cUFNTk6ZOnarx48erublZt956q/bs2aP6+npNnjzZ7RIB/B9CI5Aiw4cP15133im/368f/ehH2r17tyZNmqSzzz5b//M//6P9+/e7XSKQMfbs2aOKigqNGTNGJSUlOuqoo/SrX/1Ku3bt0te//nUdf/zxbpcIoBtCI5BiQ4YM0dy5c/Xiiy9qw4YNOvnkk7VgwQKNGTNG3/jGNxQIBNwuEXBFZ2enfv3rX2vWrFn62Mc+ph/+8IdasGCB/vKXv6i5uVkXXXSR8vI4LAGZitYJDKDzzjtPP/vZz/Taa6/pS1/6kh588EGNGTNG8+fP10svveR2eUBa/P3vf9d3vvMdjR07VhdddJHC4bDq6+vV0dGhe+65R2PGjHG7RAAJIDQCaTBy5Ejde++96ujo0A9+8AP98Y9/1FlnnaXPfOYzampq0sGDB90uEUi5DRs2aO7cuRo5cqTuuecefeELX9Cf//xnPfPMM7riiitUWFjodokAkkBoBNLoiCOO0KJFi/Tyyy/r6aef1rHHHqs5c+bo4x//uO6//3699dZbbpcI9Ms///lP1dTU6Mwzz9TkyZO1bds2Pfjgg3rjjTciD+cGkJ0IjYBLpk2bpscff1w7d+7UpZdeqnvvvVejRo3Stddeq1deecXt8oCk/OlPf9KXv/xljRgxQl/96lf1yU9+Ups3b9Yf/vAHLVy4UEceeaTbJQLoJ8sYY9wuAsAHZ2h++tOf6oEHHtDOnTs1bdo0XX/99Zo5cyadA5Lw6KOP6rbbbtOJJ54YGbZhwwYVFxfruOOOkySFQiFNnjxZDz74oFtl5oT3339fP//5z1VTU6MNGzZo3Lhx8vl8mj9/vj7ykY+4XR6AFCM0Ahmms7NTv/nNb7Rs2TL99re/1cc//nFdd911WrBgAQ83TkBlZaXuvvvuhMbl669vdu/erRUrVuiRRx5RKBSS1+uVz+fTtGnTZFmW2+UBGCCERiCDbdu2TQ888IB++tOfKi8vTwsWLNB1112nsWPHul1axvrzn/+s8ePH9zpOYWGh7rjjDt15551pqir7HTp0SGvXrtXDDz+sp556SieccIIWLVqkRYsWdTmrCyB3ERqBLBAKhbRy5Uo99NBD6ujo0EUXXaQbbrhB06dP58xODKeffrpeffXVXsfZtm2biouL01RR9vrb3/6mH/7wh1qxYoX27t2rGTNmyOfzyev1Kj8/3+3yAKQRN0oBWWDYsGFasmSJdu3apaamJr3zzjv63Oc+p9NPP10rVqzQu+++2+v0u3btkmVZCV+2zXZz586N+zgXy7J0xhlnDLrA+NZbb2ny5MlqaGhwHNcYE3kszujRo/W9731PV1xxhbZv367f/OY3uuSSSwiMwCBEaASySH5+vr74xS/qd7/7ndra2nTuuefqxhtv1MiRI/X1r39dfr8/5nQrVqyQ9MH9frfddlvO38tXUlIS99mX+fn5mj9/fporcpff79e5556rDRs26Lrrros7Xjgc1gMPPKDx48dr6tSp8vv9Wrlypfbu3Rt5ODeAwYvL00CWe/PNN7VixQrV1NQoEAjokksu0eLFi/XZz35WkvTee++pqKhI77zzjiQpLy9PV111lZYvX57TZ4smTpyoF198UZ2dnV2GW5alPXv2aMSIES5Vll5tbW2aMWOGwuGwDhw4IEn63e9+p/PPPz8yzpYtW1RTU6OGhgbl5eWptLRU5eXlOvPMM90qG0AG4kwjkOWOP/54VVRU6PXXX9eqVau0Z88enX/++TrrrLP04x//WD/+8Y/13nvvRcbv7OzUI488ossuu0zvv/++i5UPrPnz5/e43zMvL0/nnXfeoAmMTz/9tCZNmqRQKBQJjIWFhXr44Yf13nvv6Uc/+pHOPfdcffrTn9amTZv07W9/W3v37tWKFSsIjAB64EwjkIM2b96sBx54QE1NTTrqqKMUDod7nHErKCjQpEmT9Mtf/lJHHXWUS5UOnDfffFMnnHCCDh06FBmWn5+vhx9+WNdcc42LlaXHqlWrdNVVV6mzszPmZ3/kkUfq3//+ty655BKVl5dHzkwDQDyERiCHrV69WmVlZXHfLyws1Pjx4yN/0jDXzJgxQ88880wkOBYUFCgQCOT8g6fvu+8+3X777XHvXS0oKJDX61VNTY2KiorSXB2AbMXlaSCH1dfXq6CgIO77Bw4c0KuvvqqJEydqz549aawsPa688spIcMrPz9eMGTNyOjAeOnRIX/nKV3oNjJJ08OBBbdq0KfIXcgAgEYRGIEft2rVLTz75ZNxexLYDBw7I7/dr4sSJ2rZtW5qqS49Zs2ZFHr1jjNHcuXNdrmjgvPfee/riF7+oFStWJNQ7ft++fVq7dm0aKgOQKwiNQI761re+lfCjdQ4cOKBgMKjzzjtPW7ZsGeDK0ufDH/6wZs6cKUkaMmSILr74YpcrGhhvvfWWLrjgAq1du7bLPZy9sSxLS5cuHeDKAOSS+NetgAywceNG7d271+0yspL9iJ0hQ4Zo//79Pd7Py8tTXl6eLMtSZ2enDh48qLfffluf/vSndcstt+jss89Od8kD4uSTT478m4tn1kKhUI+OPdGfraRIZ5joXyKMMXrhhRe0Zs0a/qpQgvLz8+X1enu95QPIZXSEQUbjYAYgkzz22GOaNWuW22UAruDXJWS8+vp6lZaWul0GgEHOsizHP9kJ5DLuaQQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEYPGpk2bVF5eLsuyVF5ervb29gFfZjAYVGNjo7xe74AvK1cku80G0zYeTOuaKPYXII0MkMEkmfr6+oTHD4VCJtZuvW7dOiPJ+P1+Y4wxDQ0NxuPxpKzOeHw+n5EUs6ZUibfO2SrZbdaXbWyPn+pXb/P2eDymqqrKbN++vU/bpa/rmqxk96fu67lx48a4427cuDHmNuuPdOwvtmS/j4Bcw5lG5JT169fHHN7U1CRJGj16tCRpzpw5am5uHvB6ampqBnwZ8dY5WyW7zfq6jRsaGmSMibxs0cMaGhoiw0KhUMxxjDFat25dl/cCgUCPcVeuXKlQKKTi4uI+n+XOxP3JGCO/3x/5+Sc/+UnccaPfCwQCXbZ7X6VrfwHA5WnkkHA4rLq6upjvLV++PM3VpEdv64zezZkzx3Gciy66KPL/oUOHxh1v6tSpXX4uKirqMU5RUZGWLFkiKXP3x77uT/YvY1VVVVq+fLk6Ojp6jNPR0aFTTjkl8nOsbQQgsxEakTOqqqrU0tIiSbIsq8vL1v1nJ8FgUC0tLfJ6vQqHwyovL1dlZWXk/XA4rMbGxsh86+rqFAwG486ruro6ck9lrANr9Dher1etra291hFrne266urqIsMqKyvj1tXbukff+9XS0tKjdnvdY61Potsmejyv16sdO3Y4br/obRNLZWVll8+pu+gzY70ZOnSo47j2Nk/krJkdPOOFxmzfn6ZPny5Jev7553u89/zzz0fe78+6D8T+AiBBab8gDiRBSd5DpDj3KsUb7sTj8XS5V6utrc34fL4u79fW1hpjjAkEAsbj8RiPx2NCoVCPZdv3etnjSTKBQCAynj28oaHBGPOf+zDb2tp6rSPWutn3bQUCAeP3+42kLnUnu+5tbW3GmP/ck+bz+SLrE2/+iWwbezyfzxcZ3tDQ0GOdets2sbZBRUWFqaioSGp9E9lHuo9jr3ui87PHr6qqijlNNu9P9jT2uN31Nv9E190eL9X7S6KS/T4Ccg2hERnN7dAYPW33g5d9IIo+UNuhyj5YxVv29u3bjaTIQdKY/xz8ui/bDj/x6og1/4qKii4H9f4cJGPV5DQs0W3T3NxsJHXpHGJ3xIieX6Lbpj+SCY3dX4nMzw5rHo+ny3axZfv+ZP9sr0d0h5i2tjazbt26uNNmy/5CaMRgR2hERsuk0NhdrDMq9gEsumd2ojVFn/2JFUr6sm5+v99UVVWlPTQmum3inZVK1bZJRjKh0ZbImcbolx2cYsn2/Sn6Z6nrmcjos76xps2W/YXQiMHOMiYF3deAAWJZlurr61VaWprw+JJ63F+WzH1n/Z1n9+H9Ha+vddTV1amlpUVVVVUqLi7udd7xxJp3IsMyZdskI5F5xFv3WNN0H9fr9WrChAlaunRpUsvPlv0pejs0NjaqpKREfr9fhx9+uFpbWyMdjxLdp9xYdyfJfh8BuYaOMEAfeTweSYp5s7L1PigAABmpSURBVL7P50toHrHGi3djfzIaGxt1zTXX6MEHH9S4ceP6Pb9kpWLbxJKKbZNqiQaPlStXqr29PW4HnVzan8477zxJH3R+aW1tjfwcz2DaX4BsRmgE+sg+27B79+7IsHA4LEmaPXt2r9Paz+k7//zzI8Nqa2slSatWrYrMx+4BmqySkhJJ/3kUSrolum3sdXZ6bmEqt41bioqKeg2OubQ/jR49WhUVFSopKdEbb7zhOB37C5AlBuiyN5ASSvIeouhepHYP1ba2tsg9TMn+NY5AIBD3/qdQKNSjY0NDQ0PMXsSKup/N7tnZvQdt9LKiX36/v9c6Yq2zPczv90c6Sdjj9GXd7c4S0cPsecUalui2se8J9Hg8kb/WY3eKUNR9cYluG3tZyfaejjWP7qI7XHTvPJLM/Oz9sba2tst72bw/2fOLXh97Pe0ey71tF7f3l0Ql+30E5BpCIzJasl/S9oGqoqIi7oEjmd+VoqeJ9WcHA4GAqa2tjYzT0NAQM1CsW7cucuD1+XxxO0T4/X5TUVERGc8+MPZWR/d1jjXM7v1qzy/Zdbe3WaLDktk2fr8/0sHB5/N1eVxK9EE9kW1jLz+Z0JjIPpLMfpTIuNG/yESHvWzcn3pb11g9rjNxf0kUoRGDHR1hkNG48RxApuD7CIMd9zQCAADAEaERAAAAjgrcLgBwQ6J/fzoX794YzOsOAOg7QiMGpcEciAbzugMA+o7L0wAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAARwVuFwA4aWpqUmFhodtlAAAwqFnGGON2EUA8hx12mPbv3+92GQAgSdq8ebPOOecct8sAXEFoBJDzLMtSfX29SktL3S4FALIW9zQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAUYHbBQBAKu3evVtPP/10j+Gtra165513Ij+PHTtWU6ZMSWdpAJDVLGOMcbsIAEiVxYsX68EHH1RhYWFkWGdnpyzLkmVZkqQDBw5Ikvj6A4DEcXkaQE6ZOXOmpA+Cof06dOiQDh48GPm5sLBQV111lcuVAkB2ITQCyCnTp0/XMccc0+s4Bw4c0Jw5c9JUEQDkBkIjgJxSUFCgkpKSLpenuzv22GM1derUNFYFANmP0Agg55SUlETuW+xuyJAhmjt3rvLz89NcFQBkNzrCAMg5xhiNHDlS+/bti/n+pk2bdO6556a5KgDIbpxpBJBzLMvSlVdeGfMS9ciRI3XOOee4UBUAZDdCI4CcNGfOnB6XqAsLCzV//vzIo3cAAInj8jSAnDV27Fjt2rWry7BXX31Vp512mksVAUD24kwjgJz1pS99qcsl6lNPPZXACAB9RGgEkLNKSkp08OBBSR9cmr7yyitdrggAsheXpwHktLPOOksvvfSSLMvSa6+9pjFjxrhdEgBkJc40Ashp9tnFCRMmEBgBoB840wgMQocddpj279/vdhkYQJs3b+bRQgBSqsDtAgCk3/79+zVr1iyVlpa6XUpa7Nu3Tx/96EeVlzc4Lq5cfvnl2rVrF6ERQEoRGoFBavbs2Zo9e7bbZQAAssTg+LUbAAAA/UJoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQC6JNgMKjGxkZ5vV63SwEApEGB2wUAyE533nmnli9f7nYZSbMsK+57VVVVGjdunD772c9q6NChaawKADIfZxoB9ElNTY3bJfSJMUaBQCDycygUkjFGxhhNnz5ddXV1mjdvnoLBoItVAkDmITQCGHSKiooi/48+ozhhwgStXLlSkrRw4UKFw+G01wYAmYrQCCAh4XBYjY2NsixLXq9XO3bsiDleMBhUdXV1ZLzW1tbI8Oh7IFtaWiLjdHR0dJmHPX1dXZ2CwWCPS8rxliFJlZWVqqys7PN6FhUV6cYbb1RLS4vWr1+fUesGAK4yAAYdSaa+vj6paTwej/H5fCYUChljjGloaDCSTPTXSCAQMB6PxzQ0NBhjjFm3bp2RZNra2ozH44mMv3HjRmOMMX6/30gyPp8vMo+qqirj9/uNMcaEQiFTUVGR8DKMMaaiosJUVFQktA3ifQWGQqEedWXCuiWqL58vADghNAKDULKhorm52Ugy27dvjwyzg1V06LGDZPdl2SEuVlDrPkySCQQCkZ8DgUBSy0hUb6Ex1vvZtm6ERgCpxuVpAI7Wrl0rSRo3blxkWKzexatXr5b0QQ9l+yVJd999d8LL8vl8Gj58uBobGxUOh1VUVCRjTEqX0Re5vG4AkAhCIwBHiT5ap6WlRZIivZGjX4m66aab5PF4VFJSomHDhqm6ujrly3Bid4CpqKhI6XIzYd0AoK8IjQBSLl4nmUSMGzdOzc3Namtrk8/n05IlS3qEq/4uw8mWLVskSVOmTEnpcjNh3QCgrwiNABzV1tZKktrb2xMab9WqVZGzdXZv4ERZlqVwOKwJEyaopqZGbW1tWrJkSUqX0ZtgMKhly5bJ4/Fo6tSpKV2u2+sGAP2SzhsoAWQGJdlRwu4J7PF4Ir1/7Z69iuohbHfs6P7y+/1d3rN7YEd3prE7iOj/On7Yy/H7/aaqqipSS2/LMCax3tPRy7VrMcZEekJ7PJ4uHVYyZd0SleznCwCJ4EwjAEejR4+W3+/XiBEjNGbMGJWXl+v000+Xx+NRQ0ODvvGNb0j64BmHfr8/ci+gz+eT3+/X6NGjNXz48Mj8hg0b1uVfSV3eX7x4sZqammRZlpqamvTVr3418l5vy0iEZVldljts2LBIp5Onn35at99+u5qbm7s8ADxb1g0ABpJlDHdYA4ONZVmqr69XaWmp26VgAPD5AhgInGkEAACAI0IjAAAAHBEaAQAA4IjQCAAAAEeERgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI0IjAAAAHBEaAQAA4IjQCAAAAEeERgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI8sYY9wuAkB6WZbldgkYYI899phmzZrldhkAckiB2wUASL/nn39ee/fudbuMtLn88st1/fXXa/LkyW6Xkhb5+fn6whe+4HYZAHIMZxoB5DzLslRfX6/S0lK3SwGArMU9jQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4K3C4AAFLt7bff7jHsX//6V5fhRx55pIYMGZLOsgAgq1nGGON2EQCQKrfeeqvuv/9+x/GGDBmi999/Pw0VAUBu4PI0gJxy8sknJzTe2LFjB7gSAMgthEYAOeWyyy5TQUHvd97k5+fr5ptvTlNFAJAbCI0AcspHPvIRzZgxQ/n5+XHHycvL0xe/+MU0VgUA2Y/QCCDnzJ07V/Fu1y4oKNBFF12kYcOGpbkqAMhuhEYAOefiiy+O2zP60KFDmjdvXporAoDsR2gEkHOOPPJIzZo1S4WFhT3eO/zwwzVz5kwXqgKA7EZoBJCTysrKdODAgS7DCgsLdemll+qII45wqSoAyF6ERgA56XOf+5yOPvroLsMOHDigsrIylyoCgOxGaASQk4YMGaIrrriiyyXqY445RtOnT3exKgDIXoRGADkr+hJ1YWGh5syZ4/gMRwBAbPwZQQA5q7OzUyeeeKICgYAk6bnnntPkyZNdrgoAshNnGgHkrLy8vMg9jCeeeKImTZrkckUAkL24TgO44G9/+5tuuukmHTp0yO1Sct7bb78t6YOzjldccYXL1QwO8+bNk8fjcbsMACnGmUbABa2trWpsbHS7jEHhmGOO0emnn64JEya4Xcqg0NTUxL4N5CjONAIuWrNmjdslACnFI42A3MWZRgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI0IjAAAAHBEaAQAA4IjQCAAAAEeERgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI0IjAAAAHBEagSwWDAbV2Ngor9frdikAgBxHaASy2J133qmSkhK1tLS4XUq/hMNhWZbV5+livRobG+NOV1dXl/Ty4i3HsixVV1erpaVF4XA46XXIRH39PADkNkIjkMVqamrcLiEl1q9f36fptm7dGve9qVOnxhze3t6ua665JullGWMUCAQiP4dCIRljZIzR9OnTVVdXp3nz5ikYDCY970zT188DQG4jNAJwVTgcVl1dXZ+mff311+X3+yPhzQ52FRUVKioqirmsn//8532uNXqeQ4cOjfx/woQJWrlypSRp4cKFWX3GsT+fB4DcRmgEskg4HFZjY6Msy5LX69WOHTu6vB8MBtXS0iKv16twOKzy8nJVVlbGnN6yLNXV1XU5MxY9vfSfy7jl5eU9lpXI/KIv4cYbVlVVFbm83n1cJ1OnTtXo0aO7DGttbdVll10Wc/yVK1dq8eLFMd+rrKzssq2SVVRUpBtvvFEtLS2RM3WD7fMAkNsIjUAWmTdvnp599lmFQiE1NzfrpZde6vL+woUL5fV61dLSoq1bt8rn8+nvf/97l+n/+c9/Rs7ItbS0dDkzNnz48Mj0mzZt0qJFixQKhSRJxcXFPYKK0/yiL+fa/H5/l5+XLl0a+b99tjBRsc4mPvvss5owYUKP4a2trZo0aVLMaVLlrLPOkiStXbtW0uD7PADkOAMg7err602yza+5udlIMtu3b48MC4VCRlKXedk/h0KhLtOvW7fOSDKBQCAybOPGjUaSaWho6DF9tLa2NiPJVFVVpWR+8Wrur7a2ti7LtgUCAVNbW5uS5TlNO9g/j9LSUlNaWtqnaQFkNs40AlnCPns1bty4yLDo++q66/5eU1OTpK5n50499VRJ0urVq3tdtn3mbsmSJSmZ30D5+c9/HrMDzBNPPKFFixa5UNF/DMbPA0BusYzh2gOQbqtXr1ZZWVlSl/7se8u6T9N9eKLj9Xf6/oyX6LySEQwG9YMf/KDL5VVJamlp0YQJE7rc+9if5fU2bTgc1rBhw1RRURGpY7B9HmVlZZKk+vr6pKcFkNk40wgMEh6PR5JiPhLG5/MlNI/o8VIxv1SK1wHG6/VqzJgxcTuBpNKWLVskSVOmTHEcN9c/DwC5h9AIZIna2lpJHzxnsC9KS0slSbt3744MsztIzJ49u9dp7Q4X//3f/52S+Q2EeB1gTNTjeOxX9HupEgwGtWzZMnk8nrjPiIyW658HgNxDaASyxIUXXijpg0fDdHR0SPrg7JqtvLy81wdLX3TRRfJ4PPrWt74VGe/JJ5+Uz+eLGXLsv6gSDoe1atUqeTyeyNmsZOZnn+Wyg86mTZu61Cx1PUtWXV2d0PaI1t7ervPPPz/p6aIl8sid6OcvRv+/vb1dCxculKTI8xql2Gf9bLn8eQDIUQPe1QZAD33pPW2MMX6/3/h8PiPJ+Hw+EwgEjMfjMQ0NDSYQCER6vUoyHo+nx/R2L2J7nIaGhh69eu332trajMfjMZJMbW1tj/ESnZ/f74/Mp7m52RhjutRszH96A1dUVHTp/ZuoZKez6+0+j4qKCsdpYr2qqqrMxo0be51msHwe9J4GchcdYQAX9KUjTLqkolMKUifbPg86wgC5i8vTAAAAcERoBBDR/U/YwV18HgAySYHbBQDIHMOHD+/yf7cuiSb6KJxsuWTbV5nyeQCARGgEECVTQkmm1OE2tgOATMLlaQAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAI0IjAAAAHBEaAQAA4IjQCAAAAEeERgAAADgiNAIAAMARoREAAACOCI0AAABwRGgEAACAowK3CwAGs8svv9ztEoCUampqUmlpqdtlABgAnGkEXDB16lTNmTPH7TIGjfXr1ysYDLpdxqAwe/Zs9m0gR1nGGON2EQAwkCzLUn19PWfAAKAfONMIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR5YxxrhdBACkyqOPPqrbbrtNJ554YmTYhg0bVFxcrOOOO06SFAqFNHnyZD344INulQkAWYfQCCCnVFZW6u67705oXL7+ACBxXJ4GkFNKSkocxyksLNRdd9018MUAQA7hTCOAnHP66afr1Vdf7XWcbdu2qbi4OE0VAUD240wjgJwzd+5cFRYWxnzPsiydccYZBEYASBKhEUDOKSkp0cGDB2O+l5+fr/nz56e5IgDIflyeBpCTJk6cqBdffFGdnZ1dhluWpT179mjEiBEuVQYA2YkzjQBy0vz582VZVpdheXl5Ou+88wiMANAHhEYAOemyyy7rMcyyLF155ZUuVAMA2Y/QCCAnHX/88ZoyZYry8/MjwyzLihkmAQDOCI0ActaVV14ZeYB3fn6+ZsyYoY985CMuVwUA2YnQCCBnzZo1K/LoHWOM5s6d63JFAJC9CI0ActaHP/xhzZw5U5I0ZMgQXXzxxS5XBADZq8DtAgBkro0bN2rv3r1ul9EvJ598cuTftWvXulxN/+Tn58vr9aqggK9uAOnHcxoBxNX9kTVw32OPPaZZs2a5XQaAQYhfVwH0qr6+XqWlpW6XAX0Q4t999123ywAwSHFPIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBAAAgCNCIwAAABwRGgEAAOCI0AgAAABHhEYAAAA4IjQCAADAEaERAAAAjgiNAAAAcERoBJATLMuK+6qurlZLS4vC4bDbZQJA1iI0AsgJxhgFAoHIz6FQSMYYGWM0ffp01dXVad68eQoGgy5WCQDZi9AIIGcUFRVF/j906NDI/ydMmKCVK1dKkhYuXMgZRwDoA0IjgJQIBoNqbGyU1+uVJLW0tMiyLHm9XnV0dPQYt7q6OvJ+a2trZHhLS4u8Xq/C4bDKy8tVWVkpSaqsrIz8vy+Kiop04403qqWlRevXr0+4nkTXyZ6+rq5OwWBQlmUltAwAyBaERgApsXDhQpWUlKilpUWbNm2Sx+OR3+9XS0uL7r333sh4wWBQCxcu1IgRI2SM0Y033qhp06apvb1dCxculNfrVUtLi7Zu3Sqfz6e///3vKavxrLPOkiStXbs24XoSWafq6mrNnj1bxhhdfvnl+sEPftBlub0tAwCyhgGAOCSZ+vr6pMbv/rXSfVhDQ0PMcSoqKrqMHwqF+lxzb19tfa2nt3lIMoFAIPJzIBBIahmJSvbzAIBU4kwjgLRavXq1pK69nSXp7rvv7jJe9D2JmVBPb3w+n4YPH67GxkaFw2EVFRXJGJPSZQCA2wiNANKqpaVFkiI9m6NfA83uAFNRUZHSem666SZ5PB6VlJRo2LBhqq6u7vK+m+sMAKlCaATgih07dqR9mVu2bJEkTZkypcd7/aln3Lhxam5uVltbm3w+n5YsWdIjOPZ3GQDgNkIjgLSqra2VJK1atSpy5s/uWTyQgsGgli1bJo/Ho6lTp6a0HsuyFA6HNWHCBNXU1KitrU1LlixJ6TIAwG2ERgApEf3QbDsYRT8P0X7/4osvlvTB/XzDhg2TZVkaPny4Zs+e3euDtxN55E708qL/b/eElhR5XqMt0Xp6WydJqqqqijyG55hjjlFVVVVCywCAbEFoBJASw4cPj/x/2LBhXf6Nfr+oqEh+vz9yX6HP55Pf79fo0aO7zMN+NmKiLMvqsjw7nFmWpaefflq33367mpubuzwAPJl6elsnSVq8eLGamppkWZaampr01a9+NaFlAEC2sAx3YgOIw7Is1dfXq7S01O1SID4PAO7iTCMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcERoBAADgiNAIAAAAR4RGAAAAOCI0AgAAwBGhEQAAAI4IjQAAAHBEaAQAAIAjQiMAAAAcFbhdAIDM1tTUpMLCQrfLAAC4zDLGGLeLAJCZDjvsMO3fv9/tMhBl8+bNOuecc9wuA8AgRGgEAACAI+5pBAAAgCNCIwAAABwRGgEAAOCI0AgAAABH/x/Q7mfj/5qcugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model_Roberta, to_file='model_r.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py:129 _create_slots\n        self.add_slot(var, 'v')\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:760 add_slot\n        weight = tf_variables.Variable(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:244 _variable_v2_call\n        return previous_getter(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:683 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3402 fill\n        _ops.raise_from_not_ok_status(e, name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-1b09bbd67bb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_Roberta.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mbert_inputs_train_k\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m{\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels_train_k\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_inputs_test_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels_test_k\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py:129 _create_slots\n        self.add_slot(var, 'v')\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:760 add_slot\n        weight = tf_variables.Variable(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:244 _variable_v2_call\n        return previous_getter(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:683 variable_capturing_scope\n        v = UnliftedInitializerVariable(\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:264 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:226 __init__\n        initial_value() if init_from_fn else initial_value,\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v2.py:137 __call__\n        return super(Zeros, self).__call__(shape, dtype=_get_dtype(dtype))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops_v2.py:132 __call__\n        return array_ops.zeros(shape, dtype)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2747 wrapped\n        tensor = fun(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2806 zeros\n        output = fill(shape, constant(zero, dtype=dtype), name=name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:239 fill\n        result = gen_array_ops.fill(dims, value, name=name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3402 fill\n        _ops.raise_from_not_ok_status(e, name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6843 raise_from_not_ok_status\n        six.raise_from(core._status_to_exception(e.code, message), None)\n    <string>:3 raise_from\n        \n\n    ResourceExhaustedError: OOM when allocating tensor with shape[1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "model_Roberta.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "result_Roberta = model_Roberta.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True labels')\n",
    "print(nerLabels_test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted labels - BERT')\n",
    "print(np.argmax(result, axis=2)[6])\n",
    "print('Predicted labels - RoBERTa')\n",
    "print(np.argmax(result_Roberta, axis=2)[6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result_Roberta, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 17:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
