{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['protest'],\n",
       " 'posToken': ['VB'],\n",
       " 'nerToken': ['O'],\n",
       " 'tokenLength': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('protest', 'VB', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68124"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import config\n",
    "vocab_params = config.VocabParameters()\n",
    "training_params = config.TrainingParameters()\n",
    "eval_params = config.EvalParameters()\n",
    "\n",
    "with open(vocab_params.data_dir+ '/train.json') as infile:\n",
    "    json_data = json.load(infile)\n",
    "len(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>relation</th>\n",
       "      <th>token</th>\n",
       "      <th>subj_start</th>\n",
       "      <th>subj_end</th>\n",
       "      <th>obj_start</th>\n",
       "      <th>obj_end</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>stanford_pos</th>\n",
       "      <th>stanford_ner</th>\n",
       "      <th>stanford_head</th>\n",
       "      <th>stanford_deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>42795</td>\n",
       "      <td>61b3a65fb960f284ebac</td>\n",
       "      <td>03c67d9ee4bf4ed33cbeddaa3a7b82cc</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Red, Sox, 12, ,, Athletics, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[NNP, NNP, CD, ,, NNP, CD]</td>\n",
       "      <td>[ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...</td>\n",
       "      <td>[2, 0, 2, 2, 2, 5]</td>\n",
       "      <td>[compound, ROOT, nummod, punct, appos, nummod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46757</td>\n",
       "      <td>61b3a65fb9080a05b4ee</td>\n",
       "      <td>15df2fc6a9a895432237cb2bdfcbd1b5</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Thomas, ', assertion, of, 85, %, reporters, v...</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...</td>\n",
       "      <td>[PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...</td>\n",
       "      <td>[3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...</td>\n",
       "      <td>[nmod:poss, case, nsubj, case, compound, amod,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36591</td>\n",
       "      <td>61b3a65fb9883fc52f01</td>\n",
       "      <td>274e368f381c1476fe0da7f201bfc331</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Kerry, did, his, duty, and, did, it, well, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2, 0, 4, 2, 2, 2, 6, 6, 2]</td>\n",
       "      <td>[nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22262</td>\n",
       "      <td>61b3a65fb937b50fc05a</td>\n",
       "      <td>409fa10efff702a41701bdddab89a2dd</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[This, August, ,, Moschella, 's, name, came, u...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "      <td>[DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...</td>\n",
       "      <td>[DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...</td>\n",
       "      <td>[det, ROOT, punct, nmod:poss, case, nsubj, acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>61b3a5f2e85a8088c7bb</td>\n",
       "      <td>78d7e406b6911492f6f7f122d1f112ad</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[Sharpton, is, president, of, the, National, A...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...</td>\n",
       "      <td>[3, 3, 0, 8, 8, 8, 8, 3, 3]</td>\n",
       "      <td>[nsubj, cop, ROOT, case, det, compound, compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4138</td>\n",
       "      <td>61b3a37935aa6ae21928</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:origin</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NATIONALITY</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32520</td>\n",
       "      <td>61b3afb926759a7aef5a</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41668</td>\n",
       "      <td>61b3afb926aa0b82acad</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62446</td>\n",
       "      <td>61b3a65fb9b37d516ec9</td>\n",
       "      <td>85b9cca690e98657db2480fb91e05489</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Washington, ,, DC, :, American, Psychiatric, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]</td>\n",
       "      <td>[LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...</td>\n",
       "      <td>[0, 1, 1, 1, 7, 7, 1, 7, 1]</td>\n",
       "      <td>[ROOT, punct, appos, punct, compound, compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66891</td>\n",
       "      <td>61b3a65fb9b8efc052b6</td>\n",
       "      <td>AFP_ENG_19941018.0328.LDC2007T07</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[President, Bill, Clinton, 's, top, defense, a...</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...</td>\n",
       "      <td>[O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...</td>\n",
       "      <td>[compound, compound, nmod:poss, case, amod, ns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                             docid  \\\n",
       "42795  61b3a65fb960f284ebac  03c67d9ee4bf4ed33cbeddaa3a7b82cc   \n",
       "46757  61b3a65fb9080a05b4ee  15df2fc6a9a895432237cb2bdfcbd1b5   \n",
       "36591  61b3a65fb9883fc52f01  274e368f381c1476fe0da7f201bfc331   \n",
       "22262  61b3a65fb937b50fc05a  409fa10efff702a41701bdddab89a2dd   \n",
       "644    61b3a5f2e85a8088c7bb  78d7e406b6911492f6f7f122d1f112ad   \n",
       "4138   61b3a37935aa6ae21928  84e924385dc7fb52b0417306a8500cb1   \n",
       "32520  61b3afb926759a7aef5a  84e924385dc7fb52b0417306a8500cb1   \n",
       "41668  61b3afb926aa0b82acad  84e924385dc7fb52b0417306a8500cb1   \n",
       "62446  61b3a65fb9b37d516ec9  85b9cca690e98657db2480fb91e05489   \n",
       "66891  61b3a65fb9b8efc052b6  AFP_ENG_19941018.0328.LDC2007T07   \n",
       "\n",
       "                        relation  \\\n",
       "42795                no_relation   \n",
       "46757                no_relation   \n",
       "36591                no_relation   \n",
       "22262                no_relation   \n",
       "644    org:top_members/employees   \n",
       "4138                  per:origin   \n",
       "32520                  per:title   \n",
       "41668                  per:title   \n",
       "62446                no_relation   \n",
       "66891                no_relation   \n",
       "\n",
       "                                                   token  subj_start  \\\n",
       "42795                    [Red, Sox, 12, ,, Athletics, 2]           0   \n",
       "46757  [Thomas, ', assertion, of, 85, %, reporters, v...          41   \n",
       "36591     [Kerry, did, his, duty, and, did, it, well, .]           2   \n",
       "22262  [This, August, ,, Moschella, 's, name, came, u...          17   \n",
       "644    [Sharpton, is, president, of, the, National, A...           5   \n",
       "4138    [She, is, an, American, actress, and, singer, .]           0   \n",
       "32520   [She, is, an, American, actress, and, singer, .]           0   \n",
       "41668   [She, is, an, American, actress, and, singer, .]           0   \n",
       "62446  [Washington, ,, DC, :, American, Psychiatric, ...           4   \n",
       "66891  [President, Bill, Clinton, 's, top, defense, a...          42   \n",
       "\n",
       "       subj_end  obj_start  obj_end     subj_type           obj_type  \\\n",
       "42795         1          4        4  ORGANIZATION       ORGANIZATION   \n",
       "46757        43         31       31  ORGANIZATION               DATE   \n",
       "36591         2          0        0        PERSON             PERSON   \n",
       "22262        20         39       40  ORGANIZATION  STATE_OR_PROVINCE   \n",
       "644           7          0        0  ORGANIZATION             PERSON   \n",
       "4138          0          3        3        PERSON        NATIONALITY   \n",
       "32520         0          6        6        PERSON              TITLE   \n",
       "41668         0          4        4        PERSON              TITLE   \n",
       "62446         6          7        7  ORGANIZATION             NUMBER   \n",
       "66891        44         39       39        PERSON             PERSON   \n",
       "\n",
       "                                            stanford_pos  \\\n",
       "42795                         [NNP, NNP, CD, ,, NNP, CD]   \n",
       "46757  [NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...   \n",
       "36591          [NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]   \n",
       "22262  [DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...   \n",
       "644             [NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]   \n",
       "4138                   [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "32520                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "41668                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "62446             [NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]   \n",
       "66891  [NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...   \n",
       "\n",
       "                                            stanford_ner  \\\n",
       "42795  [ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...   \n",
       "46757  [PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...   \n",
       "36591                   [PERSON, O, O, O, O, O, O, O, O]   \n",
       "22262  [DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...   \n",
       "644    [PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...   \n",
       "4138                         [O, O, O, MISC, O, O, O, O]   \n",
       "32520                        [O, O, O, MISC, O, O, O, O]   \n",
       "41668                        [O, O, O, MISC, O, O, O, O]   \n",
       "62446  [LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...   \n",
       "66891  [O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "                                           stanford_head  \\\n",
       "42795                                 [2, 0, 2, 2, 2, 5]   \n",
       "46757  [3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...   \n",
       "36591                        [2, 0, 4, 2, 2, 2, 6, 6, 2]   \n",
       "22262  [2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...   \n",
       "644                          [3, 3, 0, 8, 8, 8, 8, 3, 3]   \n",
       "4138                            [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "32520                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "41668                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "62446                        [0, 1, 1, 1, 7, 7, 1, 7, 1]   \n",
       "66891  [3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...   \n",
       "\n",
       "                                         stanford_deprel  \n",
       "42795     [compound, ROOT, nummod, punct, appos, nummod]  \n",
       "46757  [nmod:poss, case, nsubj, case, compound, amod,...  \n",
       "36591  [nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...  \n",
       "22262  [det, ROOT, punct, nmod:poss, case, nsubj, acl...  \n",
       "644    [nsubj, cop, ROOT, case, det, compound, compou...  \n",
       "4138      [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "32520     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "41668     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "62446  [ROOT, punct, appos, punct, compound, compound...  \n",
       "66891  [compound, compound, nmod:poss, case, amod, ns...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(json_data )\n",
    "train_df_sorted = train_df.sort_values(by=['docid','id'], ascending = True)\n",
    "train_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from above dataframe.  Each row in the dataframe represents a  \n",
    "    \n",
    "\"\"\"\n",
    "max_length = 50\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "pos_column = 'stanford_pos'\n",
    "ner_column = 'stanford_ner'\n",
    "token_column = 'token'\n",
    "\n",
    "\n",
    "for ind in train_df.index:    \n",
    "    word_list = train_df[token_column][ind]\n",
    "    ner_list = train_df[ner_column][ind]\n",
    "    pos_list = train_df[pos_column][ind]\n",
    "\n",
    "    for i in range(0,len(word_list)):\n",
    "\n",
    "        word = word_list[i]\n",
    "        ner = ner_list[i]\n",
    "        pos = pos_list[i]\n",
    "        addDict = addWord(word, pos, ner)\n",
    "    \n",
    "        sentenceTokens += addDict['wordToken']\n",
    "        posTokens += addDict['posToken']\n",
    "        nerTokens += addDict['nerToken']        \n",
    "\n",
    "#     print(sentenceTokens, posTokens, nerTokens, \"\\n\")\n",
    "    \n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "    \n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        posTokens = posTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding\n",
    "\n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "\n",
    "    posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    sentenceList.append(sentence)\n",
    "\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    posTokenList.append(posTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    sentence = ''\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    posTokens = ['[posCLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    sentence += ' ' + word\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "sentLengthList = sentLengthList[2:]\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "posTokenList = posTokenList[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 1.8000e+01, 1.5000e+01, 4.1000e+01, 5.2000e+01,\n",
       "        8.3000e+01, 1.0400e+02, 1.8400e+02, 1.8300e+02, 2.2500e+02,\n",
       "        2.8800e+02, 2.8300e+02, 3.0700e+02, 4.2600e+02, 4.4500e+02,\n",
       "        0.0000e+00, 4.9200e+02, 5.8300e+02, 6.6000e+02, 6.8500e+02,\n",
       "        6.7700e+02, 7.3600e+02, 9.6100e+02, 9.5000e+02, 1.0360e+03,\n",
       "        1.1930e+03, 1.2540e+03, 1.3570e+03, 1.3160e+03, 1.3940e+03,\n",
       "        1.4160e+03, 0.0000e+00, 1.4130e+03, 1.5450e+03, 1.4980e+03,\n",
       "        1.5340e+03, 1.6630e+03, 1.5620e+03, 1.5780e+03, 1.7290e+03,\n",
       "        1.5580e+03, 1.5800e+03, 1.5840e+03, 1.5740e+03, 1.4380e+03,\n",
       "        1.3910e+03, 1.4520e+03, 2.7658e+04]),\n",
       " array([ 4.    ,  4.9375,  5.875 ,  6.8125,  7.75  ,  8.6875,  9.625 ,\n",
       "        10.5625, 11.5   , 12.4375, 13.375 , 14.3125, 15.25  , 16.1875,\n",
       "        17.125 , 18.0625, 19.    , 19.9375, 20.875 , 21.8125, 22.75  ,\n",
       "        23.6875, 24.625 , 25.5625, 26.5   , 27.4375, 28.375 , 29.3125,\n",
       "        30.25  , 31.1875, 32.125 , 33.0625, 34.    , 34.9375, 35.875 ,\n",
       "        36.8125, 37.75  , 38.6875, 39.625 , 40.5625, 41.5   , 42.4375,\n",
       "        43.375 , 44.3125, 45.25  , 46.1875, 47.125 , 48.0625, 49.    ]),\n",
       " <a list of 48 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQM0lEQVR4nO3df4xdZZ3H8fdnW3CNP0KBQkjb3bJm/gDNWrWBJuwfiLuloNliIglkVxpDUmNKgombtfpPXZQE/vDHkihJlYaSqLVRWRqtW5suG9dEsYN0KbWaziILYxs6bEExJpjid/+4z4RLudOZ3mnnTrnvV3Jzz/ne55x5zgP3fOY+59xpqgpJ0nD7s0F3QJI0eIaBJMkwkCQZBpIkDANJErBw0B3o14UXXljLly8fdDck6azy6KOPPldVi0+sn7VhsHz5ckZHRwfdDUk6qyT53151p4kkSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAksRZ/A1kSXo9W77x+z3rT931/jPy8/xkIEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJKYQRgkWZbk4SQHkxxIcnurfybJb5Lsa4/ru7b5VJKxJL9Kcm1XfU2rjSXZ2FW/NMkjSQ4l+VaSc0/3gUqSpjaTTwbHgU9U1WXAKmBDksvba1+sqhXtsROgvXYT8HZgDfCVJAuSLAC+DFwHXA7c3LWfu9u+RoDngVtP0/FJkmZg2jCoqiNV9fO2/CJwEFhykk3WAtuq6qWq+jUwBlzRHmNV9WRV/RHYBqxNEuAa4Ntt+63ADf0ekCTp1J3SNYMky4F3AY+00m1JHk+yJcmiVlsCPNO12XirTVW/AHihqo6fUJckzZEZh0GSNwPfAT5eVb8D7gXeBqwAjgCfn2zaY/Pqo96rD+uTjCYZnZiYmGnXJUnTmFEYJDmHThB8vaq+C1BVz1bVy1X1J+CrdKaBoPOb/bKuzZcCh09Sfw44L8nCE+qvUVWbq2plVa1cvHjxTLouSZqBmdxNFOA+4GBVfaGrfklXsw8CT7TlHcBNSd6Q5FJgBPgZsBcYaXcOnUvnIvOOqirgYeBDbft1wEOzOyxJ0qlYOH0TrgI+DOxPsq/VPk3nbqAVdKZ0ngI+ClBVB5JsB35B506kDVX1MkCS24BdwAJgS1UdaPv7JLAtyeeAx+iEjyRpjkwbBlX1Y3rP6+88yTZ3Anf2qO/stV1VPckr00ySpDnmN5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkScwgDJIsS/JwkoNJDiS5vdXPT7I7yaH2vKjVk+SeJGNJHk/y7q59rWvtDyVZ11V/T5L9bZt7kuRMHKwkqbeZfDI4Dnyiqi4DVgEbklwObAT2VNUIsKetA1wHjLTHeuBe6IQHsAm4ErgC2DQZIK3N+q7t1sz+0CRJMzVtGFTVkar6eVt+ETgILAHWAltbs63ADW15LfBAdfwUOC/JJcC1wO6qOlZVzwO7gTXttbdW1U+qqoAHuvYlSZoDp3TNIMly4F3AI8DFVXUEOoEBXNSaLQGe6dpsvNVOVh/vUe/189cnGU0yOjExcSpdlySdxIzDIMmbge8AH6+q352saY9a9VF/bbFqc1WtrKqVixcvnq7LkqQZmlEYJDmHThB8vaq+28rPtike2vPRVh8HlnVtvhQ4PE19aY+6JGmOzORuogD3AQer6gtdL+0AJu8IWgc81FW/pd1VtAr4bZtG2gWsTrKoXTheDexqr72YZFX7Wbd07UuSNAcWzqDNVcCHgf1J9rXap4G7gO1JbgWeBm5sr+0ErgfGgD8AHwGoqmNJPgvsbe3uqKpjbfljwP3AG4EftIckaY5MGwZV9WN6z+sDvK9H+wI2TLGvLcCWHvVR4B3T9UWSdGb4DWRJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkZhAGSbYkOZrkia7aZ5L8Jsm+9ri+67VPJRlL8qsk13bV17TaWJKNXfVLkzyS5FCSbyU593QeoCRpejP5ZHA/sKZH/YtVtaI9dgIkuRy4CXh72+YrSRYkWQB8GbgOuBy4ubUFuLvtawR4Hrh1NgckSTp104ZBVf0IODbD/a0FtlXVS1X1a2AMuKI9xqrqyar6I7ANWJskwDXAt9v2W4EbTvEYJEmzNJtrBrclebxNIy1qtSXAM11txlttqvoFwAtVdfyEek9J1icZTTI6MTExi65Lkrr1Gwb3Am8DVgBHgM+3enq0rT7qPVXV5qpaWVUrFy9efGo9liRNaWE/G1XVs5PLSb4KfK+tjgPLupouBQ635V7154Dzkixsnw6620uS5khfnwySXNK1+kFg8k6jHcBNSd6Q5FJgBPgZsBcYaXcOnUvnIvOOqirgYeBDbft1wEP99EmS1L9pPxkk+SZwNXBhknFgE3B1khV0pnSeAj4KUFUHkmwHfgEcBzZU1cttP7cBu4AFwJaqOtB+xCeBbUk+BzwG3Hfajk6SNCPThkFV3dyjPOUJu6ruBO7sUd8J7OxRf5LO3UaSpAHxG8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJGYQBkm2JDma5Imu2vlJdic51J4XtXqS3JNkLMnjSd7dtc261v5QknVd9fck2d+2uSdJTvdBSpJObiafDO4H1pxQ2wjsqaoRYE9bB7gOGGmP9cC90AkPYBNwJXAFsGkyQFqb9V3bnfizJEln2LRhUFU/Ao6dUF4LbG3LW4EbuuoPVMdPgfOSXAJcC+yuqmNV9TywG1jTXntrVf2kqgp4oGtfkqQ50u81g4ur6ghAe76o1ZcAz3S1G2+1k9XHe9R7SrI+yWiS0YmJiT67Lkk60em+gNxrvr/6qPdUVZuramVVrVy8eHGfXZQknajfMHi2TfHQno+2+jiwrKvdUuDwNPWlPeqSpDnUbxjsACbvCFoHPNRVv6XdVbQK+G2bRtoFrE6yqF04Xg3saq+9mGRVu4volq59SZLmyMLpGiT5JnA1cGGScTp3Bd0FbE9yK/A0cGNrvhO4HhgD/gB8BKCqjiX5LLC3tbujqiYvSn+Mzh1LbwR+0B6SpDk0bRhU1c1TvPS+Hm0L2DDFfrYAW3rUR4F3TNcPSdKZ4zeQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkphlGCR5Ksn+JPuSjLba+Ul2JznUnhe1epLck2QsyeNJ3t21n3Wt/aEk62Z3SJKkU3U6Phm8t6pWVNXKtr4R2FNVI8Cetg5wHTDSHuuBe6ETHsAm4ErgCmDTZIBIkubGmZgmWgtsbctbgRu66g9Ux0+B85JcAlwL7K6qY1X1PLAbWHMG+iVJmsJsw6CAHyZ5NMn6Vru4qo4AtOeLWn0J8EzXtuOtNlX9NZKsTzKaZHRiYmKWXZckTVo4y+2vqqrDSS4Cdif55UnapketTlJ/bbFqM7AZYOXKlT3bSJJO3aw+GVTV4fZ8FHiQzpz/s236h/Z8tDUfB5Z1bb4UOHySuiRpjvQdBknelOQtk8vAauAJYAcweUfQOuChtrwDuKXdVbQK+G2bRtoFrE6yqF04Xt1qkqQ5MptpoouBB5NM7ucbVfXvSfYC25PcCjwN3Nja7wSuB8aAPwAfAaiqY0k+C+xt7e6oqmOz6Jck6RT1HQZV9STwzh71/wPe16NewIYp9rUF2NJvXyRJs+M3kCVJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkMYt/A1mSToflG78/5WtP3fX+ge1r2BgGkk5ZPyfdk23Tz88Z5L6mcjYHjmEgvc6c6knvZCewQZ/AzzZn87EbBtI8cDafRPT6YBhIc8QTvuYzw0A6zTzp62xkGEgn4Yldw8IwkPCkLxkGmvc8UUtnnmGgM2KqE/jpvAdd0ukzb8IgyRrgX4EFwNeq6q4Bd0nT8AQuvX7MizBIsgD4MvB3wDiwN8mOqvrFYHs2P3jSlXSmzYswAK4AxqrqSYAk24C1wLwPA0/Ukl4P5ksYLAGe6VofB648sVGS9cD6tvr7JL+ag74NyoXAc4PuxOmWu/va7HU5FrNwWsejz/8m88XQ/b8xzX+vmYzHX/YqzpcwSI9avaZQtRnYfOa7M3hJRqtq5aD7MR84Fq/meLzCsXi12YzHfPn3DMaBZV3rS4HDA+qLJA2d+RIGe4GRJJcmORe4Cdgx4D5J0tCYF9NEVXU8yW3ALjq3lm6pqgMD7tagDcV02Aw5Fq/meLzCsXi1vscjVa+ZmpckDZn5Mk0kSRogw0CSZBgMWpItSY4meaKrdn6S3UkOtedFg+zjXEqyLMnDSQ4mOZDk9lYfujFJ8udJfpbkv9tY/EurX5rkkTYW32o3XQyNJAuSPJbke219aMcjyVNJ9ifZl2S01fp6rxgGg3c/sOaE2kZgT1WNAHva+rA4Dnyiqi4DVgEbklzOcI7JS8A1VfVOYAWwJskq4G7gi20sngduHWAfB+F24GDX+rCPx3urakXX9wv6eq8YBgNWVT8Cjp1QXgtsbctbgRvmtFMDVFVHqurnbflFOm/6JQzhmFTH79vqOe1RwDXAt1t9KMZiUpKlwPuBr7X1MMTjMYW+3iuGwfx0cVUdgc7JEbhowP0ZiCTLgXcBjzCkY9KmRPYBR4HdwP8AL1TV8dZknE5YDosvAf8M/KmtX8Bwj0cBP0zyaPtzPdDne2VefM9AOlGSNwPfAT5eVb/r/AI4fKrqZWBFkvOAB4HLejWb214NRpIPAEer6tEkV0+WezQdivForqqqw0kuAnYn+WW/O/KTwfz0bJJLANrz0QH3Z04lOYdOEHy9qr7bykM9JlX1AvCfdK6jnJdk8he5YfrTLVcBf5/kKWAbnemhLzG840FVHW7PR+n8snAFfb5XDIP5aQewri2vAx4aYF/mVJsDvg84WFVf6Hpp6MYkyeL2iYAkbwT+ls41lIeBD7VmQzEWAFX1qapaWlXL6fzJmv+oqn9gSMcjyZuSvGVyGVgNPEGf7xW/gTxgSb4JXE3nT88+C2wC/g3YDvwF8DRwY1WdeJH5dSnJ3wD/BeznlXnhT9O5bjBUY5Lkr+lcAFxA5xe37VV1R5K/ovOb8fnAY8A/VtVLg+vp3GvTRP9UVR8Y1vFox/1gW10IfKOq7kxyAX28VwwDSZLTRJIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiTg/wHxFusaHrqrmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3406095</td>\n",
       "      <td>nerX</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3406096</td>\n",
       "      <td>nerX</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3406097</td>\n",
       "      <td>nerX</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3406098</td>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3406099</td>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3406100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  cat  sym\n",
       "0        [nerCLS]   13   13\n",
       "1               O    6    6\n",
       "2               O    6    6\n",
       "3               O    6    6\n",
       "4               O    6    6\n",
       "...           ...  ...  ...\n",
       "3406095      nerX   16   16\n",
       "3406096      nerX   16   16\n",
       "3406097      nerX   16   16\n",
       "3406098  [nerSEP]   15   15\n",
       "3406099  [nerPAD]   14   14\n",
       "\n",
       "[3406100 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000022B631D7CC8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag  cat  occurences\n",
       "6   O    6     1600623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution[nerDistribution['tag'] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>DATE</td>\n",
       "      <td>58622</td>\n",
       "      <td>58622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DURATION</td>\n",
       "      <td>12584</td>\n",
       "      <td>12584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LOCATION</td>\n",
       "      <td>61239</td>\n",
       "      <td>61239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MISC</td>\n",
       "      <td>25240</td>\n",
       "      <td>25240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MONEY</td>\n",
       "      <td>9224</td>\n",
       "      <td>9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NUMBER</td>\n",
       "      <td>31327</td>\n",
       "      <td>31327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>O</td>\n",
       "      <td>1600623</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>4689</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>140654</td>\n",
       "      <td>140654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PERCENT</td>\n",
       "      <td>5065</td>\n",
       "      <td>5065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PERSON</td>\n",
       "      <td>152696</td>\n",
       "      <td>152696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SET</td>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TIME</td>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>623605</td>\n",
       "      <td>623605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nerX</td>\n",
       "      <td>539711</td>\n",
       "      <td>539711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cat      sym\n",
       "tag                           \n",
       "DATE            58622    58622\n",
       "DURATION        12584    12584\n",
       "LOCATION        61239    61239\n",
       "MISC            25240    25240\n",
       "MONEY            9224     9224\n",
       "NUMBER          31327    31327\n",
       "O             1600623  1600623\n",
       "ORDINAL          4689     4689\n",
       "ORGANIZATION   140654   140654\n",
       "PERCENT          5065     5065\n",
       "PERSON         152696   152696\n",
       "SET              1101     1101\n",
       "TIME             3476     3476\n",
       "[nerCLS]        68122    68122\n",
       "[nerPAD]       623605   623605\n",
       "[nerSEP]        68122    68122\n",
       "nerX           539711   539711"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses.groupby(\"tag\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now split into Train,Test and Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 47758, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699283638178562"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = nerDistribution[nerDistribution['tag'] == 'O']['occurences'].sum()\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 17]['occurences'].sum()\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [bert_inputs_train_k, labels_train_k]\n",
    "test_all = [bert_inputs_test_k, labels_test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_train_k, labels_train_k = train_all = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_test_k, labels_test_k = test_all = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 10,  6,  2,  2,  6,  6, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label != 16)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.531251, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing customer loss function\n",
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "# Nice to have eager execution now...\n",
    "print(custom_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label != 16)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label != 16)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.3333333333333333, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "print(custom_acc_orig_tokens(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 68122, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(21, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load weights for 'bert-base-cased'. Make sure that:\n\n- 'bert-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-cased' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-96ce4440ef13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-cased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m                 )\n\u001b[1;32m--> 587\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading weights file {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load weights for 'bert-base-cased'. Make sure that:\n\n- 'bert-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-cased' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_layer = TFBertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. Model Runs/Experiments<a id=\"runs\"/>\n",
    "\n",
    "### V.1. With BERT-Layer Re-Training<a id=\"retrain\"/>\n",
    "\n",
    "It is time to run the first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_train_k[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load weights for 'bert-base-cased'. Make sure that:\n\n- 'bert-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-cased' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7cb3695a606b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# retrain all layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madam_customized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# model.describe()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-454eabf7fe3e>\u001b[0m in \u001b[0;36mner_model\u001b[1;34m(max_input_length, train_layers, optimizer)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mbert_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-cased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Freeze layers, i.e. only train number of layers specified, starting from the top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m                 )\n\u001b[1;32m--> 587\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading weights file {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load weights for 'bert-base-cased'. Make sure that:\n\n- 'bert-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-base-cased' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# retrain all layers\n",
    "model = ner_model(max_length + 1, train_layers=-1, optimizer = adam_customized)\n",
    "\n",
    "# model.describe()\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2. Predictions & Confusion Matrix<a id=\"confusion\" />\n",
    "\n",
    "\n",
    "Let us look and see how well the model performs. We use the test here. (It probably would be better to split the data into train/validation/test, we are somewhat casual here).\n",
    "\n",
    "First, get all of the predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
