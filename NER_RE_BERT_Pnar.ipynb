{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['protest'],\n",
       " 'posToken': ['VB'],\n",
       " 'nerToken': ['O'],\n",
       " 'tokenLength': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('protest', 'VB', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import config\n",
    "vocab_params = config.VocabParameters()\n",
    "training_params = config.TrainingParameters()\n",
    "eval_params = config.EvalParameters()\n",
    "\n",
    "with open(vocab_params.data_dir+ '/train.json') as infile:\n",
    "    json_data = json.load(infile)\n",
    "len(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>docid</th>\n",
       "      <th>relation</th>\n",
       "      <th>token</th>\n",
       "      <th>subj_start</th>\n",
       "      <th>subj_end</th>\n",
       "      <th>obj_start</th>\n",
       "      <th>obj_end</th>\n",
       "      <th>subj_type</th>\n",
       "      <th>obj_type</th>\n",
       "      <th>stanford_pos</th>\n",
       "      <th>stanford_ner</th>\n",
       "      <th>stanford_head</th>\n",
       "      <th>stanford_deprel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42795</th>\n",
       "      <td>61b3a65fb960f284ebac</td>\n",
       "      <td>03c67d9ee4bf4ed33cbeddaa3a7b82cc</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Red, Sox, 12, ,, Athletics, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[NNP, NNP, CD, ,, NNP, CD]</td>\n",
       "      <td>[ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...</td>\n",
       "      <td>[2, 0, 2, 2, 2, 5]</td>\n",
       "      <td>[compound, ROOT, nummod, punct, appos, nummod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46757</th>\n",
       "      <td>61b3a65fb9080a05b4ee</td>\n",
       "      <td>15df2fc6a9a895432237cb2bdfcbd1b5</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Thomas, ', assertion, of, 85, %, reporters, v...</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...</td>\n",
       "      <td>[PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...</td>\n",
       "      <td>[3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...</td>\n",
       "      <td>[nmod:poss, case, nsubj, case, compound, amod,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36591</th>\n",
       "      <td>61b3a65fb9883fc52f01</td>\n",
       "      <td>274e368f381c1476fe0da7f201bfc331</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Kerry, did, his, duty, and, did, it, well, .]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2, 0, 4, 2, 2, 2, 6, 6, 2]</td>\n",
       "      <td>[nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22262</th>\n",
       "      <td>61b3a65fb937b50fc05a</td>\n",
       "      <td>409fa10efff702a41701bdddab89a2dd</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[This, August, ,, Moschella, 's, name, came, u...</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>STATE_OR_PROVINCE</td>\n",
       "      <td>[DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...</td>\n",
       "      <td>[DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...</td>\n",
       "      <td>[det, ROOT, punct, nmod:poss, case, nsubj, acl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>61b3a5f2e85a8088c7bb</td>\n",
       "      <td>78d7e406b6911492f6f7f122d1f112ad</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>[Sharpton, is, president, of, the, National, A...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]</td>\n",
       "      <td>[PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...</td>\n",
       "      <td>[3, 3, 0, 8, 8, 8, 8, 3, 3]</td>\n",
       "      <td>[nsubj, cop, ROOT, case, det, compound, compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>61b3a37935aa6ae21928</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:origin</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NATIONALITY</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32520</th>\n",
       "      <td>61b3afb926759a7aef5a</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41668</th>\n",
       "      <td>61b3afb926aa0b82acad</td>\n",
       "      <td>84e924385dc7fb52b0417306a8500cb1</td>\n",
       "      <td>per:title</td>\n",
       "      <td>[She, is, an, American, actress, and, singer, .]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>[PRP, VBZ, DT, JJ, NN, CC, NN, .]</td>\n",
       "      <td>[O, O, O, MISC, O, O, O, O]</td>\n",
       "      <td>[5, 5, 5, 5, 0, 5, 5, 5]</td>\n",
       "      <td>[nsubj, cop, det, amod, ROOT, cc, conj, punct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62446</th>\n",
       "      <td>61b3a65fb9b37d516ec9</td>\n",
       "      <td>85b9cca690e98657db2480fb91e05489</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[Washington, ,, DC, :, American, Psychiatric, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]</td>\n",
       "      <td>[LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...</td>\n",
       "      <td>[0, 1, 1, 1, 7, 7, 1, 7, 1]</td>\n",
       "      <td>[ROOT, punct, appos, punct, compound, compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66891</th>\n",
       "      <td>61b3a65fb9b8efc052b6</td>\n",
       "      <td>AFP_ENG_19941018.0328.LDC2007T07</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>[President, Bill, Clinton, 's, top, defense, a...</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>[NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...</td>\n",
       "      <td>[O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...</td>\n",
       "      <td>[compound, compound, nmod:poss, case, amod, ns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                             docid  \\\n",
       "42795  61b3a65fb960f284ebac  03c67d9ee4bf4ed33cbeddaa3a7b82cc   \n",
       "46757  61b3a65fb9080a05b4ee  15df2fc6a9a895432237cb2bdfcbd1b5   \n",
       "36591  61b3a65fb9883fc52f01  274e368f381c1476fe0da7f201bfc331   \n",
       "22262  61b3a65fb937b50fc05a  409fa10efff702a41701bdddab89a2dd   \n",
       "644    61b3a5f2e85a8088c7bb  78d7e406b6911492f6f7f122d1f112ad   \n",
       "4138   61b3a37935aa6ae21928  84e924385dc7fb52b0417306a8500cb1   \n",
       "32520  61b3afb926759a7aef5a  84e924385dc7fb52b0417306a8500cb1   \n",
       "41668  61b3afb926aa0b82acad  84e924385dc7fb52b0417306a8500cb1   \n",
       "62446  61b3a65fb9b37d516ec9  85b9cca690e98657db2480fb91e05489   \n",
       "66891  61b3a65fb9b8efc052b6  AFP_ENG_19941018.0328.LDC2007T07   \n",
       "\n",
       "                        relation  \\\n",
       "42795                no_relation   \n",
       "46757                no_relation   \n",
       "36591                no_relation   \n",
       "22262                no_relation   \n",
       "644    org:top_members/employees   \n",
       "4138                  per:origin   \n",
       "32520                  per:title   \n",
       "41668                  per:title   \n",
       "62446                no_relation   \n",
       "66891                no_relation   \n",
       "\n",
       "                                                   token  subj_start  \\\n",
       "42795                    [Red, Sox, 12, ,, Athletics, 2]           0   \n",
       "46757  [Thomas, ', assertion, of, 85, %, reporters, v...          41   \n",
       "36591     [Kerry, did, his, duty, and, did, it, well, .]           2   \n",
       "22262  [This, August, ,, Moschella, 's, name, came, u...          17   \n",
       "644    [Sharpton, is, president, of, the, National, A...           5   \n",
       "4138    [She, is, an, American, actress, and, singer, .]           0   \n",
       "32520   [She, is, an, American, actress, and, singer, .]           0   \n",
       "41668   [She, is, an, American, actress, and, singer, .]           0   \n",
       "62446  [Washington, ,, DC, :, American, Psychiatric, ...           4   \n",
       "66891  [President, Bill, Clinton, 's, top, defense, a...          42   \n",
       "\n",
       "       subj_end  obj_start  obj_end     subj_type           obj_type  \\\n",
       "42795         1          4        4  ORGANIZATION       ORGANIZATION   \n",
       "46757        43         31       31  ORGANIZATION               DATE   \n",
       "36591         2          0        0        PERSON             PERSON   \n",
       "22262        20         39       40  ORGANIZATION  STATE_OR_PROVINCE   \n",
       "644           7          0        0  ORGANIZATION             PERSON   \n",
       "4138          0          3        3        PERSON        NATIONALITY   \n",
       "32520         0          6        6        PERSON              TITLE   \n",
       "41668         0          4        4        PERSON              TITLE   \n",
       "62446         6          7        7  ORGANIZATION             NUMBER   \n",
       "66891        44         39       39        PERSON             PERSON   \n",
       "\n",
       "                                            stanford_pos  \\\n",
       "42795                         [NNP, NNP, CD, ,, NNP, CD]   \n",
       "46757  [NNP, POS, NN, IN, CD, NN, NNS, VBP, JJ, VBZ, ...   \n",
       "36591          [NNP, VBD, PRP$, NN, CC, VBD, PRP, RB, .]   \n",
       "22262  [DT, NNP, ,, NNP, POS, NN, VBD, RP, IN, DT, NN...   \n",
       "644             [NNP, VBZ, NN, IN, DT, NNP, NNP, NNP, .]   \n",
       "4138                   [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "32520                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "41668                  [PRP, VBZ, DT, JJ, NN, CC, NN, .]   \n",
       "62446             [NNP, ,, NNP, :, NNP, NNP, NNP, CD, .]   \n",
       "66891  [NNP, NNP, NNP, POS, JJ, NN, CC, JJ, NN, NNS, ...   \n",
       "\n",
       "                                            stanford_ner  \\\n",
       "42795  [ORGANIZATION, ORGANIZATION, NUMBER, O, ORGANI...   \n",
       "46757  [PERSON, O, O, O, PERCENT, PERCENT, O, O, MISC...   \n",
       "36591                   [PERSON, O, O, O, O, O, O, O, O]   \n",
       "22262  [DATE, DATE, O, PERSON, O, O, O, O, O, O, O, O...   \n",
       "644    [PERSON, O, O, O, O, ORGANIZATION, ORGANIZATIO...   \n",
       "4138                         [O, O, O, MISC, O, O, O, O]   \n",
       "32520                        [O, O, O, MISC, O, O, O, O]   \n",
       "41668                        [O, O, O, MISC, O, O, O, O]   \n",
       "62446  [LOCATION, O, LOCATION, O, ORGANIZATION, ORGAN...   \n",
       "66891  [O, PERSON, PERSON, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "                                           stanford_head  \\\n",
       "42795                                 [2, 0, 2, 2, 2, 5]   \n",
       "46757  [3, 1, 8, 7, 6, 7, 3, 0, 12, 12, 12, 8, 12, 18...   \n",
       "36591                        [2, 0, 4, 2, 2, 2, 6, 6, 2]   \n",
       "22262  [2, 0, 2, 6, 4, 7, 2, 7, 11, 11, 7, 15, 15, 15...   \n",
       "644                          [3, 3, 0, 8, 8, 8, 8, 3, 3]   \n",
       "4138                            [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "32520                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "41668                           [5, 5, 5, 5, 0, 5, 5, 5]   \n",
       "62446                        [0, 1, 1, 1, 7, 7, 1, 7, 1]   \n",
       "66891  [3, 3, 6, 3, 6, 11, 6, 10, 10, 6, 0, 13, 46, 1...   \n",
       "\n",
       "                                         stanford_deprel  \n",
       "42795     [compound, ROOT, nummod, punct, appos, nummod]  \n",
       "46757  [nmod:poss, case, nsubj, case, compound, amod,...  \n",
       "36591  [nsubj, ROOT, nmod:poss, dobj, cc, conj, dobj,...  \n",
       "22262  [det, ROOT, punct, nmod:poss, case, nsubj, acl...  \n",
       "644    [nsubj, cop, ROOT, case, det, compound, compou...  \n",
       "4138      [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "32520     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "41668     [nsubj, cop, det, amod, ROOT, cc, conj, punct]  \n",
       "62446  [ROOT, punct, appos, punct, compound, compound...  \n",
       "66891  [compound, compound, nmod:poss, case, amod, ns...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(json_data )\n",
    "train_df_sorted = train_df.sort_values(by=['docid','id'], ascending = True)\n",
    "train_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from above dataframe.  Each row in the dataframe represents a  \n",
    "    \n",
    "\"\"\"\n",
    "max_length = 50\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "pos_column = 'stanford_pos'\n",
    "ner_column = 'stanford_ner'\n",
    "token_column = 'token'\n",
    "\n",
    "\n",
    "for ind in train_df.index:    \n",
    "    word_list = train_df[token_column][ind]\n",
    "    ner_list = train_df[ner_column][ind]\n",
    "    pos_list = train_df[pos_column][ind]\n",
    "\n",
    "    for i in range(0,len(word_list)):\n",
    "\n",
    "        word = word_list[i]\n",
    "        ner = ner_list[i]\n",
    "        pos = pos_list[i]\n",
    "        addDict = addWord(word, pos, ner)\n",
    "    \n",
    "        sentenceTokens += addDict['wordToken']\n",
    "        posTokens += addDict['posToken']\n",
    "        nerTokens += addDict['nerToken']        \n",
    "\n",
    "#     print(sentenceTokens, posTokens, nerTokens, \"\\n\")\n",
    "    \n",
    "    sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "    sentLengthList.append(sentenceLength)\n",
    "    \n",
    "    # Create space for at least a final '[SEP]' token\n",
    "    if sentenceLength >= max_length - 1: \n",
    "        sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "        posTokens = posTokens[:max_length - 2]\n",
    "        nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "    # add a ['SEP'] token and padding\n",
    "\n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "\n",
    "    posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "    nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "\n",
    "    sentenceList.append(sentence)\n",
    "\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "    bertSequenceIDs.append([0] * (max_length))\n",
    "\n",
    "    posTokenList.append(posTokens)\n",
    "    nerTokenList.append(nerTokens)\n",
    "\n",
    "    sentence = ''\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    posTokens = ['[posCLS]']\n",
    "    nerTokens = ['[nerCLS]']\n",
    "\n",
    "    sentence += ' ' + word\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "sentLengthList = sentLengthList[2:]\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "posTokenList = posTokenList[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 1.8000e+01, 1.5000e+01, 4.1000e+01, 5.2000e+01,\n",
       "        8.3000e+01, 1.0400e+02, 1.8400e+02, 1.8300e+02, 2.2500e+02,\n",
       "        2.8800e+02, 2.8300e+02, 3.0700e+02, 4.2600e+02, 4.4500e+02,\n",
       "        0.0000e+00, 4.9200e+02, 5.8300e+02, 6.6000e+02, 6.8500e+02,\n",
       "        6.7700e+02, 7.3600e+02, 9.6100e+02, 9.5000e+02, 1.0360e+03,\n",
       "        1.1930e+03, 1.2540e+03, 1.3570e+03, 1.3160e+03, 1.3940e+03,\n",
       "        1.4160e+03, 0.0000e+00, 1.4130e+03, 1.5450e+03, 1.4980e+03,\n",
       "        1.5340e+03, 1.6630e+03, 1.5620e+03, 1.5780e+03, 1.7290e+03,\n",
       "        1.5580e+03, 1.5800e+03, 1.5840e+03, 1.5740e+03, 1.4380e+03,\n",
       "        1.3910e+03, 1.4520e+03, 2.7658e+04]),\n",
       " array([ 4.    ,  4.9375,  5.875 ,  6.8125,  7.75  ,  8.6875,  9.625 ,\n",
       "        10.5625, 11.5   , 12.4375, 13.375 , 14.3125, 15.25  , 16.1875,\n",
       "        17.125 , 18.0625, 19.    , 19.9375, 20.875 , 21.8125, 22.75  ,\n",
       "        23.6875, 24.625 , 25.5625, 26.5   , 27.4375, 28.375 , 29.3125,\n",
       "        30.25  , 31.1875, 32.125 , 33.0625, 34.    , 34.9375, 35.875 ,\n",
       "        36.8125, 37.75  , 38.6875, 39.625 , 40.5625, 41.5   , 42.4375,\n",
       "        43.375 , 44.3125, 45.25  , 46.1875, 47.125 , 48.0625, 49.    ]),\n",
       " <BarContainer object of 48 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nerCLS]\n",
      "O\n",
      "ORGANIZATION\n",
      "nerX\n",
      "MISC\n",
      "[nerSEP]\n",
      "[nerPAD]\n",
      "DATE\n",
      "PERSON\n",
      "NUMBER\n",
      "LOCATION\n",
      "ORDINAL\n",
      "DURATION\n",
      "SET\n",
      "MONEY\n",
      "TIME\n",
      "PERCENT\n"
     ]
    }
   ],
   "source": [
    "x =nerClasses['tag'].unique()\n",
    "x\n",
    "for a in x:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405474</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405475</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405476</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405953</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405965</th>\n",
       "      <td>DURATION</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12584 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  cat  sym\n",
       "526      DURATION    1    1\n",
       "718      DURATION    1    1\n",
       "719      DURATION    1    1\n",
       "831      DURATION    1    1\n",
       "834      DURATION    1    1\n",
       "...           ...  ...  ...\n",
       "3405474  DURATION    1    1\n",
       "3405475  DURATION    1    1\n",
       "3405476  DURATION    1    1\n",
       "3405953  DURATION    1    1\n",
       "3405965  DURATION    1    1\n",
       "\n",
       "[12584 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses[nerClasses['cat']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, '[nerCLS]', 6, 'O', 8, 'ORGANIZATION', 16, 'nerX', 3, 'MISC',\n",
       "       15, '[nerSEP]', 14, '[nerPAD]', 0, 'DATE', 10, 'PERSON', 5,\n",
       "       'NUMBER', 2, 'LOCATION', 7, 'ORDINAL', 1, 'DURATION', 11, 'SET', 4,\n",
       "       'MONEY', 12, 'TIME', 9, 'PERCENT'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_values = nerClasses[[\"cat\", \"tag\"]].values.ravel()\n",
    "unique_values =  pd.unique(column_values)\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'cat'}>]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  6,  8, 16,  3, 15, 14,  0, 10,  5,  2,  7,  1, 11,  4, 12,  9],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses['cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>O</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>O</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>O</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>O</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>O</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>O</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tag  cat  occurences\n",
       "102   O    0           0\n",
       "103   O    1           0\n",
       "104   O    2           0\n",
       "105   O    3           0\n",
       "106   O    4           0\n",
       "107   O    5           0\n",
       "108   O    6     1600623\n",
       "109   O    7           0\n",
       "110   O    8           0\n",
       "111   O    9           0\n",
       "112   O   10           0\n",
       "113   O   11           0\n",
       "114   O   12           0\n",
       "115   O   13           0\n",
       "116   O   14           0\n",
       "117   O   15           0\n",
       "118   O   16           0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution[nerDistribution['tag'] == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>sym</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>58622</td>\n",
       "      <td>58622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DURATION</th>\n",
       "      <td>12584</td>\n",
       "      <td>12584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>61239</td>\n",
       "      <td>61239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>25240</td>\n",
       "      <td>25240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONEY</th>\n",
       "      <td>9224</td>\n",
       "      <td>9224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMBER</th>\n",
       "      <td>31327</td>\n",
       "      <td>31327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>1600623</td>\n",
       "      <td>1600623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDINAL</th>\n",
       "      <td>4689</td>\n",
       "      <td>4689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>140654</td>\n",
       "      <td>140654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT</th>\n",
       "      <td>5065</td>\n",
       "      <td>5065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>152696</td>\n",
       "      <td>152696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SET</th>\n",
       "      <td>1101</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <td>3476</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerCLS]</th>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerPAD]</th>\n",
       "      <td>623605</td>\n",
       "      <td>623605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[nerSEP]</th>\n",
       "      <td>68122</td>\n",
       "      <td>68122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerX</th>\n",
       "      <td>539711</td>\n",
       "      <td>539711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  cat      sym\n",
       "tag                           \n",
       "DATE            58622    58622\n",
       "DURATION        12584    12584\n",
       "LOCATION        61239    61239\n",
       "MISC            25240    25240\n",
       "MONEY            9224     9224\n",
       "NUMBER          31327    31327\n",
       "O             1600623  1600623\n",
       "ORDINAL          4689     4689\n",
       "ORGANIZATION   140654   140654\n",
       "PERCENT          5065     5065\n",
       "PERSON         152696   152696\n",
       "SET              1101     1101\n",
       "TIME             3476     3476\n",
       "[nerCLS]        68122    68122\n",
       "[nerPAD]       623605   623605\n",
       "[nerSEP]        68122    68122\n",
       "nerX           539711   539711"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerClasses.groupby(\"tag\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now split into Train,Test and Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 47758, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14],\n",
       "       ...,\n",
       "       [13,  6,  6, ..., 16, 15, 14],\n",
       "       [13, 10, 16, ..., 14, 14, 14],\n",
       "       [13,  6,  6, ..., 14, 14, 14]], dtype=int8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerLabels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699283638178562"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = nerDistribution[nerDistribution['tag'] == 'O']['occurences'].sum()\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 17]['occurences'].sum()\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [bert_inputs_train_k, labels_train_k]\n",
    "test_all = [bert_inputs_test_k, labels_test_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(train_all, output_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test_all, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"./bert_train_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_train_k, labels_train_k = train_all = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"./bert_test_data.pickle\", \"rb\") as input_file:\n",
    "    bert_inputs_test_k, labels_test_k = test_all = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 10,  6,  2,  2,  6,  6, 15, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.1971407, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Testing customer loss function\n",
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "# Nice to have eager execution now...\n",
    "print(custom_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47758, 50)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 13  )\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.keras.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[12],[0],[14],[0],[0],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "print(custom_acc_orig_tokens(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 68122, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer, numclasses):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-cased')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(numclasses, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. Model Runs/Experiments<a id=\"runs\"/>\n",
    "\n",
    "### V.1. With BERT-Layer Re-Training<a id=\"retrain\"/>\n",
    "\n",
    "It is time to run the first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1188,  1108,  1621,   170, 15817,  1104, 16885,  3500,\n",
       "        1291,   112,   188, 14387,  1115,   146,  1108,  1549,  1112,\n",
       "         170, 10703,  1111,  3455,  1105, 18912,  1113,   170,  8323,\n",
       "        1111,   152, 18124,   119,   102,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs_train_k[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: Tensor(\"tf_bert_model/bert/encoder/layer_._11/output/LayerNorm/batchnorm/add_1:0\", shape=(None, 50, 768), dtype=float32)\n",
      "pred:  Tensor(\"ner/truediv:0\", shape=(None, 50, 17), dtype=float32)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 50, 768), (N 108310272   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      196864      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,511,505\n",
      "Trainable params: 108,511,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 50, 768), (N 108310272   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      196864      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 108,511,505\n",
      "Trainable params: 108,511,505\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# retrain all layers\n",
    "numclasses = 17\n",
    "model = ner_model(max_length + 1, train_layers=-1, optimizer = adam_customized, numclasses = numclasses)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 47758, 50), (47758, 50), (3, 20364, 50), (20364, 50))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bert_inputs_train_k).shape, labels_train_k.shape,np.array(bert_inputs_test_k).shape, labels_test_k.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2985/2985 [==============================] - 1348s 451ms/step - loss: 1.0187 - custom_acc_orig_tokens: 0.7589 - custom_acc_orig_non_other_tokens: 0.7589 - val_loss: 0.9944 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 2/5\n",
      "2985/2985 [==============================] - 1348s 452ms/step - loss: 0.9152 - custom_acc_orig_tokens: 0.7799 - custom_acc_orig_non_other_tokens: 0.7799 - val_loss: 0.2994 - val_custom_acc_orig_tokens: 0.9143 - val_custom_acc_orig_non_other_tokens: 0.9143\n",
      "Epoch 3/5\n",
      "2985/2985 [==============================] - 1343s 450ms/step - loss: 0.2378 - custom_acc_orig_tokens: 0.9298 - custom_acc_orig_non_other_tokens: 0.9298 - val_loss: 0.1612 - val_custom_acc_orig_tokens: 0.9503 - val_custom_acc_orig_non_other_tokens: 0.9503\n",
      "Epoch 4/5\n",
      "2985/2985 [==============================] - 1339s 449ms/step - loss: 0.1673 - custom_acc_orig_tokens: 0.9491 - custom_acc_orig_non_other_tokens: 0.9491 - val_loss: 0.1353 - val_custom_acc_orig_tokens: 0.9585 - val_custom_acc_orig_non_other_tokens: 0.9585\n",
      "Epoch 5/5\n",
      "2985/2985 [==============================] - 1344s 450ms/step - loss: 0.1438 - custom_acc_orig_tokens: 0.9563 - custom_acc_orig_non_other_tokens: 0.9563 - val_loss: 0.1232 - val_custom_acc_orig_tokens: 0.9624 - val_custom_acc_orig_non_other_tokens: 0.9624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29249c137c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poch 1/5\n",
    "2985/2985 [==============================] - 1348s 451ms/step - loss: 1.0187 - custom_acc_orig_tokens: 0.7589 - custom_acc_orig_non_other_tokens: 0.7589 - val_loss: 0.9944 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
    "Epoch 2/5\n",
    "2985/2985 [==============================] - 1348s 452ms/step - loss: 0.9152 - custom_acc_orig_tokens: 0.7799 - custom_acc_orig_non_other_tokens: 0.7799 - val_loss: 0.2994 - val_custom_acc_orig_tokens: 0.9143 - val_custom_acc_orig_non_other_tokens: 0.9143\n",
    "Epoch 3/5\n",
    "2985/2985 [==============================] - 1343s 450ms/step - loss: 0.2378 - custom_acc_orig_tokens: 0.9298 - custom_acc_orig_non_other_tokens: 0.9298 - val_loss: 0.1612 - val_custom_acc_orig_tokens: 0.9503 - val_custom_acc_orig_non_other_tokens: 0.9503\n",
    "Epoch 4/5\n",
    "2985/2985 [==============================] - 1339s 449ms/step - loss: 0.1673 - custom_acc_orig_tokens: 0.9491 - custom_acc_orig_non_other_tokens: 0.9491 - val_loss: 0.1353 - val_custom_acc_orig_tokens: 0.9585 - val_custom_acc_orig_non_other_tokens: 0.9585\n",
    "Epoch 5/5\n",
    "2985/2985 [==============================] - 1344s 450ms/step - loss: 0.1438 - custom_acc_orig_tokens: 0.9563 - custom_acc_orig_non_other_tokens: 0.9563 - val_loss: 0.1232 - val_custom_acc_orig_tokens: 0.9624 - val_custom_acc_orig_non_other_tokens: 0.9624\n",
    "<tensorflow.python.keras.callbacks.History at 0x29249c137c0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2. Predictions & Confusion Matrix<a id=\"confusion\" />\n",
    "\n",
    "\n",
    "Let us look and see how well the model performs. We use the test here. (It probably would be better to split the data into train/validation/test, we are somewhat casual here).\n",
    "\n",
    "First, get all of the predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20364, 50, 17)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels\n",
      "[13  6  3  3  6  6  6  6  6  6  6  8  6  6  6  6  6  6  6  6  6  2  6  6\n",
      " 16 16 10 10 16  6  6  6  6  1 16 16  6  6  6  6  6  6  8  8  8  8  6  6\n",
      " 15 14]\n"
     ]
    }
   ],
   "source": [
    "print('True labels')\n",
    "print(nerLabels_test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels\n",
      "[ 6  6  3  3  6  6  6  6  6  6  6  8  6  6  6  6  6  6  6  6  6  2  6  6\n",
      "  6  6 10 10 10  6  6  6  6  6  6  7  6  6  6  6  6  6  8  8  8  8  6  6\n",
      "  6  6]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted labels')\n",
    "print(np.argmax(result, axis=2)[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now getting confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 17:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19467,   4981,  25063,   6410,   5122,  12074, 782318,   1437,\n",
       "        77512,   2227,  79194,    173,   2222,      0,      0,      0,\n",
       "            0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x291328082b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cm, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To release GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda \n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for RoBerta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel\n",
    "def ner_model_ROBERTa(max_input_length, train_layers, optimizer, numclasses):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = TFRobertaModel.from_pretrained('roberta-large')\n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    if not train_layers == -1:\n",
    "        \n",
    "        retrain_layers = []\n",
    "    \n",
    "        for retrain_layer_number in range(train_layers):\n",
    "\n",
    "            layer_code = '_' + str(11 - retrain_layer_number)\n",
    "            retrain_layers.append(layer_code)\n",
    "\n",
    "        for w in bert_layer.weights:\n",
    "            if not any([x in w.name for x in retrain_layers]):\n",
    "                w._trainable = False\n",
    "\n",
    "        # End of freezing section\n",
    "    \n",
    "    bert_sequence = bert_layer(bert_inputs)[0]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(numclasses, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: Tensor(\"tf_roberta_model/roberta/encoder/layer_._23/output/LayerNorm/batchnorm/add_1:0\", shape=(None, 50, 1024), dtype=float32)\n",
      "pred:  Tensor(\"ner/truediv:0\", shape=(None, 50, 17), dtype=float32)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 50, 1024), ( 355359744   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      262400      tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,626,513\n",
      "Trainable params: 355,626,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 50, 1024), ( 355359744   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 256)      262400      tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 50, 256)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, 50, 17)       4369        dropout_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,626,513\n",
      "Trainable params: 355,626,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "numclasses = 17\n",
    "model_Roberta = ner_model_ROBERTa(max_length + 1, train_layers=-1, optimizer = adam_customized, numclasses = numclasses)\n",
    "\n",
    "model_Roberta.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model_Roberta, to_file='model_r.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2985/2985 [==============================] - 3953s 1s/step - loss: 1.0230 - custom_acc_orig_tokens: 0.7586 - custom_acc_orig_non_other_tokens: 0.7586 - val_loss: 0.9961 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 2/5\n",
      "2985/2985 [==============================] - 3948s 1s/step - loss: 1.0150 - custom_acc_orig_tokens: 0.7594 - custom_acc_orig_non_other_tokens: 0.7594 - val_loss: 0.9965 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 3/5\n",
      "2985/2985 [==============================] - 3937s 1s/step - loss: 1.0140 - custom_acc_orig_tokens: 0.7594 - custom_acc_orig_non_other_tokens: 0.7594 - val_loss: 0.9958 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 4/5\n",
      "2985/2985 [==============================] - 3929s 1s/step - loss: 1.0133 - custom_acc_orig_tokens: 0.7595 - custom_acc_orig_non_other_tokens: 0.7595 - val_loss: 0.9956 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n",
      "Epoch 5/5\n",
      "2985/2985 [==============================] - 3931s 1s/step - loss: 1.0130 - custom_acc_orig_tokens: 0.7594 - custom_acc_orig_non_other_tokens: 0.7594 - val_loss: 0.9954 - val_custom_acc_orig_tokens: 0.7597 - val_custom_acc_orig_non_other_tokens: 0.7597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fff3babf88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Roberta.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "result_Roberta = model_Roberta.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels\n",
      "[13  6  3  3  6  6  6  6  6  6  6  8  6  6  6  6  6  6  6  6  6  2  6  6\n",
      " 16 16 10 10 16  6  6  6  6  1 16 16  6  6  6  6  6  6  8  8  8  8  6  6\n",
      " 15 14]\n"
     ]
    }
   ],
   "source": [
    "print('True labels')\n",
    "print(nerLabels_test[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels - BERT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-790292ba0fd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels - BERT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted labels - RoBERTa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_Roberta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print('Predicted labels - BERT')\n",
    "print(np.argmax(result, axis=2)[6])\n",
    "print('Predicted labels - RoBERTa')\n",
    "print(np.argmax(result_Roberta, axis=2)[6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result_Roberta, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 17:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
