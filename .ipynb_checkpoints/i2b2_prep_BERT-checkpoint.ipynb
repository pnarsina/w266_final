{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I2b2 dataset\n",
    "Contents of the 2018 Task2 challenge:  \n",
    "This dataset was created to identify Adverse Drug Events and Medication Extraction in EHRs. This challenge focused on three tasks:\n",
    "- Identifying concepts: drug names, dosages, durations, etc.  \n",
    "- Identifying relations: relation of drugs with ADE's and other entities given gold standard entities (generated by human annotators). \n",
    "- Running an end-to-end model that identifies relation of drugs with ADE's and other entittes on system predicted entitites.  \n",
    "\n",
    "See documentation for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is composed of individual notes (.txt extension) and corresponding individual annotation files (.ann extension).   \n",
    "Annotation files contain tags (labeled with a leading 'T') and relations (labeled with a leading 'R'):\n",
    "- For tags, the structure is: Tag_id, Tag_entity, Start_character_loc, End_character_loc  \n",
    "- For relations, the structure is: Relation_id, Relation_entity, Arg1:Tag_id, Arg2:Tag_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import i2b2_evaluate as i2b2e\n",
    "import glob, os\n",
    "import sys, io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import statistics as stats\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_path = '/Users/valeriemeausoone/Documents/W266/github_repo/w266_final/data/i2b2/2018/training_20180910/training_20180910/100035.ann'\n",
    "file_path = '/Users/valeriemeausoone/Documents/W266/github_repo/w266_final/data/i2b2/2018/training_20180910/training_20180910/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing training data for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = sorted(glob.glob(\"*.txt\"))\n",
    "ann_directory = sorted(glob.glob(\"*.ann\"))\n",
    "list_files=[]\n",
    "for file in text_directory:\n",
    "    with open(file, 'rb') as f:\n",
    "        text=f.read().decode(\"utf-8\")\n",
    "        list_files.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100035.txt', '100039.txt', '100187.txt', '100229.txt', '100564.txt', '100579.txt', '100590.txt', '100677.txt', '100847.txt', '100883.txt']\n",
      "['100035.ann', '100039.ann', '100187.ann', '100229.ann', '100564.ann', '100579.ann', '100590.ann', '100677.ann', '100847.ann', '100883.ann']\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(text_directory[0:10])\n",
    "print(ann_directory[0:10])\n",
    "print(len(list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenization(text):\n",
    "    '''Splitting discharge summaries into sentences. Because discharge summaries are not consistently organized,\n",
    "    extra processing is done to clean-up sentences and phrases. Chunks of texts are kept together to avoid splitting\n",
    "    phrases too granularly'''\n",
    "    #Using NLTK's sent_tokenize \n",
    "    sentence_tokens = sent_tokenize(text) \n",
    "    \n",
    "    #Splititng paragraphs\n",
    "    sentence_tokens2 = [paragraph for sentence in sentence_tokens for paragraph in sentence.split(\"\\n\\n\\n\")]\n",
    "        \n",
    "    #Removing sentences that are too short: only one dot (.) or a numerical bullet point (1., 2., 3.., ...10., etc.)    \n",
    "    sentence_tokens3 = [sentence.strip() for sentence in sentence_tokens2 if (sentence != \".\") or (re.match(r'\\d*\\.', sentence) is None)]\n",
    "\n",
    "    #Cleaning up line breaks and replacing them with empty spaces\n",
    "    sentence_tokens_clean = [sentence.replace('\\n', ' ') for sentence in sentence_tokens3]\n",
    "    \n",
    "    #Saving results as dataframe \n",
    "    #sentences = pd.DataFrame(sentence_tokens_clean)\n",
    "    #sentences = sentences.rename(columns={0:\"sentences\"})\n",
    "    \n",
    "    return sentence_tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission Date:  [**2115-2-22**]              Discharge Date:   [**2115-3-19**]  Date of Birth:  [**2078-8-9**]             Sex:   M  Service: MEDICINE  Allergies: Vicodin  Attending:[**First Name3 (LF) 4891**] Chief Complaint: Post-cardiac arrest, asthma exacerbation  Major Surgical or Invasive Procedure: Intubation Removal of chest tubes placed at an outside hospital R CVL placement\n",
      "Admission Date:  [**2174-4-18**]              Discharge Date:   [**2174-5-17**]  Date of Birth:  [**2135-11-15**]             Sex:   F  Service: MEDICINE  Allergies: Prochlorperazine / Heparin Agents  Attending:[**First Name3 (LF) 3918**] Chief Complaint: Abdominal Pain  Major Surgical or Invasive Procedure: Upper GI series with small bowel follow through Right heart catheterization IR guided paracentesis\n"
     ]
    }
   ],
   "source": [
    "#100035.txt and 100039.txt\n",
    "for file in list_files[0:2]:\n",
    "    print(sentence_tokenization(file)[0])\n",
    "    sentences = sentence_tokenization(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking with relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations_processing(file):\n",
    "    '''This function processes the annotation files into dataframes (relation and concept). \n",
    "    It then combines these dataframes to create an enhanced relations dictionary'''\n",
    "    \n",
    "    #Reading the annotation file into a combined dataframe. \n",
    "    ann_df = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "    ann_df = ann_df.rename(columns={0:\"tag\", 1:\"description\", 2:\"text\"})\n",
    "\n",
    "    #Splitting concept entities and relations\n",
    "    #Relations dataframe\n",
    "    null_entries = pd.isnull(ann_df[\"text\"])\n",
    "    rf_df = ann_df[null_entries]\n",
    "    rf_df = rf_df.rename(columns={'tag':\"relation_id\", 'description':\"relation_description\", 'text': 'relation_text'})\n",
    "    #Cleaning up\n",
    "    rf_df[['relation','arg1', 'arg2']] = rf_df['relation_description'].str.split(' ',expand=True)\n",
    "    rf_df[['arg1_delete','arg1_keep']] = rf_df['arg1'].str.split(':',expand=True)\n",
    "    rf_df[['arg2_delete','arg2_keep']] = rf_df['arg2'].str.split(':',expand=True)\n",
    "    rf_df = rf_df.drop(columns=['relation_text', 'arg1', 'arg2', 'arg1_delete', 'arg2_delete'])\n",
    "    rf_df = rf_df.rename(columns={'arg1_keep':\"arg1\", 'arg2_keep':\"arg2\"})\n",
    "    \n",
    "    #Concepts dataframe\n",
    "    entries = pd.notnull(ann_df[\"text\"])\n",
    "    tag_df = ann_df[entries]\n",
    "    tag_df = tag_df.rename(columns={'tag':\"concept_id\", 'description':\"concept_description\", 'text': 'concept_text'})\n",
    "\n",
    "    #Combining relations and tags dataframes to create an enhanced relations dataframe\n",
    "    rf_df = pd.merge(rf_df, tag_df, left_on = 'arg1', right_on='concept_id')\n",
    "    rf_df = rf_df.rename(columns={'concept_id': 'arg1_id', 'concept_description':\"arg1_description\", 'concept_text':\"arg1_text\"})\n",
    "    rf_df = pd.merge(rf_df, tag_df, left_on = 'arg2', right_on='concept_id')\n",
    "    rf_df = rf_df.rename(columns={'concept_id': 'arg2_id', 'concept_description':\"arg2_description\", 'concept_text':\"arg2_text\"})\n",
    "    rf_df = rf_df.drop(columns=['arg1_id', 'arg2_id'])\n",
    "\n",
    "    #Creating a relations dictionary\n",
    "    #Note that there could be \"duplicate\" relations that we will have to re-identify later. \n",
    "    dict_relation = {}\n",
    "    for sentence in sentences: \n",
    "        for i in range(len(rf_df)):\n",
    "            arg1 = rf_df['arg1_text'][i]\n",
    "            arg2 = rf_df['arg2_text'][i]\n",
    "            relation = rf_df['relation'][i]\n",
    "            dict_relation[(arg1, arg2)] = relation\n",
    "    return dict_relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_relations = [annotations_processing(file) for file in ann_directory]\n",
    "len(list_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num=0\n",
    "relation_sentences=[]\n",
    "errors= 0\n",
    "for file in list_files:\n",
    "    \n",
    "    #implementing sentence tokenization\n",
    "    sentences = np.array(sentence_tokenization(file))\n",
    "\n",
    "    #listing entities that make up relations\n",
    "    list_entities = list(list_relations[file_num].keys())\n",
    "    \n",
    "    #looking for relation tags in sentences and pulling out sentences. \n",
    "    for e in list_entities:        \n",
    "        new_e = e\n",
    "        arg1_indices = np.where(np.char.find(sentences, new_e[0])>=0)[0]\n",
    "        if arg1_indices.size==0: \n",
    "            new_e = list(new_e)\n",
    "            new_e[0] = new_e[0].replace(' ', '')\n",
    "            new_e = tuple(new_e)\n",
    "            arg1_indices = np.where(np.char.find(sentences, new_e[0])>=0)[0]\n",
    "\n",
    "        arg2_indices = np.where(np.char.find(sentences, new_e[1])>=0)[0]\n",
    "        if arg2_indices.size==0: \n",
    "            new_e = list(new_e)\n",
    "            new_e[1] = new_e[1].replace(' ', '')\n",
    "            new_e = tuple(new_e)\n",
    "            arg2_indices = np.where(np.char.find(sentences, new_e[1])>=0)[0]\n",
    "\n",
    "        #extract where minimum. \n",
    "        combinations = [(i,j,abs(i-j)) for i,j in list(itertools.product(arg1_indices, arg2_indices))]\n",
    "        try:\n",
    "            min_distance = min(combinations, key = lambda t: t[2])[2]\n",
    "        except ValueError:\n",
    "            min_distance = \"none\"\n",
    "        if min_distance != \"none\":\n",
    "            min_combinations = [(t[0], t[1]) for t in combinations if t[2] == min_distance]\n",
    "            for c in min_combinations:\n",
    "                if c[0]==c[1]:\n",
    "                    include_sentence = sentences[c[0]]\n",
    "                    include_sentence = include_sentence.replace(new_e[0], (\"SUB_B \" + new_e[0] + \" SUB_E\"))\n",
    "                    include_sentence = include_sentence.replace(new_e[1], (\"OBJ_B \" + new_e[1] + \" OBJ_E\"))\n",
    "                    relation_sentences.append((new_e, list_relations[file_num][e], include_sentence))\n",
    "                    sentences.tolist().pop(c[0])\n",
    "                elif c[0]!=c[1]:\n",
    "                    include_sentence = sentences[c[0]] + \" \" + sentences[c[1]]\n",
    "                    include_sentence = include_sentence.replace(new_e[0], (\"SUB_B \" + new_e[0] + \" SUB_E\"))\n",
    "                    include_sentence = include_sentence.replace(new_e[1], (\"OBJ_B \" + new_e[1] + \" OBJ_E\"))\n",
    "                    relation_sentences.append((new_e, list_relations[file_num][e], include_sentence))\n",
    "                    sentences.tolist().pop(c[0])\n",
    "                    sentences.tolist().pop(c[1])\n",
    "                    \n",
    "    for s in range(len(sentences)):\n",
    "        relation_sentences.append((\"none\", \"no relation\", sentences[s]))   \n",
    "    #print(\"output length\", len(relation_sentences))\n",
    "    \n",
    "    file_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences 76318\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences\", len(relation_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>args</th>\n",
       "      <th>relation</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(recurrent seizures, ativan)</td>\n",
       "      <td>Reason-Drug</td>\n",
       "      <td>He also may have SUB_B recurrent seizures SUB_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(IM, ativan)</td>\n",
       "      <td>Route-Drug</td>\n",
       "      <td>He also may have recurrent seizures which shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(IV, ativan)</td>\n",
       "      <td>Route-Drug</td>\n",
       "      <td>He also may have recurrent seizures which shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(25mg, Topiramate)</td>\n",
       "      <td>Strength-Drug</td>\n",
       "      <td>-patient will be on OBJ_B Topiramate OBJ_E SUB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(PO, Topiramate)</td>\n",
       "      <td>Route-Drug</td>\n",
       "      <td>-patient will be on OBJ_B Topiramate OBJ_E 25m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           args       relation  \\\n",
       "0  (recurrent seizures, ativan)    Reason-Drug   \n",
       "1                  (IM, ativan)     Route-Drug   \n",
       "2                  (IV, ativan)     Route-Drug   \n",
       "3            (25mg, Topiramate)  Strength-Drug   \n",
       "4              (PO, Topiramate)     Route-Drug   \n",
       "\n",
       "                                            sentence  \n",
       "0  He also may have SUB_B recurrent seizures SUB_...  \n",
       "1  He also may have recurrent seizures which shou...  \n",
       "2  He also may have recurrent seizures which shou...  \n",
       "3  -patient will be on OBJ_B Topiramate OBJ_E SUB...  \n",
       "4  -patient will be on OBJ_B Topiramate OBJ_E 25m...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating a dataframe\n",
    "train_df = pd.DataFrame(relation_sentences)\n",
    "train_df = train_df.rename(columns={0:\"args\", 1:\"relation\", 2: \"sentence\"})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no relation       40955\n",
       "Strength-Drug      6781\n",
       "Frequency-Drug     6484\n",
       "Route-Drug         5925\n",
       "Reason-Drug        5263\n",
       "Form-Drug          4643\n",
       "Dosage-Drug        4455\n",
       "ADE-Drug           1167\n",
       "Duration-Drug       645\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage without a relation: 53.66361801934013\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage without a relation:\", 40955*100/len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = train_df['relation'].unique()\n",
    "\n",
    "label_to_ids_map =  {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def to_label_id(series):\n",
    "    return label_to_ids_map[series]\n",
    "\n",
    "train_df_bert = train_df.copy()\n",
    "train_df_bert = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label': train_df['relation'].apply(to_label_id),\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df['sentence']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>He also may have SUB_B recurrent seizures SUB_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>He also may have recurrent seizures which shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>He also may have recurrent seizures which shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>-patient will be on OBJ_B Topiramate OBJ_E SUB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>-patient will be on OBJ_B Topiramate OBJ_E 25m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  He also may have SUB_B recurrent seizures SUB_...\n",
       "1   1      1     a  He also may have recurrent seizures which shou...\n",
       "2   2      1     a  He also may have recurrent seizures which shou...\n",
       "3   3      2     a  -patient will be on OBJ_B Topiramate OBJ_E SUB...\n",
       "4   4      1     a  -patient will be on OBJ_B Topiramate OBJ_E 25m..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert.to_csv(\"i2b2_train_bert.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processting test data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_path = '/Users/valeriemeausoone/Documents/W266/github_repo/w266_final/data/i2b2/2018/training_20180910/training_20180910/100035.ann'\n",
    "file_path = '/Users/valeriemeausoone/Documents/W266/github_repo/w266_final/data/i2b2/2018/gold_standard_test/'\n",
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_directory = sorted(glob.glob(\"*.txt\"))\n",
    "ann_directory = sorted(glob.glob(\"*.ann\"))\n",
    "list_files=[]\n",
    "for file in text_directory:\n",
    "    with open(file, 'rb') as f:\n",
    "        text=f.read().decode(\"utf-8\")\n",
    "        list_files.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_relations = [annotations_processing(file) for file in ann_directory]\n",
    "len(list_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences 50010\n"
     ]
    }
   ],
   "source": [
    "file_num=0\n",
    "relation_sentences=[]\n",
    "errors= 0\n",
    "for file in list_files:\n",
    "    \n",
    "    #implementing sentence tokenization\n",
    "    sentences = np.array(sentence_tokenization(file))\n",
    "\n",
    "    #listing entities that make up relations\n",
    "    list_entities = list(list_relations[file_num].keys())\n",
    "    \n",
    "    #looking for relation tags in sentences and pulling out sentences. \n",
    "    for e in list_entities:        \n",
    "        new_e = e\n",
    "        arg1_indices = np.where(np.char.find(sentences, new_e[0])>=0)[0]\n",
    "        if arg1_indices.size==0: \n",
    "            new_e = list(new_e)\n",
    "            new_e[0] = new_e[0].replace(' ', '')\n",
    "            new_e = tuple(new_e)\n",
    "            arg1_indices = np.where(np.char.find(sentences, new_e[0])>=0)[0]\n",
    "\n",
    "        arg2_indices = np.where(np.char.find(sentences, new_e[1])>=0)[0]\n",
    "        if arg2_indices.size==0: \n",
    "            new_e = list(new_e)\n",
    "            new_e[1] = new_e[1].replace(' ', '')\n",
    "            new_e = tuple(new_e)\n",
    "            arg2_indices = np.where(np.char.find(sentences, new_e[1])>=0)[0]\n",
    "\n",
    "        #extract where minimum. \n",
    "        combinations = [(i,j,abs(i-j)) for i,j in list(itertools.product(arg1_indices, arg2_indices))]\n",
    "        try:\n",
    "            min_distance = min(combinations, key = lambda t: t[2])[2]\n",
    "        except ValueError:\n",
    "            min_distance = \"none\"\n",
    "        if min_distance != \"none\":\n",
    "            min_combinations = [(t[0], t[1]) for t in combinations if t[2] == min_distance]\n",
    "            for c in min_combinations:\n",
    "                if c[0]==c[1]:\n",
    "                    include_sentence = sentences[c[0]]\n",
    "                    include_sentence = include_sentence.replace(new_e[0], (\"SUB_B \" + new_e[0] + \" SUB_E\"))\n",
    "                    include_sentence = include_sentence.replace(new_e[1], (\"OBJ_B \" + new_e[1] + \" OBJ_E\"))\n",
    "                    relation_sentences.append((new_e, list_relations[file_num][e], include_sentence))\n",
    "                    sentences.tolist().pop(c[0])\n",
    "                elif c[0]!=c[1]:\n",
    "                    include_sentence = sentences[c[0]] + \" \" + sentences[c[1]]\n",
    "                    include_sentence = include_sentence.replace(new_e[0], (\"SUB_B \" + new_e[0] + \" SUB_E\"))\n",
    "                    include_sentence = include_sentence.replace(new_e[1], (\"OBJ_B \" + new_e[1] + \" OBJ_E\"))\n",
    "                    relation_sentences.append((new_e, list_relations[file_num][e], include_sentence))\n",
    "                    sentences.tolist().pop(c[0])\n",
    "                    sentences.tolist().pop(c[1])\n",
    "                    \n",
    "    for s in range(len(sentences)):\n",
    "        relation_sentences.append((\"none\", \"no relation\", sentences[s]))   \n",
    "    #print(\"output length\", len(relation_sentences))\n",
    "    \n",
    "    file_num+=1\n",
    "    \n",
    "print(\"Number of sentences\", len(relation_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no relation       27438\n",
       "Strength-Drug      4274\n",
       "Frequency-Drug     4090\n",
       "Route-Drug         3723\n",
       "Reason-Drug        3464\n",
       "Form-Drug          3024\n",
       "Dosage-Drug        2802\n",
       "ADE-Drug            775\n",
       "Duration-Drug       420\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating a dataframe\n",
    "test_df = pd.DataFrame(relation_sentences)\n",
    "test_df = test_df.rename(columns={0:\"args\", 1:\"relation\", 2: \"sentence\"})\n",
    "\n",
    "test_df['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage without a relation: 81.89362127574485\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage without a relation:\", 40955*100/len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>MEDICATIONS:  Lipitor, Tylenol with Codeine, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>tapered over SUB_B one week SUB_E and disconti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>She was started on prophylactic OBJ_B Oxacilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>The patient's course in the Intensive Care Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>The patient's course in the Intensive Care Uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  MEDICATIONS:  Lipitor, Tylenol with Codeine, D...\n",
       "1   1      1     a  tapered over SUB_B one week SUB_E and disconti...\n",
       "2   2      2     a  She was started on prophylactic OBJ_B Oxacilli...\n",
       "3   3      0     a  The patient's course in the Intensive Care Uni...\n",
       "4   4      3     a  The patient's course in the Intensive Care Uni..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = test_df['relation'].unique()\n",
    "\n",
    "label_to_ids_map =  {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def to_label_id(series):\n",
    "    return label_to_ids_map[series]\n",
    "\n",
    "test_df_bert = test_df.copy()\n",
    "test_df_bert = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'label': test_df['relation'].apply(to_label_id),\n",
    "    'alpha':['a']*test_df.shape[0],\n",
    "    'text': test_df['sentence']\n",
    "})\n",
    "\n",
    "test_df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_bert.to_csv(\"i2b2_test_bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
