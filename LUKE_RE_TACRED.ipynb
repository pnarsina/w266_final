{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import WEIGHTS_NAME\n",
    "\n",
    "from luke.luke_utils.entity_vocab import MASK_TOKEN\n",
    "\n",
    "from luke.utils import set_seed\n",
    "from luke.utils.trainer import Trainer, trainer_args\n",
    "from luke.model import LukeForRelationClassification\n",
    "from luke.re_utils import HEAD_TOKEN, TAIL_TOKEN, convert_examples_to_features, DatasetProcessor\n",
    "from transformers.tokenization_roberta import RobertaTokenizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values (hardcode) for now\n",
    "class params:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"luke/data/tacred/json\"\n",
    "        self.do_train = \"--no-train\"\n",
    "        self.train_batch_size = 4\n",
    "        self.num_train_epochs = 5.0\n",
    "        self.do_val = \"--no-eval\"\n",
    "        self.eval_batch_size = 128\n",
    "        self.seed = 42\n",
    "        self.bert_model_name = \"roberta-large\"\n",
    "        self.max_mention_length = 30\n",
    "        self.local_rank = -1\n",
    "        self.tokenizer =  RobertaTokenizer.from_pretrained(self.bert_model_name)\n",
    "#         self.tokenizer = {\"max_len\": 512, \"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"sep_token\": \"</s>\", \"pad_token\": \"<pad>\", \"cls_token\": \"<s>\", \"mask_token\": \"<mask>\", \"init_inputs\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = params()\n",
    "args.tokenizer.pad_token_id\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, fold=\"train\"):\n",
    "\n",
    "    processor = DatasetProcessor()\n",
    "    if fold == \"train\":\n",
    "        examples = processor.get_train_examples(args.data_dir)\n",
    "    elif fold == \"dev\":\n",
    "        examples = processor.get_dev_examples(args.data_dir)\n",
    "    else:\n",
    "        examples = processor.get_test_examples(args.data_dir)\n",
    "\n",
    "    label_list = processor.get_label_list(args.data_dir)\n",
    "\n",
    "    bert_model_name = args.bert_model_name\n",
    "\n",
    "    cache_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_\" + \"_\".join((args.bert_model_name.split(\"-\")[0], str(args.max_mention_length), fold)) + \".pkl\",\n",
    "    )\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cache_file)\n",
    "        features = torch.load(cache_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file\")\n",
    "        features = convert_examples_to_features(examples, label_list, args.tokenizer, args.max_mention_length)\n",
    "\n",
    "        if args.local_rank in (-1, 0):\n",
    "            torch.save(features, cache_file)\n",
    "\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        def create_padded_sequence(attr_name, padding_value):\n",
    "            tensors = [torch.tensor(getattr(o, attr_name), dtype=torch.long) for o in batch]\n",
    "            return torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "        return dict(\n",
    "            word_ids=create_padded_sequence(\"word_ids\", args.tokenizer.pad_token_id),\n",
    "            word_attention_mask=create_padded_sequence(\"word_attention_mask\", 0),\n",
    "            word_segment_ids=create_padded_sequence(\"word_segment_ids\", 0),\n",
    "            entity_ids=create_padded_sequence(\"entity_ids\", 0),\n",
    "            entity_attention_mask=create_padded_sequence(\"entity_attention_mask\", 0),\n",
    "            entity_position_ids=create_padded_sequence(\"entity_position_ids\", -1),\n",
    "            entity_segment_ids=create_padded_sequence(\"entity_segment_ids\", 0),\n",
    "            label=torch.tensor([o.label for o in batch], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    if fold in (\"dev\", \"test\"):\n",
    "        dataloader = DataLoader(features, batch_size=args.eval_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    else:\n",
    "        if args.local_rank == -1:\n",
    "            sampler = RandomSampler(features)\n",
    "        else:\n",
    "            sampler = DistributedSampler(features)\n",
    "        dataloader = DataLoader(features, sampler=sampler, batch_size=args.train_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader, examples, features, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, examples, features, label_list = load_and_cache_examples(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader, list, list, list)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader), type(examples), type(features), type(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68124,), (68124,), (42,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(examples).shape, np.array(features).shape, np.array(label_list).shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_relation', 'org:alternate_names', 'org:city_of_headquarters', 'org:country_of_headquarters', 'org:dissolved', 'org:founded', 'org:founded_by', 'org:member_of', 'org:members', 'org:number_of_employees/members', 'org:parents', 'org:political/religious_affiliation', 'org:shareholders', 'org:stateorprovince_of_headquarters', 'org:subsidiaries', 'org:top_members/employees', 'org:website', 'per:age', 'per:alternate_names', 'per:cause_of_death']\n"
     ]
    }
   ],
   "source": [
    "print(label_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
